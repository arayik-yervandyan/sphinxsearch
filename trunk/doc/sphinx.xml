<!DOCTYPE book [
<!ENTITY iuml "&#xEF;">
]>

<book>

<title>Sphinx 1.11-beta reference manual</title>
<subtitle>Free open-source SQL full-text search engine</subtitle>

<bookinfo>
<copyright>
<year>2001-2011</year>
<holder>Andrew Aksyonoff</holder>
</copyright>
<copyright>
<year>2008-2011</year>
<holder>Sphinx Technologies Inc, <ulink 
url="http://sphinxsearch.com">http://sphinxsearch.com</ulink></holder>
</copyright>
</bookinfo>


<chapter id="intro"><title>Introduction</title>


<sect1 id="about"><title>About</title>
<para>
Sphinx is a full-text search engine, publicly distributed under GPL version 2.
Commercial licensing (eg. for embedded use) is available upon request. 
</para>
<para>
Technically, Sphinx is a standalone software package provides
fast and relevant full-text search functionality to client applications.
It was specially designed to integrate well with SQL databases storing
the data, and to be easily accessed scripting languages. However, Sphinx
does not depend on nor require any specific database to function.
</para>
<para>
Applications can access Sphinx search daemon (searchd) using any of
the three different access methods: a) via native search API (SphinxAPI),
b) via Sphinx own implementation of MySQL network protocol (using a small
SQL subset called SphinxQL), or c) via MySQL server with a pluggable
storage engine (SphinxSE).
</para>
<para>
Official native SphinxAPI implementations for PHP, Perl, Ruby, and Java
are included within the distribution package. API is very lightweight
so porting it to a new language is known to take a few hours or days.
Third party API ports and plugins exist for Perl, C#, Haskell,
Ruby-on-Rails, and possibly other languages and frameworks.
</para>
<para>
Starting version 1.10-beta, Sphinx supports two different indexing
backends: "disk" index backend, and "realtime" (RT) index backend.
Disk indexes support online full-text index rebuilds, but online updates
can only be done on non-text (attribute) data.  RT indexes additionally
allow for online full-text index updates. Previous versions only
supported disk indexes.
</para>
<para>
Data can be loaded into disk indexes using a so-called data source.
Built-in sources can fetch data directly from MySQL, PostgreSQL, ODBC
compliant database (MS SQL, Oracle, etc), or a pipe in a custom XML format.
Adding new data sources drivers (eg. to natively support other DBMSes)
is designed to be as easy as possible. RT indexes, as of 1.10-beta,
can only be populated using SphinxQL.
</para>
<para>
As for the name, Sphinx is an acronym which is officially decoded
as SQL Phrase Index. Yes, I know about CMU's Sphinx project. 
</para>
</sect1>


<sect1 id="features"><title>Sphinx features</title>
<para>
Key Sphinx features are:
<itemizedlist>
<listitem>high indexing and searching performance;</listitem>
<listitem>advanced indexing and querying tools (flexible and feature-rich text tokenizer, querying language, several different ranking modes, etc);</listitem>
<listitem>advanced result set post-processing (SELECT with expressions, WHERE, ORDER BY, GROUP BY etc over text search results);</listitem>
<listitem>proven scalability up to billions of documents, terabytes of data, and thousands of queries per second;</listitem>
<listitem>easy integration with SQL and XML data sources, and SphinxAPI, SphinxQL, or SphinxSE search interfaces;</listitem>
<listitem>easy scaling with distributed searches.</listitem>
</itemizedlist>
To expand a bit, Sphinx:
<itemizedlist>
<listitem>has high indexing speed (upto 10-15 MB/sec per core on an internal benchmark);</listitem>
<listitem>has high search speed (upto 150-250 queries/sec per core against 1,000,000 documents, 1.2 GB of data on an internal benchmark);</listitem>
<listitem>has high scalability (biggest known cluster indexes over 3,000,000,000 documents, and busiest one peaks over 50,000,000 queries/day);</listitem>
<listitem>provides good relevance ranking through combination of phrase proximity ranking and statistical (BM25) ranking;</listitem>
<listitem>provides distributed searching capabilities;</listitem>
<listitem>provides document excerpts (snippets) generation;</listitem>
<listitem>provides searching from within application with SphinxAPI or SphinxQL interfaces, and from within MySQL with pluggable SphinxSE storage engine;</listitem>
<listitem>supports boolean, phrase, word proximity and other types of queries;</listitem>
<listitem>supports multiple full-text fields per document (upto 32 by default);</listitem>
<listitem>supports multiple additional attributes per document (ie. groups, timestamps, etc);</listitem>
<listitem>supports stopwords;</listitem>
<listitem>supports morphological word forms dictionaries;</listitem>
<listitem>supports tokenizing exceptions;</listitem>
<listitem>supports both single-byte encodings and UTF-8;</listitem>
<listitem>supports stemming (stemmers for English, Russian and Czech are built-in; and stemmers for
French, Spanish, Portuguese, Italian, Romanian, German, Dutch, Swedish, Norwegian, Danish, Finnish, Hungarian,
are available by building third party <ulink url="http://snowball.tartarus.org/">libstemmer library</ulink>);</listitem>
<listitem>supports MySQL natively (all types of tables, including MyISAM, InnoDB, NDB, Archive, etc are supported);</listitem>
<listitem>supports PostgreSQL natively;</listitem>
<listitem>supports ODBC compliant databases (MS SQL, Oracle, etc) natively;</listitem>
<listitem>...has 50+ other features not listed here, refer to API and configuration manual!</listitem>
</itemizedlist>
</para>
</sect1>


<sect1 id="getting"><title>Where to get Sphinx</title>
<para>Sphinx is available through its official Web site at <ulink url="http://sphinxsearch.com/">http://sphinxsearch.com/</ulink>.
</para>
<para>Currently, Sphinx distribution tarball includes the following software:
<itemizedlist>
<listitem><filename>indexer</filename>: an utility which creates fulltext indexes;</listitem>
<listitem><filename>search</filename>: a simple command-line (CLI) test utility which searches through fulltext indexes;</listitem>
<listitem><filename>searchd</filename>: a daemon which enables external software (eg. Web applications) to search through fulltext indexes;</listitem>
<listitem><filename>sphinxapi</filename>: a set of searchd client API libraries for popular Web scripting languages (PHP, Python, Perl, Ruby).</listitem>
<listitem><filename>spelldump</filename>: a simple command-line tool to extract the items from an <filename>ispell</filename> or <filename>MySpell</filename>
(as bundled with OpenOffice) format dictionary to help customize your index, for use with <link linkend="conf-wordforms">wordforms</link>.</listitem>
<listitem><filename>indextool</filename>: an utility to dump miscellaneous debug information about the index, added in version 0.9.9-rc2.</listitem>
</itemizedlist>
</para>
</sect1>


<sect1 id="license"><title>License</title>
<para>
This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License,
or (at your option) any later version. See COPYING file for details.
</para>
<para>
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
more details. 
</para>
<para>
You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software Foundation, Inc.,
59 Temple Place, Suite 330, Boston, MA 02111-1307 USA 
</para>
<para>
Non-GPL licensing (for OEM/ISV embedded use) can also be arranged, please
<ulink url="http://sphinxsearch.com/contacts.html">contact us</ulink> to discuss
commercial licensing possibilities.
</para>
</sect1>


<sect1 id="credits"><title>Credits</title>
<bridgehead>Author</bridgehead>
<para>
Sphinx initial author (and a benevolent dictator ever since):
<itemizedlist>
<listitem>Andrew Aksyonoff, <ulink url="http://shodan.ru">http://shodan.ru</ulink></listitem>
</itemizedlist>
</para>
<bridgehead>Team</bridgehead>
<para>
Past and present employees of Sphinx Technologies Inc who should be
noted on their work on Sphinx (in alphabetical order):
<itemizedlist>
<listitem>Alexander Klimenko</listitem>
<listitem>Alexey Dvoichenkov</listitem>
<listitem>Alexey Vinogradov</listitem>
<listitem>Ilya Kuznetsov</listitem>
<listitem>Stanislav Klinov</listitem>
</itemizedlist>
</para>
<bridgehead>Contributors</bridgehead>
<para>People who contributed to Sphinx and their contributions (in no particular order):
<itemizedlist>
<listitem>Robert "coredev" Bengtsson (Sweden), initial version of PostgreSQL data source</listitem>
<listitem>Len Kranendonk, Perl API</listitem>
<listitem>Dmytro Shteflyuk, Ruby API</listitem>
</itemizedlist>
</para>
<para>
Many other people have contributed ideas, bug reports, fixes, etc.
Thank you!
</para>
</sect1>


<sect1 id="history"><title>History</title>
<para>
Sphinx development was started back in 2001, because I didn't manage
to find an acceptable search solution (for a database driven Web site)
which would meet my requirements. Actually, each and every important aspect was a problem: 
<itemizedlist>
<listitem>search quality (ie. good relevance)
<itemizedlist><listitem>statistical ranking methods performed rather bad, especially on large collections of small documents (forums, blogs, etc)</listitem></itemizedlist>
</listitem>
<listitem>search speed
<itemizedlist><listitem>especially if searching for phrases which contain stopwords, as in "to be or not to be"</listitem></itemizedlist>
</listitem>
<listitem>moderate disk and CPU requirements when indexing
<itemizedlist><listitem>important in shared hosting enivronment, not to mention the indexing speed.</listitem></itemizedlist>
</listitem>
</itemizedlist>
</para>
<para>
Despite the amount of time passed and numerous improvements made in the
other solutions, there's still no solution which I personally would
be eager to migrate to. 
</para>
<para>
Considering that and a lot of positive feedback received from Sphinx users
during last years, the obvious decision is to continue developing Sphinx
(and, eventually, to take over the world).
</para>
</sect1>


</chapter>
<chapter id="installation"><title>Installation</title>


<sect1 id="supported-system"><title>Supported systems</title>
<para>
Most modern UNIX systems with a C++ compiler should be able
to compile and run Sphinx without any modifications.
</para>
<para>
Currently known systems Sphinx has been successfully running on are:
<itemizedlist>
<listitem>Linux 2.4.x, 2.6.x (many various distributions)</listitem>
<listitem>Windows 2000, XP</listitem>
<listitem>FreeBSD 4.x, 5.x, 6.x, 7.x</listitem>
<listitem>NetBSD 1.6, 3.0</listitem>
<listitem>Solaris 9, 11</listitem>
<listitem>Mac OS X</listitem>
</itemizedlist>
</para>
<para>
CPU architectures known to work include X86, X86-64, SPARC64, ARM.
</para>
<para>
Chance are good that Sphinx should work on other Unix platforms as well;
please report any platforms missing from this list that worked for you!
</para>
</sect1>


<sect1 id="required-tools"><title>Required tools</title>
<para>
On UNIX, you will need the following tools to build
and install Sphinx:
<itemizedlist>
<listitem>a working C++ compiler. GNU gcc is known to work.</listitem>
<listitem>a good make program. GNU make is known to work.</listitem>
</itemizedlist>
</para>
<para>
On Windows, you will need Microsoft Visual C/C++ Studio .NET 2003 or 2005.
Other compilers/environments will probably work as well, but for the
time being, you will have to build makefile (or other environment
specific project files) manually.
</para>
</sect1>


<sect1 id="installing"><title>Installing Sphinx on Linux</title>
<para><orderedlist>
<listitem>
	<para>
	Extract everything from the distribution tarball (haven't you already?)
	and go to the <filename>sphinx</filename> subdirectory. (We are using
	version 1.11-beta here for the sake of example only; be sure to change this
	to a specific version you're using.)
	</para>
<para><userinput><literallayout>$ tar xzvf sphinx-1.11-beta.tar.gz
$ cd sphinx
</literallayout></userinput></para>
</listitem>
<listitem>
	<para>Run the configuration program:</para>
	<para><userinput><literallayout>$ ./configure</literallayout></userinput></para>
	<para>
	There's a number of options to configure. The complete listing may
	be obtained by using <option>--help</option> switch. The most important ones are:
	<itemizedlist>
		<listitem><option>--prefix</option>, which specifies where to install Sphinx; such as <option>--prefix=/usr/local/sphinx</option> (all of the examples use this prefix)</listitem>
		<listitem><option>--with-mysql</option>, which specifies where to look for MySQL
			include and library files, if auto-detection fails;</listitem>
		<listitem><option>--with-pgsql</option>, which specifies where to look for PostgreSQL
			include and library files.</listitem>
	</itemizedlist>
	</para>
</listitem>
<listitem>
	<para>Build the binaries:</para>
	<para><userinput><literallayout>$ make</literallayout></userinput></para>
</listitem>
<listitem>
	<para>Install the binaries in the directory of your choice:
	(defaults to <filename>/usr/local/bin/</filename> on *nix systems,
	but is overridden with <option>configure --prefix</option>)</para>
	<para><userinput><literallayout>$ make install</literallayout></userinput></para>
</listitem>
</orderedlist></para>
</sect1>

<sect1 id="installing-windows"><title>Installing Sphinx on Windows</title>
<para>Installing Sphinx on a Windows server is often easier than installing on a Linux environment;
unless you are preparing code patches, you can use the pre-compiled binary files from the Downloads
area on the website.</para>
<orderedlist>
<listitem>
	<para>Extract everything from the .zip file you have downloaded -
	<filename>sphinx-1.11-beta-win32.zip</filename>,
	or <filename>sphinx-1.11-beta-win32-pgsql.zip</filename> if you need PostgresSQL support as well.
	(We are using version 1.11-beta here for the sake of example only;
	be sure to change this to a specific version you're using.)
	You can use Windows Explorer in Windows XP and up to extract the files,
	or a freeware package like 7Zip to open the archive.</para>
	<para>For the remainder of this guide, we will assume that the folders are unzipped into <filename>C:\Sphinx</filename>,
	such that <filename>searchd.exe</filename> can be found in <filename>C:\Sphinx\bin\searchd.exe</filename>. If you decide
	to use any different location for the folders or configuration file, please change it accordingly.</para>
</listitem>
<listitem>
	<para>Edit the contents of sphinx.conf.in - specifically entries relating to @CONFDIR@ - to paths suitable for your system.</para>
</listitem>
<listitem>
	<para>Install the <filename>searchd</filename> system as a Windows service:</para>
	<para><userinput>C:\Sphinx\bin&gt; C:\Sphinx\bin\searchd --install --config C:\Sphinx\sphinx.conf.in --servicename SphinxSearch</userinput></para>
</listitem>
<listitem>
	<para>The <filename>searchd</filename> service will now be listed in the Services panel
	within the Management Console, available from Administrative Tools. It will not have been
	started, as you will need to configure it and build your indexes with <filename>indexer</filename>
	before starting the service. A guide to do this can be found under
	<link linkend="quick-tour">Quick tour</link>.</para>
	<para>During the next steps of the install (which involve running indexer pretty much as
	you would on Linux) you may find that you get an error relating to libmysql.dll not being found.
	If you have MySQL installed, you should find a copy of this library in your Windows directory,
	or sometimes in Windows\System32, or failing that in the MySQL core directories. If you
	do receive an error please copy libmysql.dll into the bin directory.</para>
</listitem>
</orderedlist>
</sect1>

<sect1 id="install-problems"><title>Known installation issues</title>
<para>
If <filename>configure</filename> fails to locate MySQL headers and/or libraries,
try checking for and installing <filename>mysql-devel</filename> package. On some systems,
it is not installed by default.
</para>
<para>
If <filename>make</filename> fails with a message which look like
<programlisting>
/bin/sh: g++: command not found
make[1]: *** [libsphinx_a-sphinx.o] Error 127
</programlisting>
try checking for and installing <filename>gcc-c++</filename> package.
</para>
<para>
If you are getting compile-time errors which look like
<programlisting>
sphinx.cpp:67: error: invalid application of `sizeof' to
    incomplete type `Private::SizeError&lt;false&gt;'
</programlisting>
this means that some compile-time type size check failed.
The most probable reason is that off_t type is less than 64-bit
on your system. As a quick hack, you can edit sphinx.h and replace off_t
with DWORD in a typedef for SphOffset_t, but note that this will prohibit
you from using full-text indexes larger than 2 GB. Even if the hack helps,
please report such issues, providing the exact error message and
compiler/OS details, so I could properly fix them in next releases.
</para>
<para>
If you keep getting any other error, or the suggestions above
do not seem to help you, please don't hesitate to contact me.
</para>
</sect1>


<sect1 id="quick-tour"><title>Quick Sphinx usage tour</title>
<para>
All the example commands below assume that you installed Sphinx
in <filename>/usr/local/sphinx</filename>, so <filename>searchd</filename> can
be found in <filename>/usr/local/sphinx/bin/searchd</filename>.
</para>
<para>
To use Sphinx, you will need to:
</para>
<orderedlist>
<listitem>
	<para>Create a configuration file.</para>
	<para>
	Default configuration file name is <filename>sphinx.conf</filename>.
	All Sphinx programs look for this file in current working directory
	by default.
	</para>
	<para>
	Sample configuration file, <filename>sphinx.conf.dist</filename>, which has
	all the options documented, is created by <filename>configure</filename>.
	Copy and edit that sample file to make your own configuration: (assuming Sphinx is installed into <filename>/usr/local/sphinx/</filename>)
	</para>
<para><userinput><literallayout>$ cd /usr/local/sphinx/etc
$ cp sphinx.conf.dist sphinx.conf
$ vi sphinx.conf</literallayout></userinput></para>
	<para>
	Sample configuration file is setup to index <filename>documents</filename>
	table from MySQL database <filename>test</filename>; so there's <filename>example.sql</filename>
	sample data file to populate that table with a few documents for testing purposes:
	</para>
	<para><userinput><literallayout>$ mysql -u test &lt; /usr/local/sphinx/etc/example.sql</literallayout></userinput></para>
</listitem>
<listitem>
	<para>Run the indexer to create full-text index from your data:</para>
<para><userinput><literallayout>$ cd /usr/local/sphinx/etc
$ /usr/local/sphinx/bin/indexer --all</literallayout></userinput></para>
</listitem>
<listitem>
	<para>Query your newly created index!</para>
</listitem>
</orderedlist>
<para>
To query the index from command line, use <filename>search</filename> utility:
</para>
<para><userinput><literallayout>$ cd /usr/local/sphinx/etc
$ /usr/local/sphinx/bin/search test</literallayout></userinput></para>
<para>
To query the index from your PHP scripts, you need to:
</para>
<orderedlist>
	<listitem>
		<para>Run the search daemon which your script will talk to:</para>
<para><userinput><literallayout>$ cd /usr/local/sphinx/etc
$ /usr/local/sphinx/bin/searchd</literallayout></userinput></para>
	</listitem>
	<listitem>
		<para>
		Run the attached PHP API test script (to ensure that the daemon
		was succesfully started and is ready to serve the queries):
		</para>
<para><userinput><literallayout>$ cd sphinx/api
$ php test.php test</literallayout></userinput></para>
	</listitem>
	<listitem>
		<para>
		Include the API (it's located in <filename>api/sphinxapi.php</filename>)
		into your own scripts and use it.
		</para>
	</listitem>
</orderedlist>
<para>
Happy searching!
</para>
</sect1>


</chapter>
<chapter id="indexing"><title>Indexing</title>


<sect1 id="sources"><title>Data sources</title>
<para>
The data to be indexed can generally come from very different
sources: SQL databases, plain text files, HTML files, mailboxes,
and so on. From Sphinx point of view, the data it indexes is a
set of structured <glossterm>documents</glossterm>, each of which has the
same set of <glossterm>fields</glossterm>. This is biased towards SQL, where
each row correspond to a document, and each column to a field.
</para>
<para>
Depending on what source Sphinx should get the data from,
different code is required to fetch the data and prepare it for indexing.
This code is called <glossterm>data source driver</glossterm> (or simply
<glossterm>driver</glossterm> or <glossterm>data source</glossterm> for brevity).
</para>
<para>
At the time of this writing, there are drivers for MySQL and
PostgreSQL databases, which can connect to the database using
its native C/C++ API, run queries and fetch the data. There's
also a driver called xmlpipe, which runs a specified command
and reads the data from its <filename>stdout</filename>.
See <xref linkend="xmlpipe"/> section for the format description.
</para>
<para>
There can be as many sources per index as necessary. They will be
sequentially processed in the very same order which was specifed in
index definition. All the documents coming from those sources
will be merged as if they were coming from a single source.
</para>
</sect1>


<sect1 id="attributes"><title>Attributes</title>
<para>
Attributes are additional values associated with each document
that can be used to perform additional filtering and sorting during search.
</para>
<para>
It is often desired to additionally process full-text search results
based not only on matching document ID and its rank, but on a number
of other per-document values as well. For instance, one might need to
sort news search results by date and then relevance,
or search through products within specified price range,
or limit blog search to posts made by selected users,
or group results by month. To do that efficiently, Sphinx allows
to attach a number of additional <glossterm>attributes</glossterm>
to each document, and store their values in the full-text index.
It's then possible to use stored values to filter, sort,
or group full-text matches.
</para>
<para>Attributes, unlike the fields, are not full-text indexed. They
are stored in the index, but it is not possible to search them as full-text,
and attempting to do so results in an error.</para>
<para>For example, it is impossible to use the extended matching mode expression
<option>@column 1</option> to match documents where column is 1, if column is an
attribute, and this is still true even if the numeric digits are normally indexed.</para>
<para>Attributes can be used for filtering, though, to restrict returned
rows, as well as sorting or <link linkend="clustering">result grouping</link>;
it is entirely possible to sort results purely based on attributes, and ignore the search
relevance tools. Additionally, attributes are returned from the search daemon, while the
indexed text is not.</para>
<para>
A good example for attributes would be a forum posts table. Assume
that only title and content fields need to be full-text searchable -
but that sometimes it is also required to limit search to a certain
author or a sub-forum (ie. search only those rows that have some
specific values of author_id or forum_id columns in the SQL table);
or to sort matches by post_date column; or to group matching posts
by month of the post_date and calculate per-group match counts.
</para>
<para>
This can be achieved by specifying all the mentioned columns
(excluding title and content, that are full-text fields) as
attributes, indexing them, and then using API calls to
setup filtering, sorting, and grouping. Here as an example.
<bridgehead>Example sphinx.conf part:</bridgehead>
<programlisting>
...
sql_query = SELECT id, title, content, \
	author_id, forum_id, post_date FROM my_forum_posts
sql_attr_uint = author_id
sql_attr_uint = forum_id
sql_attr_timestamp = post_date
...
</programlisting>
<bridgehead>Example application code (in PHP):</bridgehead>
<programlisting>
// only search posts by author whose ID is 123
$cl->SetFilter ( "author_id", array ( 123 ) );

// only search posts in sub-forums 1, 3 and 7
$cl->SetFilter ( "forum_id", array ( 1,3,7 ) );

// sort found posts by posting date in descending order
$cl->SetSortMode ( SPH_SORT_ATTR_DESC, "post_date" );
</programlisting>
</para>
<para>
Attributes are named. Attribute names are case insensitive.
Attributes are <emphasis>not</emphasis> full-text indexed; they are stored in the index as is.
Currently supported attribute types are:
<itemizedlist>
<listitem>unsigned integers (1-bit to 32-bit wide);</listitem>
<listitem>UNIX timestamps;</listitem>
<listitem>floating point values (32-bit, IEEE 754 single precision);</listitem>
<listitem>string ordinals (specially computed integers);</listitem>
<listitem><link linkend="conf-sql-attr-string">strings</link> (since 1.10-beta);</listitem>
<listitem><link linkend="mva">MVA</link>, multi-value attributes (variable-length lists of 32-bit unsigned integers).</listitem>
</itemizedlist>
</para>
<para>
The complete set of per-document attribute values is sometimes
referred to as <glossterm>docinfo</glossterm>. Docinfos can either be
<itemizedlist>
<listitem>stored separately from the main full-text index data ("extern" storage, in <filename>.spa</filename> file), or</listitem>
<listitem>attached to each occurence of document ID in full-text index data ("inline" storage, in <filename>.spd</filename> file).</listitem>
</itemizedlist>
</para>
<para>
When using extern storage, a copy of <filename>.spa</filename> file
(with all the attribute values for all the documents) is kept in RAM by
<filename>searchd</filename> at all times. This is for performance reasons;
random disk I/O would be too slow. On the contrary, inline storage does not
require any additional RAM at all, but that comes at the cost of greatly
inflating the index size: remember that it copies <emphasis>all</emphasis>
attribute value <emphasis>every</emphasis> time when the document ID
is mentioned, and that is exactly as many times as there are
different keywords in the document. Inline may be the only viable
option if you have only a few attributes and need to work with big
datasets in limited RAM. However, in most cases extern storage
makes both indexing and searching <emphasis>much</emphasis> more efficient.
</para>
<para>
Search-time memory requirements for extern storage are
(1+number_of_attrs)*number_of_docs*4 bytes, ie. 10 million docs with
2 groups and 1 timestamp will take (1+2+1)*10M*4 = 160 MB of RAM.
This is <emphasis>PER DAEMON</emphasis>, not per query. <filename>searchd</filename>
will allocate 160 MB on startup, read the data and keep it shared between queries.
The children will <emphasis>NOT</emphasis> allocate any additional
copies of this data.
</para>
</sect1>


<sect1 id="mva"><title>MVA (multi-valued attributes)</title>
<para>
MVAs, or multi-valued attributes, are an important special type of per-document attributes in Sphinx.
MVAs make it possible to attach lists of values to every document.
They are useful for article tags, product categories, etc.
Filtering and group-by (but not sorting) on MVA attributes is supported.
</para>
<para>
Currently, MVA list entries are limited to unsigned 32-bit integers.
The list length is not limited, you can have an arbitrary number of values
attached to each document as long as RAM permits (<filename>.spm</filename> file
that contains the MVA values will be precached in RAM by <filename>searchd</filename>).
The source data can be taken either from a separate query, or from a document field;
see source type in <link linkend="conf-sql-attr-multi">sql_attr_multi</link>.
In the first case the query will have to return pairs of document ID and MVA values,
in the second one the field will be parsed for integer values.
There are absolutely no requirements as to incoming data order; the values will be
automatically grouped by document ID (and internally sorted within the same ID)
during indexing anyway.
</para>
<para>
When filtering, a document will match the filter on MVA attribute
if <emphasis>any</emphasis> of the values satisfy the filtering condition.
(Therefore, documents that pass through exclude filters will not
contain any of the forbidden values.)
When grouping by MVA attribute, a document will contribute to as
many groups as there are different MVA values associated with that document.
For instance, if the collection contains exactly 1 document having a 'tag' MVA
with values 5, 7, and 11, grouping on 'tag' will produce 3 groups with
'@count' equal to 1 and '@groupby' key values of 5, 7, and 11 respectively.
Also note that grouping by MVA might lead to duplicate documents in the result set:
because each document can participate in many groups, it can be chosen as the best
one in in more than one group, leading to duplicate IDs. PHP API historically
uses ordered hash on the document ID for the resulting rows; so you'll also need to use
<link linkend="api-func-setarrayresult">SetArrayResult()</link> in order
to employ group-by on MVA with PHP API.
</para>
</sect1>


<sect1 id="indexes"><title>Indexes</title>
<para>
To be able to answer full-text search queries fast, Sphinx needs
to build a special data structure optimized for such queries from
your text data. This structure is called <glossterm>index</glossterm>; and
the process of building index from text is called <glossterm>indexing</glossterm>.
</para>
<para>
Different index types are well suited for different tasks.
For example, a disk-based tree-based index would be easy to
update (ie. insert new documents to existing index), but rather
slow to search. Therefore, Sphinx architecture allows for different
<glossterm>index types</glossterm> to be implemented easily.
</para>
<para>
The only index type which is implemented in Sphinx at the moment is
designed for maximum indexing and searching speed. This comes at a cost
of updates being really slow; theoretically, it might be slower to
update this type of index than than to reindex it from scratch.
However, this very frequently could be worked around with
muiltiple indexes, see <xref linkend="live-updates"/> for details.
</para>
<para>
It is planned to implement more index types, including the
type which would be updateable in real time.
</para>
<para>
There can be as many indexes per configuration file as necessary.
<filename>indexer</filename> utility can reindex either all of them
(if <option>--all</option> option is specified), or a certain explicitly
specified subset. <filename>searchd</filename> utility will serve all
the specified indexes, and the clients can specify what indexes to
search in run time.
</para>
</sect1>


<sect1 id="data-restrictions"><title>Restrictions on the source data</title>
<para>
There are a few different restrictions imposed on the source data
which is going to be indexed by Sphinx, of which the single most
important one is:
</para>
<para><emphasis role="bold">
ALL DOCUMENT IDS MUST BE UNIQUE UNSIGNED NON-ZERO INTEGER NUMBERS (32-BIT OR 64-BIT, DEPENDING ON BUILD TIME SETTINGS).
</emphasis></para>
<para>
If this requirement is not met, different bad things can happen.
For instance, Sphinx can crash with an internal assertion while indexing;
or produce strange results when searching due to conflicting IDs.
Also, a 1000-pound gorilla might eventually come out of your
display and start throwing barrels at you. You've been warned.
</para>
</sect1>


<sect1 id="charsets"><title>Charsets, case folding, and translation tables</title>
<para>
When indexing some index, Sphinx fetches documents from
the specified sources, splits the text into words, and does
case folding so that "Abc", "ABC" and "abc" would be treated
as the same word (or, to be pedantic, <glossterm>term</glossterm>).
</para>
<para>
To do that properly, Sphinx needs to know
<itemizedlist>
<listitem>what encoding is the source text in;</listitem>
<listitem>what characters are letters and what are not;</listitem>
<listitem>what letters should be folded to what letters.</listitem>
</itemizedlist>
This should be configured on a per-index basis using
<option><link linkend="conf-charset-type">charset_type</link></option> and 
<option><link linkend="conf-charset-table">charset_table</link></option> options.
<option><link linkend="conf-charset-type">charset_type</link></option>
specifies whether the document encoding is single-byte (SBCS) or UTF-8.
<option><link linkend="conf-charset-table">charset_table</link></option>
specifies the table that maps letter characters to their case
folded versions. The characters that are not in the table are considered
to be non-letters and will be treated as word separators when indexing
or searching through this index.
</para>
<para>
Note that while default tables do not include space character
(ASCII code 0x20, Unicode U+0020) as a letter, it's in fact
<emphasis>perfectly legal</emphasis> to do so. This can be
useful, for instance, for indexing tag clouds, so that space-separated
word sets would index as a <emphasis>single</emphasis> search query term.
</para>
<para>
Default tables currently include English and Russian characters.
Please do submit your tables for other languages!
</para>
</sect1>


<sect1 id="sql"><title>SQL data sources (MySQL, PostgreSQL)</title>
<para>
With all the SQL drivers, indexing generally works as follows.
<itemizedlist>
<listitem>connection to the database is established;</listitem>
<listitem>pre-query (see <xref linkend="conf-sql-query-pre"/>) is executed
	to perform any necessary initial setup, such as setting per-connection encoding with MySQL;</listitem>
<listitem>main query (see <xref linkend="conf-sql-query"/>) is executed and the rows it returns are indexed;</listitem>
<listitem>post-query (see <xref linkend="conf-sql-query-post"/>) is executed
	to perform any necessary cleanup;</listitem>
<listitem>connection to the database is closed;</listitem>
<listitem>indexer does the sorting phase (to be pedantic, index-type specific post-processing);</listitem>
<listitem>connection to the database is established again;</listitem>
<listitem>post-index query (see <xref linkend="conf-sql-query-post-index"/>) is executed
	to perform any necessary final cleanup;</listitem>
<listitem>connection to the database is closed again.</listitem>
</itemizedlist>
Most options, such as database user/host/password, are straightforward.
However, there are a few subtle things, which are discussed in more detail here.
</para>
<bridgehead id="ranged-queries">Ranged queries</bridgehead>
<para>
Main query, which needs to fetch all the documents, can impose
a read lock on the whole table and stall the concurrent queries
(eg. INSERTs to MyISAM table), waste a lot of memory for result set, etc.
To avoid this, Sphinx supports so-called <glossterm>ranged queries</glossterm>.
With ranged queries, Sphinx first fetches min and max document IDs from
the table, and then substitutes different ID intervals into main query text
and runs the modified query to fetch another chunk of documents.
Here's an example.
</para>
<example id="ex-ranged-queries"><title>Ranged query usage example</title>
<programlisting>
# in sphinx.conf

sql_query_range	= SELECT MIN(id),MAX(id) FROM documents
sql_range_step = 1000
sql_query = SELECT * FROM documents WHERE id&gt;=$start AND id&lt;=$end
</programlisting>
</example>
<para>
If the table contains document IDs from 1 to, say, 2345, then sql_query would
be run three times:
<orderedlist>
<listitem>with <option>$start</option> replaced with 1 and <option>$end</option> replaced with 1000;</listitem>
<listitem>with <option>$start</option> replaced with 1001 and <option>$end</option> replaced with 2000;</listitem>
<listitem>with <option>$start</option> replaced with 2000 and <option>$end</option> replaced with 2345.</listitem>
</orderedlist>
Obviously, that's not much of a difference for 2000-row table,
but when it comes to indexing 10-million-row MyISAM table,
ranged queries might be of some help.
</para>
<bridgehead><option>sql_post</option> vs. <option>sql_post_index</option></bridgehead>
<para>
The difference between post-query and post-index query is in that post-query
is run immediately when Sphinx received all the documents, but further indexing
<emphasis role="bold">may</emphasis> still fail for some other reason. On the contrary,
by the time the post-index query gets executed, it is <emphasis role="bold">guaranteed</emphasis>
that the indexing was succesful. Database connection is dropped and re-established
because sorting phase can be very lengthy and would just timeout otherwise.
</para>
</sect1>


<sect1 id="xmlpipe"><title>xmlpipe data source</title>
<para>
xmlpipe data source was designed to enable users to plug data into
Sphinx without having to implement new data sources drivers themselves.
It is limited to 2 fixed fields and 2 fixed attributes, and is deprecated
in favor of <xref linkend="xmlpipe2"/> now. For new streams, use xmlpipe2.
</para>
<para>
To use xmlpipe, configure the data source in your configuration file
as follows:
<programlisting>
source example_xmlpipe_source
{
    type = xmlpipe
    xmlpipe_command = perl /www/mysite.com/bin/sphinxpipe.pl
}
</programlisting>
The <filename>indexer</filename> will run the command specified
in <option><link linkend="conf-xmlpipe-command">xmlpipe_command</link></option>,
and then read, parse and index the data it prints to <filename>stdout</filename>.
More formally, it opens a pipe to given command and then reads
from that pipe.
</para>
<para>
indexer will expect one or more documents in custom XML format.
Here's the example document stream, consisting of two documents:
<example id="ex-xmlpipe-document"><title>XMLpipe document stream</title>
<programlisting>
&lt;document&gt;
&lt;id&gt;123&lt;/id&gt;
&lt;group&gt;45&lt;/group&gt;
&lt;timestamp&gt;1132223498&lt;/timestamp&gt;
&lt;title&gt;test title&lt;/title&gt;
&lt;body&gt;
this is my document body
&lt;/body&gt;
&lt;/document&gt;

&lt;document&gt;
&lt;id&gt;124&lt;/id&gt;
&lt;group&gt;46&lt;/group&gt;
&lt;timestamp&gt;1132223498&lt;/timestamp&gt;
&lt;title&gt;another test&lt;/title&gt;
&lt;body&gt;
this is another document
&lt;/body&gt;
&lt;/document&gt;
</programlisting>
</example>
</para>
<para>
Legacy xmlpipe legacy driver uses a builtin parser
which is pretty fast but really strict and does not actually
fully support XML. It requires that all the fields <emphasis>must</emphasis>
be present, formatted <emphasis>exactly</emphasis> as in this example, and
occur <emphasis>exactly</emphasis> in the same order. The only optional
field is <option>timestamp</option>; it defaults to 1.
</para>
</sect1>


<sect1 id="xmlpipe2"><title>xmlpipe2 data source</title>
<para>
xmlpipe2 lets you pass arbitrary full-text and attribute data to Sphinx
in yet another custom XML format. It also allows to specify the schema
(ie. the set of fields and attributes) either in the XML stream itself,
or in the source settings.
</para>
<para>
When indexing xmlpipe2 source, indexer runs the given command, opens
a pipe to its stdout, and expects well-formed XML stream. Here's sample
stream data:
<example id="ex-xmlpipe2-document"><title>xmlpipe2 document stream</title>
<programlisting>
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;sphinx:docset&gt;

&lt;sphinx:schema&gt;
&lt;sphinx:field name="subject"/&gt; 
&lt;sphinx:field name="content"/&gt;
&lt;sphinx:attr name="published" type="timestamp"/&gt;
&lt;sphinx:attr name="author_id" type="int" bits="16" default="1"/&gt;
&lt;/sphinx:schema&gt;

&lt;sphinx:document id="1234"&gt;
&lt;content&gt;this is the main content &lt;![CDATA[[and this &lt;cdata&gt; entry
must be handled properly by xml parser lib]]&gt;&lt;/content&gt;
&lt;published&gt;1012325463&lt;/published&gt;
&lt;subject&gt;note how field/attr tags can be
in &lt;b class="red"&gt;randomized&lt;/b&gt; order&lt;/subject&gt;
&lt;misc&gt;some undeclared element&lt;/misc&gt;
&lt;/sphinx:document&gt;

&lt;sphinx:document id="1235"&gt;
&lt;subject&gt;another subject&lt;/subject&gt;
&lt;content&gt;here comes another document, and i am given to understand,
that in-document field order must not matter, sir&lt;/content&gt;
&lt;published&gt;1012325467&lt;/published&gt;
&lt;/sphinx:document&gt;

&lt;!-- ... even more sphinx:document entries here ... --&gt;

&lt;sphinx:killlist&gt;
&lt;id&gt;1234&lt;/id&gt;
&lt;id&gt;4567&lt;/id&gt;
&lt;/sphinx:killlist&gt;

&lt;/sphinx:docset&gt;
</programlisting>
</example>
</para>
<para>
Arbitrary fields and attributes are allowed.
They also can occur in the stream in arbitrary order within each document; the order is ignored.
There is a restriction on maximum field length; fields longer than 2 MB will be truncated to 2 MB (this limit can be changed in the source).
</para>
<para>
The schema, ie. complete fields and attributes list, must be declared
before any document could be parsed. This can be done either in the
configuration file using <option>xmlpipe_field</option> and <option>xmlpipe_attr_XXX</option>
settings, or right in the stream using &lt;sphinx:schema&gt; element.
&lt;sphinx:schema&gt; is optional. It is only allowed to occur as the very
first sub-element in &lt;sphinx:docset&gt;. If there is no in-stream
schema definition, settings from the configuration file will be used.
Otherwise, stream settings take precedence.
</para>
<para>
Unknown tags (which were not declared neither as fields nor as attributes)
will be ignored with a warning. In the example above, &lt;misc&gt; will be ignored.
All embedded tags and their attributes (such as &lt;b&gt; in &lt;subject&gt;
in the example above) will be silently ignored.
</para>
<para>
Support for incoming stream encodings depends on whether <filename>iconv</filename>
is installed on the system. xmlpipe2 is parsed using <filename>libexpat</filename>
parser that understands US-ASCII, ISO-8859-1, UTF-8 and a few UTF-16 variants
natively. Sphinx <filename>configure</filename> script will also check
for <filename>libiconv</filename> presence, and utilize it to handle
other encodings. <filename>libexpat</filename> also enforces the
requirement to use UTF-8 charset on Sphinx side, because the
parsed data it returns is always in UTF-8.
<!-- TODO: check this vs latin-1 -->
</para>
<para>
XML elements (tags) recognized by xmlpipe2 (and their attributes where applicable) are:
<variablelist>
<varlistentry>
	<term>sphinx:docset</term>
	<listitem>Mandatory top-level element, denotes and contains xmlpipe2 document set.</listitem>
</varlistentry>
<varlistentry>
	<term>sphinx:schema</term>
	<listitem>Optional element, must either occur as the very first child
		of sphinx:docset, or never occur at all. Declares the document schema.
		Contains field and attribute declarations. If present, overrides
		per-source settings from the configuration file.
	</listitem>
</varlistentry>
<varlistentry>
	<term>sphinx:field</term>
	<listitem>Optional element, child of sphinx:schema. Declares a full-text field.
		Known attributes are:
		<itemizedlist>
		<listitem>"name", specifies the XML element name that will be treated as a full-text field in the subsequent documents.</listitem>
		<listitem>"attr", specifies whether to also index this field as a string or word count attribute. Possible values are "string" and "wordcount". Introduced in version 1.10-beta.</listitem>
		</itemizedlist>
	</listitem>
</varlistentry>
<varlistentry>
	<term>sphinx:attr</term>
	<listitem>Optional element, child of sphinx:schema. Declares an attribute.
		Known attributes are:
		<itemizedlist>
		<listitem>"name", specifies the element name that should be treated as an attribute in the subsequent documents.</listitem>
		<listitem>"type", specifies the attribute type. Possible values are "int", "timestamp", "str2ordinal", "bool", "float" and "multi".</listitem>
		<listitem>"bits", specifies the bit size for "int" attribute type. Valid values are 1 to 32.</listitem>
		<listitem>"default", specifies the default value for this attribute that should be used if the attribute's element is not present in the document.</listitem>
		</itemizedlist>
	</listitem>
</varlistentry>
<varlistentry>
	<term>sphinx:document</term>
	<listitem>Mandatory element, must be a child of sphinx:docset.
		Contains arbitrary other elements with field and attribute values
		to be indexed, as declared either using sphinx:field and sphinx:attr
		elements or in the configuration file. The only known attribute
		is "id" that must contain the unique integer document ID.
	</listitem>
</varlistentry>
<varlistentry>
	<term>sphinx:killlist</term>
	<listitem>Optional element, child of sphinx:docset.
		Contains a number of "id" elements whose contents are document IDs
		to be put into a <link linkend="conf-sql-query-killlist">kill-list</link> for this index.
	</listitem>
</varlistentry>
</variablelist>
</para>
</sect1>


<sect1 id="live-updates"><title>Live index updates</title>
<para>
There are two major approaches to maintaining the full-text index
contents up to date. Note, however, that both these approaches deal
with the task of <emphasis>full-text data updates</emphasis>, and not
attribute updates. Instant attribute updates are supported since 
version 0.9.8. Refer to <link linkend="api-func-updateatttributes">UpdateAttributes()</link>
API call description for details.
</para>
<para>
First, you can use disk-based indexes, partition them manually,
and only rebuild the smaller partitions (so-called "deltas") frequently.
By minimizing the rebuild size, you can reduce the average indexing lag
to something as low as 30-60 seconds. This approach was the the only one
available in versions 0.9.x. On huge collections it actually might be
the most efficient one. Refer to <xref linkend="delta-updates"/>
for details.
</para>
<para>
Second, versions 1.x (starting with 1.10-beta) add support for so-called
real-time indexes (RT indexes for short) that on-the-fly updates of the
full-text data. Updates on a RT index can appear in the search results in 
1-2 milliseconds, ie. 0.001-0.002 seconds. However, RT index are less 
efficient for bulk indexing huge amounts of data. Refer to 
<xref linkend="rt-indexes"/> for details.
</para>
</sect1>


<sect1 id="delta-updates"><title>Delta index updates</title>
<para>
There's a frequent situation when the total dataset is too big
to be reindexed from scratch often, but the amount of new records
is rather small. Example: a forum with a 1,000,000 archived posts,
but only 1,000 new posts per day.
</para>
<para>
In this case, "live" (almost real time) index updates could be
implemented using so called "main+delta" scheme.
</para>
<para>
The idea is to set up two sources and two indexes, with one
"main" index for the data which only changes rarely (if ever),
and one "delta" for the new documents. In the example above,
1,000,000 archived posts would go to the main index, and newly
inserted 1,000 posts/day would go to the delta index. Delta index
could then be reindexed very frequently, and the documents can
be made available to search in a matter of minutes.
</para>
<para>
Specifying which documents should go to what index and
reindexing main index could also be made fully automatical.
One option would be to make a counter table which would track
the ID which would split the documents, and update it
whenever the main index is reindexed.
<example id="ex-live-updates">
<title>Fully automated live updates</title>
<programlisting>
# in MySQL
CREATE TABLE sph_counter
(
    counter_id INTEGER PRIMARY KEY NOT NULL,
    max_doc_id INTEGER NOT NULL
);

# in sphinx.conf
source main
{
    # ...
    sql_query_pre = SET NAMES utf8
    sql_query_pre = REPLACE INTO sph_counter SELECT 1, MAX(id) FROM documents
    sql_query = SELECT id, title, body FROM documents \
        WHERE id&lt;=( SELECT max_doc_id FROM sph_counter WHERE counter_id=1 )
}

source delta : main
{
    sql_query_pre = SET NAMES utf8
    sql_query = SELECT id, title, body FROM documents \
        WHERE id&gt;( SELECT max_doc_id FROM sph_counter WHERE counter_id=1 )
}

index main
{
    source = main
    path = /path/to/main
    # ... all the other settings
}

# note how all other settings are copied from main,
# but source and path are overridden (they MUST be)
index delta : main
{
    source = delta
    path = /path/to/delta
}
</programlisting>
</example>
</para>
<para>
Note how we're overriding <code>sql_query_pre</code> in the delta source.
We need to explicitly have that override. Otherwise <code>REPLACE</code> query
would be run when indexing delta source too, effectively nullifying it. However,
when we issue the directive in the inherited source for the first time, it removes 
<emphasis>all</emphasis> inherited values, so the encoding setup is also lost.
So <code>sql_query_pre</code> in the delta can not just be empty; and we need
to issue the encoding setup query explicitly once again.
</para>
</sect1>


<sect1 id="index-merging"><title>Index merging</title>
<para>
Merging two existing indexes can be more efficient that indexing the data
from scratch, and desired in some cases (such as merging 'main' and 'delta'
indexes instead of simply reindexing 'main' in 'main+delta' partitioning
scheme). So <filename>indexer</filename> has an option to do that.
Merging the indexes is normally faster than reindexing but still
<emphasis>not</emphasis> instant on huge indexes. Basically,
it will need to read the contents of both indexes once and write
the result once. Merging 100 GB and 1 GB index, for example,
will result in 202 GB of IO (but that's still likely less than
the indexing from scratch requires).
</para>
<para>
The basic command syntax is as follows:
<programlisting>
indexer --merge DSTINDEX SRCINDEX [--rotate]
</programlisting>
Only the DSTINDEX index will be affected: the contents of SRCINDEX will be merged into it.
<option>--rotate</option> switch will be required if DSTINDEX is already being served by <filename>searchd</filename>.
The initially devised usage pattern is to merge a smaller update from SRCINDEX into DSTINDEX.
Thus, when merging the attributes, values from SRCINDEX will win if duplicate document IDs are encountered.
Note, however, that the "old" keywords will <emphasis>not</emphasis> be automatically removed in such cases.
For example, if there's a keyword "old" associated with document 123 in DSTINDEX, and a keyword "new" associated
with it in SRCINDEX, document 123 will be found by <emphasis>both</emphasis> keywords after the merge.
You can supply an explicit condition to remove documents from DSTINDEX to mitigate that;
the relevant switch is <option>--merge-dst-range</option>:
<programlisting>
indexer --merge main delta --merge-dst-range deleted 0 0
</programlisting>
This switch lets you apply filters to the destination index along with merging.
There can be several filters; all of their conditions must be met in order
to include the document in the resulting mergid index. In the example above,
the filter passes only those records where 'deleted' is 0, eliminating all
records that were flagged as deleted (for instance, using
<link linkend="api-func-updateatttributes">UpdateAttributes()</link> call).
</para>
</sect1>


</chapter>
<chapter id="rt-indexes"><title>Real-time indexes</title>
<para>
Real-time indexes (or RT indexes for brevity) are a new backend
that lets you insert, update, or delete documents (rows) on the fly.
RT indexes were added in version 1.10-beta.  While querying of RT indexes
is possible using any of the SphinxAPI, SphinxQL, or SphinxSE, updating
them is only possible via SphinxQL at the moment.  Full SphinxQL
reference is available in <xref linkend="sphinxql-reference"/>.
</para>


<sect1 id="rt-overview"><title>RT indexes overview</title>
<para>
RT indexes should be declared in <filename>sphinx.conf</filename>,
just as every other index type. Notable differences from the regular, 
disk-based indexes are that a) data sources are not required and ignored,
and b) you should explicitly enumerate all the text fields, not just
attributes. Here's an example:
</para>
<example id="ex-rt-updates">
<title>RT index declaration</title>
<programlisting>
index rt
{
	type = rt
	path = /usr/local/sphinx/data/rt
	rt_field = title
	rt_field = content
	rt_attr_uint = gid
}
</programlisting>
</example>
<para>
RT INDEXES ARE CURRENTLY (AS OF VERSION 1.10-beta) A WORK IN PROGRESS.
Therefore, they might lack certain features: for instance, prefix/infix
indexing, MVA attributes, etc are not supported yet.  There also might be
performance and stability issues.  However, all the regular indexing features
and most of the searching features are already in place, our internal
testing passes, and last but not least a number of production instances
are already using RT indexes with good results.
</para>
<para>
RT index can be accessed using MySQL protocol. INSERT, REPLACE, DELETE, and
SELECT statements against RT index are supported. For instance, this
is an example session with the sample index above:
</para>
<programlisting>
$ mysql -h 127.0.0.1 -P 9306
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1
Server version: 1.10-dev (r2153)

Type 'help;' or '\h' for help. Type '\c' to clear the buffer.

mysql> INSERT INTO rt VALUES ( 1, 'first record', 'test one', 123 );
Query OK, 1 row affected (0.05 sec)

mysql> INSERT INTO rt VALUES ( 2, 'second record', 'test two', 234 );
Query OK, 1 row affected (0.00 sec)

mysql> SELECT * FROM rt;
+------+--------+------+
| id   | weight | gid  |
+------+--------+------+
|    1 |      1 |  123 |
|    2 |      1 |  234 |
+------+--------+------+
2 rows in set (0.02 sec)

mysql> SELECT * FROM rt WHERE MATCH('test');
+------+--------+------+
| id   | weight | gid  |
+------+--------+------+
|    1 |   1643 |  123 |
|    2 |   1643 |  234 |
+------+--------+------+
2 rows in set (0.01 sec)

mysql> SELECT * FROM rt WHERE MATCH('@title test');
Empty set (0.00 sec)
</programlisting>
<para>
Both partial and batch INSERT syntaxes are supported, ie. 
you can specify a subset of columns, and insert several rows at a time. 
Deletions are also possible using DELETE statement; the only currently
supported syntax is DELETE FROM &lt;index&gt; WHERE id=&lt;id&gt;.
REPLACE is also supported, enabling you to implement updates.
</para>
<programlisting>
mysql> INSERT INTO rt ( id, title ) VALUES ( 3, 'third row' ), ( 4, 'fourth entry' );
Query OK, 2 rows affected (0.01 sec)

mysql> SELECT * FROM rt;
+------+--------+------+
| id   | weight | gid  |
+------+--------+------+
|    1 |      1 |  123 |
|    2 |      1 |  234 |
|    3 |      1 |    0 |
|    4 |      1 |    0 |
+------+--------+------+
4 rows in set (0.00 sec)

mysql> DELETE FROM rt WHERE id=2;
Query OK, 0 rows affected (0.00 sec)

mysql> SELECT * FROM rt WHERE MATCH('test');
+------+--------+------+
| id   | weight | gid  |
+------+--------+------+
|    1 |   1500 |  123 |
+------+--------+------+
1 row in set (0.00 sec)

mysql> INSERT INTO rt VALUES ( 1, 'first record on steroids', 'test one', 123 );
ERROR 1064 (42000): duplicate id '1'

mysql> REPLACE INTO rt VALUES ( 1, 'first record on steroids', 'test one', 123 );
Query OK, 1 row affected (0.01 sec)

mysql> SELECT * FROM rt WHERE MATCH('steroids');
+------+--------+------+
| id   | weight | gid  |
+------+--------+------+
|    1 |   1500 |  123 |
+------+--------+------+
1 row in set (0.01 sec)
</programlisting>
<para>
Data stored in RT index should survive clean shutdown. When binary logging
is enabled, it should also survive crash and/or dirty shutdown, and recover
on subsequent startup.
</para>
</sect1>


<sect1 id="rt-caveats"><title>Known caveats with RT indexes</title>
<para>
As of 1.10-beta, RT indexes are a beta quality feature: while no major,
showstopper-class issues are known, there still are a few known usage quirks.
Those quirks are listed in this section.
</para>
<itemizedlist>
<listitem>Prefix and infix indexing are not supported yet.</listitem>
<listitem>MVAs are not supported yet.</listitem>
<listitem>Disk chunks optimization routine is not implemented yet.</listitem>
<listitem>On initial index creation, attributes are reordered by type,
in the following order: uint, bigint, float, timestamp, string. So when
using INSERT without an explicit column names list, specify all uint
column values first, then bigint, etc.</listitem>
<listitem>Default conservative RAM chunk limit (<option>rt_mem_limit</option>)
of 32M can lead to poor performance on bigger indexes, you should raise it to
256..1024M if you're planning to index gigabytes.</listitem>
<listitem>High DELETE/REPLACE rate can lead to kill-list fragmentation
and impact searching performance.</listitem>
<listitem>No transaction size limits are currently imposed;
too many concurrent INSERT/REPLACE transactions might therefore
consume a lot of RAM.</listitem>
<listitem>In case of a damaged binlog, recovery will stop on the
first damaged transaction, even though it's technically possible
to keep looking further for subsequent undamaged transactions, and
recover those. This mid-file damage case (due to flaky HDD/CDD/tape?)
is supposed to be extremely rare, though.</listitem>
<listitem>Multiple INSERTs grouped in a single transaction perform
better than equivalent single-row transactions and are recommended for
batch loading of data.</listitem>
</itemizedlist>
</sect1>


<sect1 id="rt-internals"><title>RT index internals</title>
<para>
RT index is internally chunked.  It keeps a so-called RAM chunk
that stores all the most recent changes.  RAM chunk memory usage
is rather strictly limited with per-index
<link linkend="conf-rt-mem-limit">rt_mem_limit</link> directive.
Once RAM chunk grows over this limit, a new disk chunk is created
from its data, and RAM chunk is reset.  Thus, while most changes
on the RT index will be performed in RAM only and complete instantly
(in milliseconds), those changes that overflow the RAM chunk will
stall for the duration of disk chunk creation (a few seconds).
</para>
<para>
Disk chunks are, in fact, just regular disk-based indexes.
But they're a part of an RT index and automatically managed by it,
so you need not configure nor manage them manually.  Because a new
disk chunk is created every time RT chunk overflows the limit, and
because in-memory chunk format is close to on-disk format, the disk
chunks will be approximately <option>rt_mem_limit</option> bytes
in size each.
</para>
<para>
Generally, it is better to set the limit bigger, to minimize both
the frequency of flushes, and the index fragmentation (number of disk
chunks).  For instance, on a dedicated search server that handles
a big RT index, it can be advised to set <option>rt_mem_limit</option>
to 1-3 GB.  A global limit on all indexes is also planned, but not yet
implemented yet as of 1.10-beta.
</para>
<para>
Disk chunk full-text index data can not be actually modified,
so the full-text field changes (ie. row deletions and updates)
suppress a previous row version from a disk chunk using a kill-list,
but do not actually physicall purge the data.  Therefore, on workloads
with high full-text updates ratio index might eventually get polluted
by these previous row versions, and searching performance would
degrade.  Physical index purging that would improve the performance
is planned, but not yet implemented as of 1.10-beta.
</para>
<para>
Data in RAM chunk gets saved to disk on clean daemon shutdown, and
then loaded back on startup.  However, on daemon or server crash,
updates from RAM chunk might be lost.  To prevent that, binary logging
of transactions can be used; see <xref linkend="rt-binlog"/> for details.
</para>
<para>
Full-text changes in RT index are transactional.  They are stored
in a per-thread accumulator until COMMIT, then applied at once.
Bigger batches per single COMMIT should result in faster indexing.
</para>
</sect1>


<sect1 id="rt-binlog"><title>Binary logging</title>
<para>
Binary logs are essentially a recovery mechanism.  With binary logs
enabled, <filename>searchd</filename> writes every given transaction
to the binlog file, and uses that for recovery after an unclean shutdown.
On clean shutdown, RAM chunks are saved to disk, and then all the binlog
files are unlinked.
</para>
<para>
During normal operation, a new binlog file will be opened every time
when <option>binlog_max_log_size</option> limit (which defaults to 128M)
is reached.  Older, already closed binlog files are kept until all of the
transactions stored in them (from all indexes) are flushed as a disk chunk.
Setting the limit to 0 pretty much prevents binlog from being unlinked
at all while <filename>searchd</filename> is running; however, it will
still be unlinked on clean shutdown.
</para>
<para>
There are 3 different binlog flushing strategies, controlled by
<link linkend="conf-binlog-flush">binlog_flush</link> directive
which takes the values of 0, 1, or 2. 0 means to flush the log
to OS and sync it to disk every second; 1 means flush and sync
every transaction; and 2 (the default mode) means flush every
transaction but sync every second. Sync is relatively slow because
it has to perform physical disk writes, so mode 1 is the safest
(every committed transaction is guaranteed to be written on disk)
but the slowest. Flushing log to OS prevents from data loss on
<filename>searchd</filename> crashes but not system crashes.
Mode 2 is the default.
</para>
<para>
On recovery after an unclean shutdown, binlogs are replayed
and all logged transactions since the last good on-disk state
are restored. Transactions are checksummed so in case of binlog
file corruption garbage data will <b>not</b> be replayed; such
a broken transaction will be detected and, currently, will stop
replay. Transactions also start with a magic marker and timestamped,
so in case of binlog damage in the middle of the file, it's technically
possible to skip broken transactions and keep replaying from the next
good one, and/or it's possible to replay transactions until a given
timestamp (point-in-time recovery), but none of that is implemented yet
as of 1.10-beta.
</para>
<para>
One unwanted side effect of binlogs is that activel updating
a small RT index that fully fits into a RAM chunk part will lead
to an ever-growing binlog that can never be unlinked until clean
shutdown. Binlogs are essentially append-only deltas against
the last known good saved state on disk, and unless RAM chunk
gets saved, they can not be unlinked. An ever-growing binlog
is not very good for disk use and crash recovery time. Starting
with 1.11-beta you can configure <filename>searchd</filename>
to perform a periodic RAM chunk flush to fix that problem
using a <link linkend="conf-rt-flush-period">rt_flush_period</link>
directive. With periodic flushes enabled, <filename>searchd</filename>
will keep a separate thread, checking whether RT indexes RAM
chunks need to be written back to disk. Once that happens,
the respective binlogs can be (and are) safely unlinked.
</para>
<para>
Note that <code>rt_flush_period</code> only controls the
frequency at which the <emphasis>checks</emphasis> happen.
There are no <emphasis>guarantees</emphasis> that the
particular RAM chunk will get saved. For instance, it does
not make sense to regularly re-save a huge RAM chunk that
only gets a few rows worh of updates. The search daemon
determine whether to actually perform the flush with a few
heuristics.
</para>
</sect1>


</chapter>
<chapter id="searching"><title>Searching</title>


<!-- TODO 
<sect1 id="searching-overview"><title>Overview</title>
</sect1>
-->


<sect1 id="matching-modes"><title>Matching modes</title>
<para>
There are the following matching modes available:
<itemizedlist>
<listitem>SPH_MATCH_ALL, matches all query words (default mode);</listitem>
<listitem>SPH_MATCH_ANY, matches any of the query words;</listitem>
<listitem>SPH_MATCH_PHRASE, matches query as a phrase, requiring perfect match;</listitem>
<listitem>SPH_MATCH_BOOLEAN, matches query as a boolean expression (see <xref linkend="boolean-syntax"/>);</listitem>
<listitem>SPH_MATCH_EXTENDED, matches query as an expression in Sphinx internal query language
	(see <xref linkend="extended-syntax"/>). As of 0.9.9, this has been superceded by SPH_MATCH_EXTENDED2,
	providing additional functionality and better performance. The ident is retained for legacy application code
	that will continue to be compatible once Sphinx and its components, including the API, are upgraded.</listitem>
<listitem>SPH_MATCH_EXTENDED2, matches query using the second version of the Extended matching mode.</listitem>
<listitem>SPH_MATCH_FULLSCAN, matches query, forcibly using the "full scan" mode as below.
	NB, any query terms will be ignored, such that filters, filter-ranges and grouping 
	will still be applied, but no text-matching.</listitem>
</itemizedlist>
</para>
<para>
The SPH_MATCH_FULLSCAN mode will be automatically activated in place of the specified matching mode when the following conditions are met:
<orderedlist>
<listitem>The query string is empty (ie. its length is zero).</listitem>
<listitem><link linkend="conf-docinfo">docinfo</link> storage is set to <code>extern</code>.</listitem>
</orderedlist>
In full scan mode, all the indexed documents will be considered as matching.
Such queries will still apply filters, sorting, and group by, but will not perform any full-text searching.
This can be useful to unify full-text and non-full-text searching code, or to offload SQL server
(there are cases when Sphinx scans will perform better than analogous MySQL queries).
An example of using the full scan mode might be to find posts in a forum.
By selecting the forum's user ID via <code>SetFilter()</code> but not actually providing any search text,
Sphinx will match every document (i.e. every post) where <code>SetFilter()</code> would match -
in this case providing every post from that user. By default this will be ordered by relevancy,
followed by Sphinx document ID in ascending order (earliest first).
</para>
</sect1>

<sect1 id="boolean-syntax"><title>Boolean query syntax</title>
<para>
Boolean queries allow the following special operators to be used:
<itemizedlist>
<listitem>explicit operator AND: <programlisting>hello &amp; world</programlisting></listitem>
<listitem>operator OR: <programlisting>hello | world</programlisting></listitem>
<listitem>operator NOT:
<programlisting>
hello -world
hello !world
</programlisting>
</listitem>
<listitem>grouping: <programlisting>( hello world )</programlisting></listitem>
</itemizedlist>
Here's an example query which uses all these operators:
<example id="ex-boolean-query"><title>Boolean query example</title>
<programlisting>
( cat -dog ) | ( cat -mouse)
</programlisting>
</example>
</para>
<para>
There always is implicit AND operator, so "hello world" query actually
means "hello &amp; world".
</para>
<para>
OR operator precedence is higher than AND, so "looking for cat | dog | mouse"
means "looking for ( cat | dog | mouse )" and <emphasis>not</emphasis>
"(looking for cat) | dog | mouse".
</para>
<para>
Queries like "-dog", which implicitly include all documents from the
collection, can not be evaluated. This is both for technical and performance
reasons. Technically, Sphinx does not always keep a list of all IDs.
Performance-wise, when the collection is huge (ie. 10-100M documents),
evaluating such queries could take very long.
</para>
</sect1>


<sect1 id="extended-syntax"><title>Extended query syntax</title>
<para>
The following special operators and modifiers can be used when using the extended matching mode:
<itemizedlist>
<listitem>operator OR: <programlisting>hello | world</programlisting></listitem>
<listitem>operator NOT:
<programlisting>
hello -world
hello !world
</programlisting>
</listitem>
<listitem>field search operator: <programlisting>@title hello @body world</programlisting></listitem>
<listitem>field position limit modifier (introduced in version 0.9.9-rc1): <programlisting>@body[50] hello</programlisting></listitem>
<listitem>multiple-field search operator: <programlisting>@(title,body) hello world</programlisting></listitem>
<listitem>all-field search operator: <programlisting>@* hello</programlisting></listitem>
<listitem>phrase search operator: <programlisting>"hello world"</programlisting></listitem>
<listitem>proximity search operator: <programlisting>"hello world"~10</programlisting></listitem>
<listitem>quorum matching operator: <programlisting>"the world is a wonderful place"/3</programlisting></listitem>
<listitem>strict order operator (aka operator "before"): <programlisting>aaa &lt;&lt; bbb &lt;&lt; ccc</programlisting></listitem>
<listitem>exact form modifier (introduced in version 0.9.9-rc1): <programlisting>raining =cats and =dogs</programlisting></listitem>
<listitem>field-start and field-end modifier (introduced in version 0.9.9-rc2): <programlisting>^hello world$</programlisting></listitem>
<listitem>NEAR, generalized proximity operator (introduced in version 1.11-beta): <programlisting>hello NEAR/3 world NEAR/4 "my test"</programlisting></listitem>
<listitem>SENTENCE operator (introduced in version 1.11-beta): <programlisting>all SENTENCE words SENTENCE "in one sentence"</programlisting></listitem>
<listitem>PARAGRAPH operator (introduced in version 1.11-beta): <programlisting>"Bill Gates" PARAGRAPH "Steve Jobs"</programlisting></listitem>
<listitem>zone limit operator: <programlisting>ZONE:(h3,h4) only in these titles</programlisting></listitem>
</itemizedlist>

Here's an example query that uses some of these operators:
<example id="ex-extended-query"><title>Extended matching mode: query example</title>
<programlisting>
"hello world" @title "example program"~5 @body python -(php|perl) @* code
</programlisting>
</example>
The full meaning of this search is:

<itemizedlist>
<listitem>Find the words 'hello' and 'world' adjacently in any field in a document;</listitem>
<listitem>Additionally, the same document must also contain the words 'example' and 'program'
	in the title field, with up to, but not including, 10 words between the words in question;
	(E.g. "example PHP program" would be matched however "example script to introduce outside data
	into the correct context for your program" would not because two terms have 10 or more words between them)</listitem>
<listitem>Additionally, the same document must contain the word 'python' in the body field, but not contain either 'php' or 'perl';</listitem>
<listitem>Additionally, the same document must contain the word 'code' in any field.</listitem>
</itemizedlist>
</para>
<para>
There always is implicit AND operator, so "hello world" means that
both "hello" and "world" must be present in matching document.
</para>
<para>
OR operator precedence is higher than AND, so "looking for cat | dog | mouse"
means "looking for ( cat | dog | mouse )" and <emphasis>not</emphasis>
"(looking for cat) | dog | mouse".
</para>
<para>
Field limit operator limits subsequent searching to a given field.
Normally, query will fail with an error message if given field name does not exist
in the searched index. However, that can be suppressed by specifying "@@relaxed"
option at the very beginning of the query:
<programlisting>
@@relaxed @nosuchfield my query
</programlisting>
This can be helpful when searching through heterogeneous indexes with
different schemas.
</para>
<para>
Field position limit, introduced in version 0.9.9-rc1, additionaly restricts the searching
to first N position within given field (or fields). For example, "@body[50] hello" will
<b>not</b> match the documents where the keyword 'hello' occurs at position 51 and below
in the body.
</para>
<para>
Proximity distance is specified in words, adjusted for word count, and
applies to all words within quotes. For instance, "cat dog mouse"~5 query
means that there must be less than 8-word span which contains all 3 words,
ie. "CAT aaa bbb ccc DOG eee fff MOUSE" document will <emphasis>not</emphasis>
match this query, because this span is exactly 8 words long.
</para>
<para>
Quorum matching operator introduces a kind of fuzzy matching.
It will only match those documents that pass a given threshold of given words.
The example above ("the world is a wonderful place"/3) will match all documents
that have at least 3 of the 6 specified words.
</para>
<para>
Strict order operator (aka operator "before"), introduced in version 0.9.9-rc2,
will match the document only if its argument keywords occur in the document
exactly in the query order. For instance, "black &lt;&lt; cat" query (without
quotes) will match the document "black and white cat" but <emphasis>not</emphasis>
the "that cat was black" document. Order operator has the lowest priority.
It can be applied both to just keywords and more complex expressions,
ie. this is a valid query:
<programlisting>
(bag of words) &lt;&lt; "exact phrase" &lt;&lt; red|green|blue
</programlisting>
</para>
<para>
Exact form keyword modifier, introduced in version 0.9.9-rc1, will match the document only if the keyword occurred
in exactly the specified form. The default behaviour is to match the document
if the stemmed keyword matches. For instance, "runs" query will match both
the document that contains "runs" <emphasis>and</emphasis> the document that
contains "running", because both forms stem to just "run" - while "=runs"
query will only match the first document. Exact form operator requires
<link linkend="conf-index-exact-words">index_exact_words</link> option to be enabled.
This is a modifier that affects the keyword and thus can be used within
operators such as phrase, proximity, and quorum operators.
</para>
<para>
Field-start and field-end keyword modifiers, introduced in version 0.9.9-rc2,
will make the keyword match only if it occurred at the very start or the very end
of a fulltext field, respectively. For instance, the query "^hello world$"
(with quotes and thus combining phrase operator and start/end modifiers)
will only match documents that contain at least one field that has exactly
these two keywords.
</para>
<para>
Starting with 0.9.9-rc1, arbitrarily nested brackets and negations are allowed.
However, the query must be possible to compute without involving an implicit
list of all documents:
<programlisting>
// correct query
aaa -(bbb -(ccc ddd))

// queries that are non-computable
-aaa
aaa | -bbb
</programlisting>
</para>
<para>
<b>NEAR operator</b>, added in 1.11-beta, is a generalized version
of a proximity operator. The syntax is <code>NEAR/N</code>, it is
case-sensitive, and no spaces are allowed beetwen the NEAR keyword,
the slash sign, and the distance value.
</para>
<para>
The original proximity operator only worked on sets of keywords.
NEAR is more generic and can accept arbitrary subexpressions as
its two arguments, matching the document when both subexpressions
are found within N words of each other, no matter in which order.
NEAR is left associative and has the same (lowest) precedence
as BEFORE.
</para>
<para>
You should also note how a <code>(one NEAR/7 two NEAR/7 three)</code>
query using NEAR is not really equivalent to a
<code>("one two three"~7)</code> one using keyword proximity operator.
The difference here is that the proximity operator allows for up to
6 non-matching words between all the 3 matching words, but the version
with NEAR is less restrictive: it would allow for up to 6 words between
'one' and 'two' and then for up to 6 more between that two-word
matching and a 'three' keyword.
</para>
<para>
<b>SENTENCE and PARAGRAPH operators</b>, added in 1.11-beta,
matches the document when both its arguments are within the same
sentence or the same paragraph of text, respectively. The arguments
can be either keywords, or phrases, or the instances of the same
operator. Here are a few examples:
<programlisting>
one SENTENCE two
one SENTENCE "two three"
one SENTENCE "two three" SENTENCE four
</programlisting>
The order of the arguments within the sentence or paragraph
does not matter. These operators only work on indexes built
with <link linkend="conf-index-sp">index_sp</link> (sentence
and paragraph indexing feature) enabled, and revert to a mere
AND otherwise. Refer to the <code>index_sp</code> directive
documentation for the notes on what's considered a sentence
and a paragraph. 
</para>
<para>
<b>ZONE limit operator</b>, added in 1.11-beta, is quite similar
to field limit operator, but restricts matching to a given in-field
zone or a list of zones. Note that the subsequent subexpressions
are <emphasis>not</emphasis> required to match in a single contiguous
span of a given zone, and may match in multiple spans.
For instance, <code>(ZONE:th hello world)</code> query
<emphasis>will</emphasis> match this example document:
<programlisting>
&lt;th&gt;Table 1. Local awareness of Hello Kitty brand.&lt;/th&gt;
.. some table data goes here ..
&lt;th&gt;Table 2. World-wide brand awareness.&lt;/th&gt;
</programlisting>
ZONE operator affects the query until the next
field or ZONE limit operator, or the closing parenthesis.
It only works on the indexes built with zones support
(see <xref linkend="conf-index-zones"/>) and will be ignored
otherwise.
</para>
</sect1>


<sect1 id="weighting"><title>Weighting</title>
<para>
Specific weighting function (currently) depends on the search mode.
</para>
<para>
There are these major parts which are used in the weighting functions:
<orderedlist>
<listitem>phrase rank,</listitem>
<listitem>statistical rank.</listitem>
</orderedlist>
</para>
<para>
Phrase rank is based on a length of longest common subsequence
(LCS) of search words between document body and query phrase. So if
there's a perfect phrase match in some document then its phrase rank
would be the highest possible, and equal to query words count.
</para>
<para>
Statistical rank is based on classic BM25 function which only takes
word frequencies into account. If the word is rare in the whole database
(ie. low frequency over document collection) or mentioned a lot in specific
document (ie. high frequency over matching document), it receives more weight.
Final BM25 weight is a floating point number between 0 and 1.
</para>
<para>
In all modes, per-field weighted phrase ranks are computed as
a product of LCS multiplied by per-field weight speficifed by user.
Per-field weights are integer, default to 1, and can not be set
lower than 1.
</para>
<para>
In SPH_MATCH_BOOLEAN mode, no weighting is performed at all, every match weight
is set to 1.
</para>
<para>
In SPH_MATCH_ALL and SPH_MATCH_PHRASE modes, final weight is a sum of weighted phrase ranks.
</para>
<para>
In SPH_MATCH_ANY mode, the idea is essentially the same, but it also
adds a count of matching words in each field. Before that, weighted
phrase ranks are additionally mutliplied by a value big enough to
guarantee that higher phrase rank in <emphasis role="bold">any</emphasis> field will make the
match ranked higher, even if it's field weight is low.
</para>
<para>
In SPH_MATCH_EXTENDED mode, final weight is a sum of weighted phrase
ranks and BM25 weight, multiplied by 1000 and rounded to integer.
</para>
<para>
This is going to be changed, so that MATCH_ALL and MATCH_ANY modes
use BM25 weights as well. This would improve search results in those
match spans where phrase ranks are equal; this is especially useful
for 1-word queries.
</para>
<para>
The key idea (in all modes, besides boolean) is that better subphrase
matches are ranked higher, and perfect matches are pulled to the top. Author's
experience is that this phrase proximity based ranking provides noticeably
better search quality than any statistical scheme alone (such as BM25,
which is commonly used in other search engines).
</para>
</sect1>


<sect1 id="expressions">
<title>Expressions, functions, and operators</title>
<para>
Sphinx lets you use arbitrary arithmetic expressions both via SphinxQL
and SphinxAPI, involving attribute values, internal attributes (document ID
and relevance weight), arithmetic operations, a number of built-in functions,
and user-defined functions.
This section documents the supported operators and functions.
Here's the complete reference list for quick access.
<itemizedlist>
<listitem><link linkend="expr-ari-ops">Arithmetic operators: +, -, *, /, %, DIV, MOD</link></listitem>
<listitem><link linkend="expr-comp-ops">Comparison operators: &lt;, &gt; &lt;=, &gt;=, =, &lt;&gt;</link></listitem>
<listitem><link linkend="expr-bool-ops">Boolean operators: AND, OR, NOT</link></listitem>
<listitem><link linkend="expr-bitwise-ops">Bitwise operators: &amp;, |</link></listitem>
<listitem><link linkend="expr-func-abs">ABS()</link></listitem>
<listitem><link linkend="expr-func-bigint">BIGINT()</link></listitem>
<listitem><link linkend="expr-func-ceil">CEIL()</link></listitem>
<listitem><link linkend="expr-func-cos">COS()</link></listitem>
<listitem><link linkend="expr-func-crc32">CRC32()</link></listitem>
<listitem><link linkend="expr-func-day">DAY()</link></listitem>
<listitem><link linkend="expr-func-exp">EXP()</link></listitem>
<listitem><link linkend="expr-func-floor">FLOOR()</link></listitem>
<listitem><link linkend="expr-func-geodist">GEODIST()</link></listitem>
<listitem><link linkend="expr-func-idiv">IDIV()</link></listitem>
<listitem><link linkend="expr-func-if">IF()</link></listitem>
<listitem><link linkend="expr-func-in">IN()</link></listitem>
<listitem><link linkend="expr-func-interval">INTERVAL()</link></listitem>
<listitem><link linkend="expr-func-ln">LN()</link></listitem>
<listitem><link linkend="expr-func-log10">LOG10()</link></listitem>
<listitem><link linkend="expr-func-log2">LOG2()</link></listitem>
<listitem><link linkend="expr-func-max">MAX()</link></listitem>
<listitem><link linkend="expr-func-min">MIN()</link></listitem>
<listitem><link linkend="expr-func-month">MONTH()</link></listitem>
<listitem><link linkend="expr-func-now">NOW()</link></listitem>
<listitem><link linkend="expr-func-pow">POW()</link></listitem>
<listitem><link linkend="expr-func-sin">SIN()</link></listitem>
<listitem><link linkend="expr-func-sint">SINT()</link></listitem>
<listitem><link linkend="expr-func-sqrt">SQRT()</link></listitem>
<listitem><link linkend="expr-func-year">YEAR()</link></listitem>
<listitem><link linkend="expr-func-yearmonth">YEARMONTH()</link></listitem>
<listitem><link linkend="expr-func-yearmonthday">YEARMONTHDAY()</link></listitem>
</itemizedlist>
</para>


<sect2 id="operators">
<title>Operators</title>
<variablelist>

<varlistentry>
<term id="expr-ari-ops">Arithmetic operators: +, -, *, /, %, DIV, MOD</term>
<listitem>
The standard arithmetic operators. Arithmetic calculations involving those
can be performed in three different modes: (a) using single-precision,
32-bit IEEE 754 floating point values (the default), (b) using signed 32-bit integers,
(c) using 64-bit signed integers. The expression parser will automatically switch
to integer mode if there are no operations the result in a floating point value.
Otherwise, it will use the default floating point mode. For instance, <code>a+b</code>
will be computed using 32-bit integers if both arguments are 32-bit integers;
or using 64-bit integers if both arguments are integers but one of them is
64-bit; or in floats otherwise. However, <code>a/b</code> or <code>sqrt(a)</code>
will always be computed in floats, because these operations return a result
of non-integer type. To avoid the first, you can either use <code>IDIV(a,b)</code>
or <code>a DIV b</code> form. Also, <code>a*b</code>
will not be automatically promoted to 64-bit when the arguments are 32-bit.
To enforce 64-bit results, you can use BIGINT(). (But note that if there are
non-integer operations, BIGINT() will simply be ignored.)
</listitem>
</varlistentry>

<varlistentry>
<term id="expr-comp-ops">Comparison operators: &lt;, &gt; &lt;=, &gt;=, =, &lt;&gt;</term>
<listitem>
Comparison operators (eg. = or &lt;=) return 1.0 when the condition is true and 0.0 otherwise.
For instance, <code>(a=b)+3</code> will evaluate to 4 when attribute 'a' is equal to attribute 'b', and to 3 when 'a' is not.
Unlike MySQL, the equality comparisons (ie. = and &lt;&gt; operators) introduce a small equality threshold (1e-6 by default).
If the difference between compared values is within the threshold, they will be considered equal.
</listitem>
</varlistentry>

<varlistentry>
<term id="expr-bool-ops">Boolean operators: AND, OR, NOT</term>
<listitem>
Boolean operators (AND, OR, NOT) were introduced in 0.9.9-rc2 and behave as usual.
They are left-associative and have the least priority compared to other operators.
NOT has more priority than AND and OR but nevertheless less than any other operator.
AND and OR have the same priority so brackets use is recommended to avoid confusion
in complex expressions.
</listitem>
</varlistentry>

<varlistentry>
<term id="expr-bitwise-ops">Bitwise operators: &amp;, |</term>
<listitem>
These operators perform bitwise AND and OR respectively. The operands
must be of an integer types. Introduced in version 1.10-beta.
</listitem>
</varlistentry>

</variablelist>
</sect2>


<sect2 id="numeric-functions">
<title>Numeric functions</title>
<variablelist>

<varlistentry>
<term id="expr-func-abs">ABS()</term>
<listitem>Returns the absolute value of the argument.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-ceil">CEIL()</term>
<listitem>Returns the smallest integer value greater or equal to the argument.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-cos">COS()</term>
<listitem>Returns the cosine of the argument.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-exp">EXP()</term>
<listitem>Returns the exponent of the argument (e=2.718... to the power of the argument).</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-floor">FLOOR()</term>
<listitem>Returns the largest integer value lesser or equal to the argument.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-idiv">IDIV()</term>
<listitem>
Returns the result of an integer division of the first
argument by the second argument. Both arguments must be
of an integer type.
</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-ln">LN()</term>
<listitem>Returns the natural logarithm of the argument (with the base of e=2.718...).</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-log10">LOG10()</term>
<listitem>Returns the common logarithm of the argument (with the base of 10).</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-log2">LOG2()</term>
<listitem>Returns the binary logarithm of the argument (with the base of 2).</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-max">MAX()</term>
<listitem>Returns the bigger of two arguments.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-min">MIN()</term>
<listitem>Returns the smaller of two arguments.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-pow">POW()</term>
<listitem>Returns the first argument raised to the power of the second argument.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-sin">SIN()</term>
<listitem>Returns the sine of the argument.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-sqrt">SQRT()</term>
<listitem>Returns the square root of the argument.</listitem>
</varlistentry>

</variablelist>
</sect2>


<sect2 id="date-time-functions">
<title>Date and time functions</title>
<variablelist>

<varlistentry>
<term id="expr-func-day">DAY()</term>
<listitem>Returns the integer day of month (in 1..31 range) from a timestamp argument, according to the current timezone. Introduced in version 1.11-beta.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-month">MONTH()</term>
<listitem>Returns the integer month (in 1..12 range) from a timestamp argument, according to the current timezone. Introduced in version 1.11-beta.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-now">NOW()</term>
<listitem>Returns the current timestamp as an INTEGER. Introduced in version 0.9.9-rc1.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-year">YEAR()</term>
<listitem>Returns the integer year (in 1969..2038 range) from a timestamp argument, according to the current timezone. Introduced in version 1.11-beta.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-yearmonth">YEARMONTH()</term>
<listitem>Returns the integer year and month code (in 196912..203801 range) from a timestamp argument, according to the current timezone. Introduced in version 1.11-beta.</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-yearmonthday">YEARMONTHDAY()</term>
<listitem>Returns the integer year, month, and date code (in 19691231..20380119 range) from a timestamp argument, according to the current timezone. Introduced in version 1.11-beta.</listitem>
</varlistentry>

</variablelist>
</sect2>


<sect2 id="type-conversion-functions">
<title>Type conversion functions</title>
<variablelist>

<varlistentry>
<term id="expr-func-bigint">BIGINT()</term>
<listitem>
Forcibly promotes the integer argument to 64-bit type,
and does nothing on floating point argument. It's intended to help enforce evaluation
of certain expressions (such as <code>a*b</code>) in 64-bit mode even though all the arguments
are 32-bit.
Introduced in version 0.9.9-rc1.
</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-sint">SINT()</term>
<listitem>
Forcibly reinterprets its
32-bit unsigned integer argument as signed, and also expands it to 64-bit type
(because 32-bit type is unsigned). It's easily illustrated by the following
example: 1-2 normally evaluates to 4294967295, but SINT(1-2) evaluates to -1.
Introduced in version 1.10-beta. 
</listitem>
</varlistentry>

</variablelist>
</sect2>


<sect2 id="comparison-functions">
<title>Comparison functions</title>
<variablelist>

<varlistentry>
<term id="expr-func-if">IF()</term>
<listitem>
<code>IF()</code> behavior is slightly different that that of its MySQL counterpart.
It takes 3 arguments, check whether the 1st argument is equal to 0.0, returns the 2nd argument if it is not zero, or the 3rd one when it is.
Note that unlike comparison operators, <code>IF()</code> does <b>not</b> use a threshold!
Therefore, it's safe to use comparison results as its 1st argument, but arithmetic operators might produce unexpected results.
For instance, the following two calls will produce <emphasis>different</emphasis> results even though they are logically equivalent:
<programlisting>
IF ( sqrt(3)*sqrt(3)-3&lt;&gt;0, a, b )
IF ( sqrt(3)*sqrt(3)-3, a, b )
</programlisting>
In the first case, the comparison operator &lt;&gt; will return 0.0 (false)
because of a threshold, and <code>IF()</code> will always return 'b' as a result.
In the second one, the same <code>sqrt(3)*sqrt(3)-3</code> expression will be compared
with zero <emphasis>without</emphasis> threshold by the <code>IF()</code> function itself.
But its value will be slightly different from zero because of limited floating point
calculations precision. Because of that, the comparison with 0.0 done by <code>IF()</code>
will not pass, and the second variant will return 'a' as a result.
</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-in">IN()</term>
<listitem>
IN(expr,val1,val2,...), introduced in version 0.9.9-rc1, takes 2 or more arguments, and returns 1 if 1st argument
(expr) is equal to any of the other arguments (val1..valN), or 0 otherwise.
Currently, all the checked values (but not the expression itself!) are required
to be constant. (Its technically possible to implement arbitrary expressions too,
and that might be implemented in the future.) Constants are pre-sorted and then
binary search is used, so IN() even against a big arbitrary list of constants
will be very quick. Starting with 0.9.9-rc2, first argument can also be
a MVA attribute. In that case, IN() will return 1 if any of the MVA values
is equal to any of the other arguments. Starting with 1.11-beta, IN() also
supports <code>IN(expr,@uservar)</code> syntax to check whether the value
belongs to the list in the given global user variable.
</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-interval">INTERVAL()</term>
<listitem>
INTERVAL(expr,point1,point2,point3,...), introduced in version 0.9.9-rc1, takes 2 or more arguments, and returns
the index of the argument that is less than the first argument: it returns
0 if expr&lt;point1, 1 if point1&lt;=expr&lt;point2, and so on.
It is required that point1&lt;point2&lt;...&lt;pointN for this function
to work correctly.
</listitem>
</varlistentry>

</variablelist>
</sect2>


<sect2 id="misc-functions">
<title>Miscellaneous functions</title>
<variablelist>

<varlistentry>
<term id="expr-func-crc32">CRC32()</term>
<listitem>
Returns the CRC32 value of a string argument. Introduced in version 1.11-beta.
</listitem>
</varlistentry>

<varlistentry>
<term id="expr-func-geodist">GEODIST()</term>
<listitem>
GEODIST(lat1,long1,lat2,long2) function, introduced in version 0.9.9-rc2,
computes geosphere distance between two given points specified by their
coordinates. Note that both latitudes and longitudes must be in radians
and the result will be in meters. You can use arbitrary expression as any
of the four coordinates. An optimized path will be selected when one pair
of the arguments refers directly to a pair attributes and the other one
is constant.
</listitem>
</varlistentry>

</variablelist>
</sect2>

</sect1>


<sect1 id="sorting-modes"><title>Sorting modes</title>
<para>
There are the following result sorting modes available:
<itemizedlist>
<listitem>SPH_SORT_RELEVANCE mode, that sorts by relevance in descending order (best matches first);</listitem>
<listitem>SPH_SORT_ATTR_DESC mode, that sorts by an attribute in descending order (bigger attribute values first);</listitem>
<listitem>SPH_SORT_ATTR_ASC mode, that sorts by an attribute in ascending order (smaller attribute values first);</listitem>
<listitem>SPH_SORT_TIME_SEGMENTS mode, that sorts by time segments (last hour/day/week/month) in descending order, and then by relevance in descending order;</listitem>
<listitem>SPH_SORT_EXTENDED mode, that sorts by SQL-like combination of columns in ASC/DESC order;</listitem>
<listitem>SPH_SORT_EXPR mode, that sorts by an arithmetic expression.</listitem>
</itemizedlist>
</para>
<para>
SPH_SORT_RELEVANCE ignores any additional parameters and always sorts matches
by relevance rank. All other modes require an additional sorting clause, with the
syntax depending on specific mode. SPH_SORT_ATTR_ASC, SPH_SORT_ATTR_DESC and
SPH_SORT_TIME_SEGMENTS modes require simply an attribute name.

SPH_SORT_RELEVANCE is equivalent to sorting by "@weight DESC, @id ASC" in extended sorting mode,
SPH_SORT_ATTR_ASC is equivalent to "attribute ASC, @weight DESC, @id ASC",
and SPH_SORT_ATTR_DESC to "attribute DESC, @weight DESC, @id ASC" respectively.
</para>

<bridgehead>SPH_SORT_TIME_SEGMENTS mode</bridgehead>
<para>
In SPH_SORT_TIME_SEGMENTS mode, attribute values are split into so-called
time segments, and then sorted by time segment first, and by relevance second.
</para>
<para>
The segments are calculated according to the <emphasis>current timestamp</emphasis>
at the time when the search is performed, so the results would change over time.
The segments are as follows:
<itemizedlist>
<listitem>last hour,</listitem>
<listitem>last day,</listitem>
<listitem>last week,</listitem>
<listitem>last month,</listitem>
<listitem>last 3 months,</listitem>
<listitem>everything else.</listitem>
</itemizedlist>
These segments are hardcoded, but it is trivial to change them if necessary.
</para>
<para>
This mode was added to support searching through blogs, news headlines, etc.
When using time segments, recent records would be ranked higher because of segment,
but withing the same segment, more relevant records would be ranked higher -
unlike sorting by just the timestamp attribute, which would not take relevance
into account at all.
</para>

<bridgehead id="sort-extended">SPH_SORT_EXTENDED mode</bridgehead>
<para>
In SPH_SORT_EXTENDED mode, you can specify an SQL-like sort expression
with up to 5 attributes (including internal attributes), eg:
<programlisting>
@relevance DESC, price ASC, @id DESC
</programlisting>
</para>
<para>
Both internal attributes (that are computed by the engine on the fly)
and user attributes that were configured for this index are allowed.
Internal attribute names must start with magic @-symbol; user attribute
names can be used as is. In the example above, <option>@relevance</option>
and <option>@id</option> are internal attributes and <option>price</option> is user-specified.
</para>
<para>
Known internal attributes are:
<itemizedlist>
<listitem>@id (match ID)</listitem>
<listitem>@weight (match weight)</listitem>
<listitem>@rank (match weight)</listitem>
<listitem>@relevance (match weight)</listitem>
<listitem>@random (return results in random order)</listitem>
</itemizedlist>
<option>@rank</option> and <option>@relevance</option> are just additional
aliases to <option>@weight</option>.
</para>

<bridgehead id="sort-expr">SPH_SORT_EXPR mode</bridgehead>
<para>
Expression sorting mode lets you sort the matches by an arbitrary arithmetic
expression, involving attribute values, internal attributes (@id and @weight),
arithmetic operations, and a number of built-in functions. Here's an example:
<programlisting>
$cl->SetSortMode ( SPH_SORT_EXPR,
	"@weight + ( user_karma + ln(pageviews) )*0.1" );
</programlisting>
The operators and functions supported in the expressions are discussed
in a separate section, <xref linkend="expressions"/>.
</para>
</sect1>


<sect1 id="clustering"><title>Grouping (clustering) search results </title>
<para>
Sometimes it could be useful to group (or in other terms, cluster)
search results and/or count per-group match counts - for instance,
to draw a nice graph of how much maching blog posts were there per
each month; or to group Web search results by site; or to group
matching forum posts by author; etc.
</para>
<para>
In theory, this could be performed by doing only the full-text search
in Sphinx and then using found IDs to group on SQL server side. However,
in practice doing this with a big result set (10K-10M matches) would
typically kill performance.
</para>
<para>
To avoid that, Sphinx offers so-called grouping mode. It is enabled
with SetGroupBy() API call. When grouping, all matches are assigned to
different groups based on group-by value. This value is computed from
specified attribute using one of the following built-in functions:
<itemizedlist>
<listitem>SPH_GROUPBY_DAY, extracts year, month and day in YYYYMMDD format from timestamp;</listitem>
<listitem>SPH_GROUPBY_WEEK, extracts year and first day of the week number (counting from year start) in YYYYNNN format from timestamp;</listitem>
<listitem>SPH_GROUPBY_MONTH, extracts month in YYYYMM format from timestamp;</listitem>
<listitem>SPH_GROUPBY_YEAR, extracts year in YYYY format from timestamp;</listitem>
<listitem>SPH_GROUPBY_ATTR, uses attribute value itself for grouping.</listitem>
</itemizedlist>
</para>
<para>
The final search result set then contains one best match per group.
Grouping function value and per-group match count are returned along
as "virtual" attributes named
<emphasis role="bold">@group</emphasis> and
<emphasis role="bold">@count</emphasis> respectively.
</para>
<para>
The result set is sorted by group-by sorting clause, with the syntax similar
to <link linkend="sort-extended"><option>SPH_SORT_EXTENDED</option> sorting clause</link>
syntax. In addition to <option>@id</option> and <option>@weight</option>,
group-by sorting clause may also include:
<itemizedlist>
<listitem>@group (groupby function value),</listitem>
<listitem>@count (amount of matches in group).</listitem>
</itemizedlist>
</para>
<para>
The default mode is to sort by groupby value in descending order,
ie. by <option>"@group desc"</option>.
</para>
<para>
On completion, <option>total_found</option> result parameter would
contain total amount of matching groups over he whole index.
</para>
<para>
<emphasis role="bold">WARNING:</emphasis> grouping is done in fixed memory
and thus its results are only approximate; so there might be more groups reported
in <option>total_found</option> than actually present. <option>@count</option> might also
be underestimated. To reduce inaccuracy, one should raise <option>max_matches</option>.
If <option>max_matches</option> allows to store all found groups, results will be 100% correct.
</para>
<para>
For example, if sorting by relevance and grouping by <code>"published"</code>
attribute with <code>SPH_GROUPBY_DAY</code> function, then the result set will
contain
<itemizedlist>
<listitem>one most relevant match per each day when there were any
matches published,</listitem>
<listitem>with day number and per-day match count attached,</listitem>
<listitem>sorted by day number in descending order (ie. recent days first).</listitem>
</itemizedlist>
</para>
<para>
Starting with version 0.9.9-rc2, aggregate functions (AVG(), MIN(),
MAX(), SUM()) are supported through <link linkend="api-func-setselect">SetSelect()</link> API call
when using GROUP BY.
</para>
</sect1>


<sect1 id="distributed"><title>Distributed searching</title>
<para>
To scale well, Sphinx has distributed searching capabilities.
Distributed searching is useful to improve query latency (ie. search
time) and throughput (ie. max queries/sec) in multi-server, multi-CPU
or multi-core environments. This is essential for applications which
need to search through huge amounts data (ie. billions of records
and terabytes of text).
</para>
<para>
The key idea is to horizontally partition (HP) searched data
accross search nodes and then process it in parallel.
</para>
<para>
Partitioning is done manually. You should
<itemizedlist>
<listitem>setup several instances
of Sphinx programs (<filename>indexer</filename> and <filename>searchd</filename>)
on different servers;</listitem>
<listitem>make the instances index (and search) different parts of data;</listitem>
<listitem>configure a special distributed index on some of the <filename>searchd</filename>
instances;</listitem>
<listitem>and query this index.</listitem>.
</itemizedlist>
This index only contains references to other
local and remote indexes - so it could not be directly reindexed,
and you should reindex those indexes which it references instead.
</para>
<para>
When <filename>searchd</filename> receives a query against distributed index,
it does the following:
<orderedlist>
<listitem>connects to configured remote agents;</listitem>
<listitem>issues the query;</listitem>
<listitem>sequentially searches configured local indexes (while the remote agents are searching);</listitem>
<listitem>retrieves remote agents' search results;</listitem>
<listitem>merges all the results together, removing the duplicates;</listitem>
<listitem>sends the merged resuls to client.</listitem>
</orderedlist>
</para>
<para>
From the application's point of view, there are no differences
between searching through a regular index, or a distributed index at all.
That is, distributed indexes are fully transparent to the application,
and actually there's no way to tell whether the index you queried
was distributed or local. (Even though as of 0.9.9 Sphinx does not
allow to combine searching through distributed indexes with anything else,
this constraint will be lifted in the future.)
</para>
<para>
Any <filename>searchd</filename> instance could serve both as a master
(which aggregates the results) and a slave (which only does local searching)
at the same time. This has a number of uses:
<orderedlist>
<listitem>every machine in a cluster could serve as a master which
searches the whole cluster, and search requests could be balanced between
masters to achieve a kind of HA (high availability) in case any of the nodes fails;
</listitem>
<listitem>
if running within a single multi-CPU or multi-core machine, there
would be only 1 searchd instance quering itself as an agent and thus
utilizing all CPUs/core.
</listitem>
</orderedlist>
</para>
<para>
It is scheduled to implement better HA support which would allow
to specify which agents mirror each other, do health checks, keep track
of alive agents, load-balance requests, etc.
</para>
</sect1>


<sect1 id="query-log-format"><title><filename>searchd</filename> query log formats</title>
<para>
In version 1.11-beta and above two query log formats are supported.
Previous versions only supported a custom plain text format. That format
is still the default one. However, while it might be more convenient for
manual monitoring and review, but hard to replay for benchmarks, it only
logs <emphasis>search</emphasis> queries but not the other types
of requests, does not always contain the complete search query 
data, etc. The default text format is also harder (and sometimes
impossible) to replay for benchmarking purposes. The new <code>sphinxql</code>
format alleviates that. It aims to be complete and automatable,
even though at the cost of brevity and readability.
</para>

<sect2 id="plain-log-format"><title>Plain log format</title>
<para>
By default, <filename>searchd</filename> logs all succesfully executed search queries
into a query log file. Here's an example:
<programlisting>
[Fri Jun 29 21:17:58 2007] 0.004 sec [all/0/rel 35254 (0,20)] [lj] test
[Fri Jun 29 21:20:34 2007] 0.024 sec [all/0/rel 19886 (0,20) @channel_id] [lj] test
</programlisting>
This log format is as follows:
<programlisting>
[query-date] query-time [match-mode/filters-count/sort-mode
    total-matches (offset,limit) @groupby-attr] [index-name] query
</programlisting>
Match mode can take one of the following values:
<itemizedlist>
<listitem>"all" for SPH_MATCH_ALL mode;</listitem>
<listitem>"any" for SPH_MATCH_ANY mode;</listitem>
<listitem>"phr" for SPH_MATCH_PHRASE mode;</listitem>
<listitem>"bool" for SPH_MATCH_BOOLEAN mode;</listitem>
<listitem>"ext" for SPH_MATCH_EXTENDED mode;</listitem>
<listitem>"ext2" for SPH_MATCH_EXTENDED2 mode;</listitem>
<listitem>"scan" if the full scan mode was used, either by being specified with SPH_MATCH_FULLSCAN, or if the query was empty (as documented under <link linkend="matching-modes">Matching Modes</link>)</listitem>
</itemizedlist>
Sort mode can take one of the following values:
<itemizedlist>
<listitem>"rel" for SPH_SORT_RELEVANCE mode;</listitem>
<listitem>"attr-" for SPH_SORT_ATTR_DESC mode;</listitem>
<listitem>"attr+" for SPH_SORT_ATTR_ASC mode;</listitem>
<listitem>"tsegs" for SPH_SORT_TIME_SEGMENTS mode;</listitem>
<listitem>"ext" for SPH_SORT_EXTENDED mode.</listitem>
</itemizedlist>
</para>
<para>Additionally, if <filename>searchd</filename> was started with <option>--iostats</option>, there will be a block of data after where the index(es) searched are listed.</para>
<para>A query log entry might take the form of:</para>
<programlisting>
[Fri Jun 29 21:17:58 2007] 0.004 sec [all/0/rel 35254 (0,20)] [lj]
   [ios=6 kb=111.1 ms=0.5] test
</programlisting>
<para>
This additional block is information regarding I/O operations in performing the search:
the number of file I/O operations carried out, the amount of data in kilobytes read from
the index files and time spent on I/O operations (although there is a background processing
component, the bulk of this time is the I/O operation time).
</para>
</sect2>

<sect2 id="sphinxql-log-format"><title>SphinxQL log format</title>
<para>
This is a new log format introduced in 1.11-beta, with the goals
begin logging everything and then some, and in a format easy to automate
(for insance, automatically replay). New format can either be enabled
via the <link linkend="conf-query-log-format">query_log_format</link>
directive in the configuration file, or switched back and forth
on the fly with the
<link linkend="sphinxql-set"><code>SET GLOBAL query_log_format=...</code></link>
statement via SphinxQL. In the new format, the example from the previous
section would look as follows. (Wrapped below for readability, but with
just one query per line in the actual log.)
<programlisting>
/* Fri Jun 29 21:17:58.609 2007 2011 conn 2 wall 0.004 found 35254 */
SELECT * FROM lj WHERE MATCH('test') OPTION ranker=proximity;

/* Fri Jun 29 21:20:34 2007.555 conn 3 wall 0.024 found 19886 */
SELECT * FROM lj WHERE MATCH('test') GROUP BY channel_id
OPTION ranker=proximity;
</programlisting>
Note that <b>all</b> requests would be logged in this format,
including those sent via SphinxAPI and SphinxSE, not just those
sent via SphinxQL.
</para>
<para>
The features of SphinxQL log format compared to the default text
one are as follows.
<itemizedlist>
<listitem>All request types should be logged. (This is still work in progress.)</listitem>
<listitem>Full statement data will be logged where possible.</listitem>
<listitem>Errors and warnings are logged.</listitem>
<listitem>The log should be automatically replayable via SphinxQL.</listitem>
<listitem>Additional performance counters (currently, per-agent distributed query times) are logged.</listitem>
</itemizedlist>
<!-- FIXME! more examples with ios, kbs, agents etc; comment stuff reference?-->
</para>
<para>
Every request (including both SphinxAPI and SphinxQL) request
must result in exactly one log line. All request types, including
INSERT, CALL SNIPPETS, etc will eventually get logged, though as of
time of this writing, that is a work in progress). Every log line
must be a valid SphinxQL statement that reconstructs the full request,
except if the logged request is too big and needs shortening
for performance reasons. Additional messages, counters, etc can be
logged in the comments section after the request.
</para>
</sect2>
</sect1>


<sect1 id="sphinxql"><title>MySQL protocol support and SphinxQL</title>
<para>
Starting with version 0.9.9-rc2, Sphinx searchd daemon supports MySQL binary
network protocol and can be accessed with regular MySQL API. For instance,
'mysql' CLI client program works well. Here's an example of querying
Sphinx using MySQL client:
<programlisting>
$ mysql -P 9306
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1
Server version: 0.9.9-dev (r1734)

Type 'help;' or '\h' for help. Type '\c' to clear the buffer.

mysql&gt; SELECT * FROM test1 WHERE MATCH('test') 
    -&gt; ORDER BY group_id ASC OPTION ranker=bm25;
+------+--------+----------+------------+
| id   | weight | group_id | date_added |
+------+--------+----------+------------+
|    4 |   1442 |        2 | 1231721236 |
|    2 |   2421 |      123 | 1231721236 |
|    1 |   2421 |      456 | 1231721236 |
+------+--------+----------+------------+
3 rows in set (0.00 sec)
</programlisting>
</para>
<para>
Note that mysqld was not even running on the test machine. Everything was
handled by searchd itself.
</para>
<para>
The new access method is supported <emphasis>in addition</emphasis>
to native APIs which all still work perfectly well. In fact, both
access methods can be used at the same time. Also, native API is still
the default access method. MySQL protocol support needs to be additionally
configured. This is a matter of 1-line config change, adding a new
<link linkend="conf-listen">listener</link> with mysql41 specified
as a protocol:
<programlisting>
listen = localhost:9306:mysql41
</programlisting>
</para>
<para>
Just supporting the protocol and not the SQL syntax would be useless
so Sphinx now also supports a subset of SQL that we dubbed SphinxQL.
It supports the standard querying all the index types with SELECT,
modifying RT indexes with INSERT, REPLACE, and DELETE, and much more.
Full SphinxQL reference is available in <xref linkend="sphinxql-reference"/>.
</para>
</sect1>


<sect1 id="multi-queries"><title>Multi-queries</title>
<para>
Multi-queries, or query batches, let you send multiple queries to Sphinx
in one go (more formally, one network request).
</para>
<para>
Two API methods that implement multi-query mechanism are
<link linkend="api-func-addquery">AddQuery()</link> and
<link linkend="api-func-runqueries">RunQueries()</link>.
You can also run multiple queries with SphinxQL, see
<xref linkend="sphinxql-multi-queries"/>.
(In fact, regular <link linkend="api-func-addquery">Query()</link>
call is internally implemented as a single AddQuery() call immediately
followed by RunQueries() call.) AddQuery() captures the current state
of all the query settings set by previous API calls, and memorizes
the query. RunQueries() actually sends all the memorized queries,
and returns multiple result sets. There are no restrictions on
the queries at all, except just a sanity check on a number of queries
in a single batch (see <xref linkend="conf-max-batch-queries"/>).
</para>
<para>
Why use multi-queries? Generally, it all boils down to performance.
First, by sending requests to <filename>searchd</filename> in a batch
instead of one by one, you always save a bit by doing less network
roundtrips. Second, and somewhat more important, sending queries
in a batch enables <filename>searchd</filename> to perform certain
internal optimizations. As new types of optimizations are being
added over time, it generally makes sense to pack all the queries
into batches where possible, so that simply upgrading Sphinx
to a new version would automatically enable new optimizations.
In the case when there aren't any possible batch optimizations
to apply, queries will be processed one by one internally.
</para>
<para>
Why (or rather when) not use multi-queries? Multi-queries requires
all the queries in a batch to be independent, and sometimes they aren't.
That is, sometimes query B is based on query A results, and so can only be
set up after executing query A. For instance, you might want to display
results from a secondary index if and only if there were no results
found in a primary index. Or maybe just specify offset into 2nd result set
based on the amount of matches in the 1st result set. In that case,
you will have to use separate queries (or separate batches).
</para>
<para>
As of 0.9.10, there are two major optimizations to be aware of:
common query optimization (available since 0.9.8); and common
subtree optimization (available since 0.9.10).
</para>
<para>
<b>Common query optimization</b> means that <filename>searchd</filename>
will identify all those queries in a batch where only the sorting
and group-by settings differ, and <emphasis>only perform searching once</emphasis>.
For instance, if a batch consists of 3 queries, all of them are for
"ipod nano", but 1st query requests top-10 results sorted by price,
2nd query groups by vendor ID and requests top-5 vendors sorted by
rating, and 3rd query requests max price, full-text search for
"ipod nano" will only be performed once, and its results will be
reused to build 3 different result sets.
</para>
<para>
So-called <b>faceted searching</b> is a particularly important case
that benefits from this optimization. Indeed, faceted searching
can be implemented by running a number of queries, one to retrieve
search results themselves, and a few other ones with same full-text
query but different  group-by settings to retrieve all the required
groups of results (top-3 authors, top-5 vendors, etc). And as long
as full-text query and filtering settings stay the same, common
query optimization will trigger, and greatly improve performance.
</para>
<para>
<b>Common subtree optimization</b> is even more interesting.
It lets <filename>searchd</filename> exploit similarities between
batched full-text queries. It identifies common full-text query parts
(subtress) in all queries, and caches them between queries. For instance,
look at the following query batch:
<programlisting>
barack obama president
barack obama john mccain
barack obama speech
</programlisting>
There's a common two-word part ("barack obama") that can be computed
only once, then cached and shared across the queries. And common subtree
optimization does just that. Per-query cache size is strictly controlled
by <link linkend="conf-subtree-docs-cache">subtree_docs_cache</link>
and <link linkend="conf-subtree-hits-cache">subtree_hits_cache</link>
directives (so that caching <emphasis>all</emphasis> sxiteen gazillions
of documents that match "i am" does not exhaust the RAM and instantly
kill your server).
</para>
<para>
Here's a code sample (in PHP) that fire the same query in 3 different
sorting modes:
<programlisting>
require ( "sphinxapi.php" );
$cl = new SphinxClient ();
$cl->SetMatchMode ( SPH_MATCH_EXTENDED2 );

$cl->SetSortMode ( SPH_SORT_RELEVANCE );
$cl->AddQuery ( "the", "lj" );
$cl->SetSortMode ( SPH_SORT_EXTENDED, "published desc" );
$cl->AddQuery ( "the", "lj" );
$cl->SetSortMode ( SPH_SORT_EXTENDED, "published asc" );
$cl->AddQuery ( "the", "lj" );
$res = $cl->RunQueries();
</programlisting>
</para>
<para>
How to tell whether the queries in the batch were actually optimized?
If they were, respective query log will have a "multiplier" field that
specifies how many queries were processed together:
<programlisting>
[Sun Jul 12 15:18:17.000 2009] 0.040 sec x3 [ext2/0/rel 747541 (0,20)] [lj] the
[Sun Jul 12 15:18:17.000 2009] 0.040 sec x3 [ext2/0/ext 747541 (0,20)] [lj] the
[Sun Jul 12 15:18:17.000 2009] 0.040 sec x3 [ext2/0/ext 747541 (0,20)] [lj] the
</programlisting>
Note the "x3" field. It means that this query was optimized and
processed in a sub-batch of 3 queries. For reference, this is how
the regular log would look like if the queries were not batched:
<programlisting>
[Sun Jul 12 15:18:17.062 2009] 0.059 sec [ext2/0/rel 747541 (0,20)] [lj] the
[Sun Jul 12 15:18:17.156 2009] 0.091 sec [ext2/0/ext 747541 (0,20)] [lj] the
[Sun Jul 12 15:18:17.250 2009] 0.092 sec [ext2/0/ext 747541 (0,20)] [lj] the
</programlisting>
Note how per-query time in multi-query case was improved by a factor 
of 1.5x to 2.3x, depending on a particular sorting mode. In fact, for both
common query and common subtree optimizations, there were reports of 3x and
even more improvements, and that's from production instances, not just
synthetic tests.
</para>
</sect1>


<sect1 id="collations"><title>Collations</title>
<para>
Introduced to Sphinx in version 1.11-beta to supplement string sorting,
collations essentially affect the string attribute comparisons. They specify
both the character set encoding and the strategy that Sphinx uses to compare
strings when doing ORDER BY or GROUP BY with a string attribute involved.
</para>
<para>
String attributes are stored as is when indexing, and no character set
or language information is attached to them. That's okay as long as Sphinx
only needs to store and return the strings to the calling application verbatim.
But when you ask Sphinx to sort by a string value, that request immediately
becomes quite ambiguous.
</para>
<para>
First, single-byte (ASCII, or ISO-8859-1, or Windows-1251) strings
need to be processed differently that the UTF-8 ones that may encode
every character with a variable number of bytes. So we need to know
what is the character set type to interepret the raw bytes as meaningful
characters properly.
</para>
<para>
Second, we additionally need to know the language-specific
string sorting rules. For instance, when sorting according to US rules
in en_US locale, the accented character '&iuml;' (small letter i with diaeresis)
should be placed somewhere after 'z'. However, when sorting with French rules
and fr_FR locale in mind, it should be placed between 'i' and 'j'. And some
other set of rules might choose to ignore accents at all, allowing '&iuml;'
and 'i' to be mixed arbitrarily.
</para>
<para>
Third, but not least, we might need case-sensitive sorting in some
scenarios and case-insensitive sorting in some others.
</para>
<para>
Collations combine all of the above: the character set, the lanugage rules,
and the case sensitivity. Sphinx currently provides the following four
collations.
<orderedlist>
<listitem><option>libc_ci</option></listitem>
<listitem><option>libc_cs</option></listitem>
<listitem><option>utf8_general_ci</option></listitem>
<listitem><option>binary</option></listitem>
</orderedlist>
</para>
<para>
The first two collations rely on several standard C library (libc) calls
and can thus support any locale that is installed on your system. They provide
case-insensitive (_ci) and case-sensitive (_cs) comparisons respectively.
By default they will use C locale, effectively resorting to bytewise
comparisons. To change that, you need to specify a different available
locale using <link linkend="conf-collation-libc-locale">collation_libc_locale</link>
directive. The list of locales available on your system can usually be obtained
with the <filename>locale</filename> command:
<programlisting>
$ locale -a
C
en_AG
en_AU.utf8
en_BW.utf8
en_CA.utf8
en_DK.utf8
en_GB.utf8
en_HK.utf8
en_IE.utf8
en_IN
en_NG
en_NZ.utf8
en_PH.utf8
en_SG.utf8
en_US.utf8
en_ZA.utf8
en_ZW.utf8
es_ES
fr_FR
POSIX
ru_RU.utf8
ru_UA.utf8
</programlisting>
</para>
<para>
The specific list of the system locales may vary. Consult your OS documentation
to install additional needed locales.
</para>
<para>
<option>utf8_general_ci</option> and <option>binary</option> locales are
built-in into Sphinx. The first one is a generic collation for UTF-8 data
(without any so-called language tailoring); it should behave similar to
<option>utf8_general_ci</option> collation in MySQL. The second one
is a simple bytewise comparison.
</para>
<para>
Collation can be overriden via SphinxQL on a per-session basis using
<code>SET collation_connection</code> statement. All subsequent SphinxQL
queries will use this collation. SphinxAPI and SphinxSE queries will use
the server default collation, as specified in
<link linkend="conf-collation-server">collation_server</link> configuration
directive. Sphinx currently defaults to <option>libc_ci</option> collation.
</para>
<para>
Collations should affect all string attribute comparisons, including
those within ORDER BY and GROUP BY, so differently ordered or grouped results
can be returned depending on the collation chosen.
</para>
</sect1>


<sect1 id="udf"><title>User-defined functions (UDF)</title>
<para>
Starting with 1.11-beta, Sphinx supports User-Defined Functions,
or UDF for short. They can be loaded and unloaded dynamically into
<filename>searchd</filename> without having to restart the daemon,
and used in expressions when searching. UDF features at a glance
are as follows.
<itemizedlist>
<listitem>Functions can take integer (both 32-bit and 64-bit), float, string, or MVA arguments.</listitem>
<listitem>Functions can return integer or float values.</listitem>
<listitem>Functions can check the argument number, types, and names and raise errors.</listitem>
<listitem>Only simple functions (that is, non-aggregate ones) are currently supported.</listitem>
</itemizedlist>
</para>
<para>
User-defined functions need your OS to support dynamically loadable
libraries (aka shared objects). Most of the modern OSes are eligible,
including Linux, Windows, MacOS, Solaris, BSD and others. (The internal
testing has been done on Linux and Windows.) The UDF libraries must
reside in a directory specified by
<link linkend="conf-plugin-dir">plugin_dir</link> directive, and the
server must be configured to use <option>workers = threads</option> mode.
Relative paths to the library files are not allowed. Once the library
is succesfully built and copied to the trusted location, you can then
dynamically install and deinstall the functions using
<link linkend="sphinxql-create-function">CREATE FUNCTION</link> and
<link linkend="sphinxql-drop-function">DROP FUNCTION</link> statements
respectively. A single library can contain multiple functions. A library
gets loaded when you first install a function from it, and unloaded
when you deinstall all the functions from that library.
</para>
<para>
The library functions that will implement a UDF visible to SQL statements
need to follow C calling convention, and a simple naming convention. Sphinx
source distribution provides a sample file,
<ulink url="http://code.google.com/p/sphinxsearch/source/browse/trunk/src/udfexample.c">src/udfexample.c</ulink>,
that defines a few simple functions showing how to work with integer,
string, and MVA arguments; you can use that one as a foundation for
your new functions. It includes the UDF interface header file,
<ulink url="http://code.google.com/p/sphinxsearch/source/browse/trunk/src/udfexample.c">src/sphinxudf.h</ulink>,
that defines the required types and structures. <filename>sphinxudf.h</filename>
header is standalone, that is, does not require any other parts of Sphinx
source to compile.
</para>
<para>
Every function that you intend to use in your SELECT statements
requires at least two corresponding C/C++ functions: the initialization
call, and the function call itself. You can also optionally define
the deinitialization call if your function requires any post-query
cleanup. (For instance, if you were allocating any memory in either
the initialization call or the function calls.) Function names
in SQL are case insensitive, C function names are not. They need
to be all lower-case. Mistakes in function name prevent UDFs
from loading. You also have to pay special attention to the calling
convention used when compiling, the list and the types of arguments,
and the return type of the main function call. Mistakes in either
are likely to crash the server, or result in unexpected results
in the best case. Last but not least, all functions need to be
thread-safe.
</para>
<para>
Let's assume for the sake of example that your UDF name in SphinxQL
will be <code>MYFUNC</code>. The initialization, main, and deinitialization
functions would then need to be named as follows and take the following
arguments:
<programlisting>
/// initialization function
/// called once during query initialization
/// returns 0 on success
/// returns non-zero and fills error_message buffer on failure
int myfunc_init ( SPH_UDF_INIT * init, SPH_UDF_ARGS * args,
                  char * error_message );

/// main call function
/// returns the computed value
/// writes non-zero value into error_flag to indicate errors
RETURN_TYPE myfunc ( SPH_UDF_INIT * init, SPH_UDF_ARGS * args,
                     char * error_flag );

/// optional deinitialization function
/// called once to cleanup once query processing is done
void myfunc_deinit ( SPH_UDF_INIT * init );
</programlisting>
The two mentioned structures, <code>SPH_UDF_INIT</code> and
<code>SPH_UDF_ARGS</code>, are defined in the <filename>src/sphinxudf.h</filename>
interface header and documented there. <code>RETURN_TYPE</code> of the
main function must be one of the following:
<itemizedlist>
<listitem><code>int</code> for the functions that return INT.</listitem>
<listitem><code>sphinx_int64_t</code> for the functions that return BIGINT.</listitem>
<listitem><code>float</code> for the functions that return FLOAT.</listitem>
</itemizedlist>
</para>
<para>
The calling sequence is as follows. <code>myfunc_init()</code> is called
once when initializing the query. It can return a non-zero code to indicate
a failure; in that case query is not executed, and the error message from
the <code>error_message</code> buffer is returned. Otherwise, <code>myfunc()</code>
is be called for every row, and a <code>myfunc_deinit()</code> is then called
when the query ends. <code>myfunc()</code> can indicate an error by writing
a non-zero byte value to <code>error_flag</code>, in that case, it will
no more be called for subsequent rows, and a default value of 0 will be
substituted. Sphinx might or might not choose to terminate such queries
early, neither behavior is currently guaranteed.
</para>
</sect1>


</chapter>
<chapter id="command-line-tools"><title>Command line tools reference</title>


<para>As mentioned elsewhere, Sphinx is not a single program called 'sphinx',
but a collection of 4 separate programs which collectively form Sphinx. This section
covers these tools and how to use them.</para>


<sect1 id="ref-indexer"><title><filename>indexer</filename> command reference</title>
<para><filename>indexer</filename> is the first of the two principle tools
as part of Sphinx. Invoked from either the command line directly, or as part
of a larger script, <filename>indexer</filename> is solely responsible
for gathering the data that will be searchable.</para>
<para>The calling syntax for <filename>indexer</filename> is as follows:</para>
<programlisting>
indexer [OPTIONS] [indexname1 [indexname2 [...]]]
</programlisting>
<para>Essentially you would list the different possible indexes (that you would later
make available to search) in <filename>sphinx.conf</filename>, so when calling
<filename>indexer</filename>, as a minimum you need to be telling it what index
(or indexes) you want to index.</para>
<para>If <filename>sphinx.conf</filename> contained details on 2 indexes,
<filename>mybigindex</filename> and <filename>mysmallindex</filename>,
you could do the following:</para>
<programlisting>
$ indexer mybigindex
$ indexer mysmallindex mybigindex
</programlisting>
<para>As part of the configuration file, <filename>sphinx.conf</filename>, you specify
one or more indexes for your data. You might call <filename>indexer</filename> to reindex
one of them, ad-hoc, or you can tell it to process all indexes - you are not limited
to calling just one, or all at once, you can always pick some combination
of the available indexes.</para>
<para>The majority of the options for <filename>indexer</filename> are given
in the configuration file, however there are some options you might need to specify
on the command line as well, as they can affect how the indexing operation is performed.
These options are:
<itemizedlist>

<listitem><option>--config &lt;file&gt;</option> (<option>-c &lt;file&gt;</option> for short)
tells <filename>indexer</filename> to use the given file as its configuration. Normally,
it will look for <filename>sphinx.conf</filename> in the installation directory
(e.g. <filename>/usr/local/sphinx/etc/sphinx.conf</filename> if installed into
<filename>/usr/local/sphinx</filename>), followed by the current directory you are
in when calling <filename>indexer</filename> from the shell. This is most of use
in shared environments where the binary files are installed somewhere like
<filename>/usr/local/sphinx/</filename> but you want to provide users with
the ability to make their own custom Sphinx set-ups, or if you want to run
multiple instances on a single server. In cases like those you could allow them
to create their own <filename>sphinx.conf</filename> files and pass them to
<filename>indexer</filename> with this option. For example:
<programlisting>
$ indexer --config /home/myuser/sphinx.conf myindex
</programlisting>
</listitem>

<listitem><option>--all</option> tells <filename>indexer</filename> to update
every index listed in <filename>sphinx.conf</filename>, instead of listing individual indexes.
This would be useful in small configurations, or <filename>cron</filename>-type or maintenance
jobs where the entire index set will get rebuilt each day, or week, or whatever period is best.
Example usage:
<programlisting>
$ indexer --config /home/myuser/sphinx.conf --all
</programlisting>
</listitem>

<listitem><option>--rotate</option> is used for rotating indexes. Unless you have the situation
where you can take the search function offline without troubling users, you will almost certainly
need to keep search running whilst indexing new documents. <option>--rotate</option> creates
a second index, parallel to the first (in the same place, simply including <filename>.new</filename>
in the filenames). Once complete, <filename>indexer</filename> notifies <filename>searchd</filename>
via sending the <option>SIGHUP</option> signal, and <filename>searchd</filename> will attempt
to rename the indexes (renaming the existing ones to include <filename>.old</filename>
and renaming the <filename>.new</filename> to replace them), and then start serving
from the newer files. Depending on the setting of
<link linkend="conf-seamless-rotate">seamless_rotate</link>, there may be a slight delay
in being able to search the newer indexes. Example usage:
<programlisting>
$ indexer --rotate --all
</programlisting>
</listitem>

<listitem><option>--quiet</option> tells <filename>indexer</filename> not to output anything,
unless there is an error. Again, most used for <filename>cron</filename>-type, or other script
jobs where the output is irrelevant or unnecessary, except in the event of some kind of error.
Example usage:
<programlisting>
$ indexer --rotate --all --quiet
</programlisting>
</listitem>

<listitem><option>--noprogress</option> does not display progress details as they occur;
instead, the final status details (such as documents indexed, speed of indexing and so on
are only reported at completion of indexing. In instances where the script is not being
run on a console (or 'tty'), this will be on by default. Example usage:
<programlisting>
$ indexer --rotate --all --noprogress
</programlisting>
</listitem>

<listitem><option>--buildstops &lt;outputfile.text&gt; &lt;N&gt;</option> reviews
the index source, as if it were indexing the data, and produces a list of the terms
that are being indexed. In other words, it produces a list of all the searchable terms
that are becoming part of the index. Note; it does not update the index in question,
it simply processes the data 'as if' it were indexing, including running queries
defined with <option>sql_query_pre</option> or <option>sql_query_post</option>.
<filename>outputfile.txt</filename> will contain the list of words, one per line,
sorted by frequency with most frequent first, and <filename>N</filename> specifies
the maximum number of words that will be listed; if sufficiently large to encompass
every word in the index, only that many words will be returned. Such a dictionary list
could be used for client application features around "Did you mean..." functionality,
usually in conjunction with <option>--buildfreqs</option>, below. Example:
<programlisting>
$ indexer myindex --buildstops word_freq.txt 1000
</programlisting>
This would produce a document in the current directory, <filename>word_freq.txt</filename>
with the 1,000 most common words in 'myindex', ordered by most common first. Note that
the file will pertain to the last index indexed when specified with multiple indexes or
<option>--all</option> (i.e. the last one listed in the configuration file)
</listitem>

<listitem><option>--buildfreqs</option> works with <option>--buildstops</option>
(and is ignored if <option>--buildstops</option> is not specified).
As <option>--buildstops</option> provides the list of words used within the index,
<option>--buildfreqs</option> adds the quantity present in the index, which would be
useful in establishing whether certain words should be considered stopwords
if they are too prevalent. It will also help with developing "Did you mean..."
features where you can how much more common a given word compared to another,
similar one. Example:
<programlisting>
$ indexer myindex --buildstops word_freq.txt 1000 --buildfreqs
</programlisting>
This would produce the <filename>word_freq.txt</filename> as above, however after each word would be the number of times it occurred in the index in question.
</listitem>

<listitem><option>--merge &lt;dst-index&gt; &lt;src-index&gt;</option> is used
for physically merging indexes together, for example if you have a main+delta scheme,
where the main index rarely changes, but the delta index is rebuilt frequently,
and <option>--merge</option> would be used to combine the two. The operation moves
from right to left - the contents of <filename>src-index</filename> get examined
and physically combined with the contents of <filename>dst-index</filename>
and the result is left in <filename>dst-index</filename>.
In pseudo-code, it might be expressed as: <code>dst-index += src-index</code>
An example:
<programlisting>
$ indexer --merge main delta --rotate
</programlisting>
In the above example, where the main is the master, rarely modified index,
and delta is the less frequently modified one, you might use the above to call
<filename>indexer</filename> to combine the contents of the delta into the
main index and rotate the indexes.
</listitem>

<listitem><option>--merge-dst-range &lt;attr&gt; &lt;min&gt; &lt;max&gt;</option>
runs the filter range given upon merging. Specifically, as the merge is applied
to the destination index (as part of <option>--merge</option>, and is ignored 
if <option>--merge</option> is not specified), <filename>indexer</filename>
will also filter the documents ending up in the destination index, and only
documents will pass through the filter given will end up in the final index.
This could be used for example, in an index where there is a 'deleted' attribute,
where 0 means 'not deleted'. Such an index could be merged with:
<programlisting>
$ indexer --merge main delta --merge-dst-range deleted 0 0
</programlisting>
Any documents marked as deleted (value 1) would be removed from the newly-merged
destination index. It can be added several times to the command line,
to add successive filters to the merge, all of which must be met in order
for a document to become part of the final index.
</listitem>

<listitem><option>--dump-rows &lt;FILE&gt;</option> dumps rows fetched
by SQL source(s) into the specified file, in a MySQL compatible syntax.
Resulting dumps are the exact representation of data as received by
<filename>indexer</filename> and help to repeat indexing-time issues.
</listitem>

<listitem><option>--verbose</option> guarantees that every row that
caused problems indexing (duplicate, zero, or missing document ID;
or file field IO issues; etc) will be reported. By default, this option
is off, and problem summaries may be reported instead.
</listitem>

<listitem><option>--sighup-each</option> is useful when you are
rebuilding many big indexes, and want each one rotated into
<filename>searchd</filename> as soon as possible. With
<option>--sighup-each</option>, <filename>indexer</filename>
will send a SIGHUP signal to searchd after succesfully
completing the work on each index. (The default behavior
is to send a single SIGHUP after all the indexes were built.)
</listitem>

</itemizedlist>
</para>
</sect1>


<sect1 id="ref-searchd"><title><filename>searchd</filename> command reference</title>
<para><filename>searchd</filename> is the second of the two principle tools as part of Sphinx.
<filename>searchd</filename> is the part of the system which actually handles searches;
it functions as a server and is responsible for receiving queries, processing them and
returning a dataset back to the different APIs for client applications.</para>
<para>Unlike <filename>indexer</filename>, <filename>searchd</filename> is not designed
to be run either from a regular script or command-line calling, but instead either
as a daemon to be called from init.d (on Unix/Linux type systems) or to be called
as a service (on Windows-type systems), so not all of the command line options will
always apply, and so will be build-dependent.</para>
<para>Calling <filename>searchd</filename> is simply a case of:</para>
<programlisting>
$ searchd [OPTIONS]
</programlisting>
<para>The options available to <filename>searchd</filename> on all builds are:</para>
<itemizedlist>

<listitem><option>--help</option> (<option>-h</option> for short) lists all of the
	parameters that can be called in your particular build of <filename>searchd</filename>.
</listitem>

<listitem><option>--config &lt;file&gt;</option> (<option>-c &lt;file&gt;</option> for short)
	tells <filename>searchd</filename> to use the given file as its configuration,
	just as with <filename>indexer</filename> above.
</listitem>

<listitem><option>--stop</option> is used to asynchronously stop <filename>searchd</filename>,
	using the details of the PID file as specified in the <filename>sphinx.conf</filename> file,
	so you may also need to confirm to <filename>searchd</filename> which configuration
	file to use with the <option>--config</option> option. NB, calling <option>--stop</option>
	will also make sure any changes applied to the indexes with
	<link linkend="api-func-updateatttributes"><code>UpdateAttributes()</code></link>
	will be applied to the index files themselves. Example:
<programlisting>
$ searchd --config /home/myuser/sphinx.conf --stop
</programlisting>
</listitem>

<listitem><option>--stopwait</option> is used to synchronously stop <filename>searchd</filename>.
<option>--stop</option> essentially tells the running instance to exit (by sending it a SIGTERM)
and then immediately returns. <option>--stopwait</option> will also attempt to wait until the
running <filename>searchd</filename> instance actually finishes the shutdown (eg. saves all
the pending attribute changes) and exits. Example:
<programlisting>
$ searchd --config /home/myuser/sphinx.conf --stopwait
</programlisting>
Possible exit codes are as follows:
	<itemizedlist>
	<listitem>0 on success;</listitem>
	<listitem>1 if connection to running searchd daemon failed;</listitem>
	<listitem>2 if daemon reported an error during shutdown;</listitem>
	<listitem>3 if daemon crashed during shutdown.</listitem>
	</itemizedlist>
</listitem>

<listitem><option>--status</option> command is used to query running
<filename>searchd</filename> instance status, using the connection details
from the (optionally) provided configuration file. It will try to connect
to the running instance using the first configured UNIX socket or TCP port.
On success, it will query for a number of status and performance counter
values and print them. You can use <link linkend="api-func-status">Status()</link>
API call to access the very same counters from your application. Examples:
<programlisting>
$ searchd --status
$ searchd --config /home/myuser/sphinx.conf --status
</programlisting>
</listitem>

<listitem><option>--pidfile</option> is used to explicitly state a PID file,
where the process information is stored regarding <filename>searchd</filename>,
used for inter-process communications (for example, <filename>indexer</filename>
will need to know the PID to contact <filename>searchd</filename> for rotating
indexes). Normally, <filename>searchd</filename> would use a PID if running
in regular mode (i.e. not with <option>--console</option>), but it is possible
that you will be running it in console mode whilst the index is being updated
and rotated, for which a PID file will be needed.
<programlisting>
$ searchd --config /home/myuser/sphinx.conf --pidfile /home/myuser/sphinx.pid
</programlisting>
</listitem>

<listitem><option>--console</option> is used to force <filename>searchd</filename>
into console mode; typically it will be running as a conventional server application,
and will aim to dump information into the log files (as specified in
<filename>sphinx.conf</filename>). Sometimes though, when debugging issues
in the configuration or the daemon itself, or trying to diagnose hard-to-track-down
problems, it may be easier to force it to dump information directly
to the console/command line from which it is being called. Running in console mode
also means that the process will not be forked (so searches are done in sequence)
and logs will not be written to. (It should be noted that console mode
is not the intended method for running <filename>searchd</filename>.)
You can invoke it as such:
<programlisting>
$ searchd --config /home/myuser/sphinx.conf --console
</programlisting>
</listitem>

<listitem><option>--logdebug</option> enables additional debug output
in the daemon log. Should only be needed rarely, to assist with debugging
issues that could not be easily reproduced on request.
</listitem>

<listitem><option>--iostats</option> is used in conjuction with the
logging options (the <option>query_log</option> will need to have been
activated in <filename>sphinx.conf</filename>) to provide more detailed
information on a per-query basis as to the input/output operations
carried out in the course of that query, with a slight performance hit
and of course bigger logs. Further details are available under the
<link linkend="query-log-format">query log format</link> section.
You might start <filename>searchd</filename> thus:
<programlisting>
$ searchd --config /home/myuser/sphinx.conf --iostats
</programlisting>
</listitem>

<listitem><option>--cpustats</option> is used to provide actual CPU time
report (in addition to wall time) in both query log file (for every given
query) and status report (aggregated). It depends on clock_gettime() system
call and might therefore be unavailable on certain systems. You might start
<filename>searchd</filename> thus:
<programlisting>
$ searchd --config /home/myuser/sphinx.conf --cpustats
</programlisting>
</listitem>

<listitem><option>--port portnumber</option> (<option>-p</option> for short)
is used to specify the post that <filename>searchd</filename> should listen on,
usually for debugging purposes. This will usually default to 9312, but sometimes
you need to run it on a different port. Specifying it on the command line
will override anything specified in the configuration file. The valid range
is 0 to 65535, but ports numbered 1024 and below usually require
a privileged account in order to run. An example of usage:
<programlisting>
$ searchd --port 9313
</programlisting>
</listitem>

<listitem><option>--index &lt;index&gt;</option> forces this instance
of <filename>searchd</filename> only to serve the specified index.
Like <option>--port</option>, above, this is usually for debugging purposes;
more long-term changes would generally be applied to the configuration file
itself. Example usage:
<programlisting>
$ searchd --index myindex
</programlisting>
</listitem>

<listitem><option>--strip-path</option> strips the path names from
all the file names referenced from the index (stopwords, wordforms,
exceptions, etc). This is useful for picking up indexes built on another
machine with possibly different path layouts.
</listitem>

</itemizedlist>
<para>There are some options for <filename>searchd</filename> that are specific
to Windows platforms, concerning handling as a service, are only be available on Windows binaries.</para>
<para>Note that on Windows searchd will default to <option>--console</option> mode, unless you install it as a service.</para>
<itemizedlist>

<listitem><option>--install</option> installs <filename>searchd</filename> as a service
into the Microsoft Management Console (Control Panel / Administrative Tools / Services).
Any other parameters specified on the command line, where <option>--install</option>
is specified will also become part of the command line on future starts of the service.
For example, as part of calling <filename>searchd</filename>, you will likely also need
to specify the configuration file with <option>--config</option>, and you would do that
as well as specifying <option>--install</option>. Once called, the usual start/stop
facilities will become available via the management console, so any methods you could
use for starting, stopping and restarting services would also apply to
<filename>searchd</filename>. Example:
<programlisting>
C:\WINDOWS\system32&gt; C:\Sphinx\bin\searchd.exe --install
   --config C:\Sphinx\sphinx.conf
</programlisting>
If you wanted to have the I/O stats every time you started <filename>searchd</filename>,
you would specify its option on the same line as the <option>--install</option> command thus:
<programlisting>
C:\WINDOWS\system32&gt; C:\Sphinx\bin\searchd.exe --install
   --config C:\Sphinx\sphinx.conf --iostats
</programlisting>
</listitem>

<listitem><option>--delete</option> removes the service from the Microsoft Management Console
and other places where services are registered, after previously installed with
<option>--install</option>. Note, this does not uninstall the software or delete the indexes.
It means the service will not be called from the services systems, and will not be started
on the machine's next start. If currently running as a service, the current instance
will not be terminated (until the next reboot, or <filename>searchd</filename> is called
with <option>--stop</option>). If the service was installed with a custom name
(with <option>--servicename</option>), the same name will need to be specified
with <option>--servicename</option> when calling to uninstall. Example:
<programlisting>
C:\WINDOWS\system32&gt; C:\Sphinx\bin\searchd.exe --delete
</programlisting>
</listitem>

<listitem><option>--servicename &lt;name&gt;</option> applies the given name to
<filename>searchd</filename> when installing or deleting the service, as would appear
in the Management Console; this will default to searchd, but if being deployed on servers
where multiple administrators may log into the system, or a system with multiple
<filename>searchd</filename> instances, a more descriptive name may be applicable.
Note that unless combined with <option>--install</option> or <option>--delete</option>,
this option does not do anything. Example:
<programlisting>
C:\WINDOWS\system32&gt; C:\Sphinx\bin\searchd.exe --install
   --config C:\Sphinx\sphinx.conf --servicename SphinxSearch
</programlisting>
</listitem>

<listitem><option>--ntservice</option> is the option that is passed by the
Management Console to <filename>searchd</filename> to invoke it as a service
on Windows platforms. It would not normally be necessary to call this directly;
this would normally be called by Windows when the service would be started,
although if you wanted to call this as a regular service from the command-line
(as the complement to <option>--console</option>) you could do so in theory.
</listitem>

</itemizedlist>
<para>
Last but not least, as every other daemon, <filename>searchd</filename> supports a number of signals.
<variablelist>
<varlistentry>
	<term>SIGTERM</term>
	<listitem>Initiates a clean shutdown. New queries will not be handled; but queries
	that are already started will not be forcibly interrupted.</listitem>
</varlistentry>
<varlistentry>
	<term>SIGHUP</term>
	<listitem>Initiates index rotation. Depending on the value of
	<link linkend="conf-seamless-rotate">seamless_rotate</link> setting,
	new queries might be shortly stalled; clients will receive temporary
	errors.</listitem>
</varlistentry>
<varlistentry>
	<term>SIGUSR1</term>
	<listitem>Forces reopen of searchd log and query log files, letting
	you implement log file rotation.</listitem>
</varlistentry>
</variablelist>
</para>
</sect1>


<sect1 id="ref-search"><title><filename>search</filename> command reference</title>
<para><filename>search</filename> is one of the helper tools within the 
Sphinx package. Whereas <filename>searchd</filename> is responsible for 
searches in a server-type environment, <filename>search</filename> is 
aimed at testing the index from the command line, and testing the index 
quickly without building a framework to make the connection to the server 
and process its response.</para>
<para>Note: <filename>search</filename> is not intended to be deployed as 
part of a client application; it is strongly recommended you do not write 
an interface to <filename>search</filename> instead of 
<filename>searchd</filename>, and none of the bundled client APIs support 
this method. (In any event, <filename>search</filename> will reload files 
each time, whereas <filename>searchd</filename> will cache them in memory 
for performance.)</para>
<para>That said, many types of query that you could build in the APIs 
could also be made with <filename>search</filename>, however for very 
complex searches it may be easier to construct them using a small script 
and the corresponding API. Additionally, some newer features may be 
available in the <filename>searchd</filename> system that have not yet 
been brought into <filename>search</filename>.</para>
<para>The calling syntax for <filename>search</filename> is as 
follows:</para>
<programlisting>
search [OPTIONS] word1 [word2 [word3 [...]]]
</programlisting>
<para>When calling <filename>search</filename>, it is not necessary to 
have <filename>searchd</filename> running; simply that the account running 
<filename>search</filename> has read access to the configuration file and 
the location and files of the indexes.</para>
<para>The default behaviour is to apply a search for word1 (AND word2 AND 
word3... as specified) to all fields in all indexes as given in the 
configuration file. If constructing the equivalent in the API, this would 
be the equivalent to passing <option>SPH_MATCH_ALL</option> to 
<code>SetMatchMode</code>, and specifying <option>*</option> as the 
indexes to query as part of <code>Query</code>.</para>
<para>There are many options available to <filename>search</filename>. 
Firstly, the general options:
<itemizedlist>
<listitem><option>--config &lt;file&gt;</option> (<option>-c 
&lt;file&gt;</option> for short) tells <filename>search</filename> to use 
the given file as its configuration, just as with 
<filename>indexer</filename> above.</listitem>
<listitem><option>--index &lt;index&gt;</option> (<option>-i 
&lt;index&gt;</option> for short) tells <filename>search</filename> to 
limit searching to the specified index only; normally it would attempt to 
search all of the physical indexes listed in 
<filename>sphinx.conf</filename>, not any distributed ones.</listitem>
<listitem><option>--stdin</option> tells <filename>search</filename> to 
accept the query from the standard input, rather than the command line. 
This can be useful for testing purposes whereby you could feed input via 
pipes and from scripts.</listitem>
</itemizedlist>
</para>
<para>Options for setting matches:
<itemizedlist>
<listitem><option>--any</option> (<option>-a</option> for short) changes 
the matching mode to match any of the words as part of the query (word1 OR 
word2 OR word3). In the API this would be equivalent to passing 
<option>SPH_MATCH_ANY</option> to <code>SetMatchMode</code>.</listitem>
<listitem><option>--phrase</option> (<option>-p</option> for short) 
changes the matching mode to match all of the words as part of the query, 
and do so in the phrase given (not including punctuation). In the API this 
would be equivalent to passing <option>SPH_MATCH_PHRASE</option> to 
<code>SetMatchMode</code>.</listitem>
<listitem><option>--boolean</option> (<option>-b</option> for short) 
changes the matching mode to <link linkend="boolean-syntax">Boolean 
matching</link>. Note if using Boolean syntax matching on the command 
line, you may need to escape the symbols (with a backslash) to avoid the 
shell/command line processor applying them, such as ampersands being 
escaped on a Unix/Linux system to avoid it forking to the 
<filename>search</filename> process, although this can be resolved by 
using <option>--stdin</option>, as below. In the API this would be 
equivalent to passing <option>SPH_MATCH_BOOLEAN</option> to 
<code>SetMatchMode</code>.</listitem>
<listitem><option>--ext</option> (<option>-e</option> for short) changes 
the matching mode to <link linkend="extended-syntax">Extended 
matching</link>. In the API this would be equivalent to passing 
<option>SPH_MATCH_EXTENDED</option> to <code>SetMatchMode</code>, and it 
should be noted that use of this mode is being discouraged in favour of 
Extended2, below.</listitem>
<listitem><option>--ext2</option> (<option>-e2</option> for short) changes 
the matching mode to <link linkend="extended-syntax">Extended matching, 
version 2</link>. In the API this would be equivalent to passing 
<option>SPH_MATCH_EXTENDED2</option> to <code>SetMatchMode</code>, and it 
should be noted that use of this mode is being recommended in favour of 
Extended, due to being more efficient and providing other 
features.</listitem>
<listitem><option>--filter &lt;attr&gt; &lt;v&gt;</option> (<option>-f 
&lt;attr&gt; &lt;v&gt;</option> for short) filters the results such that 
only documents where the attribute given (attr) matches the value given 
(v). For example, <option>--filter deleted 0</option> only matches 
documents with an attribute called 'deleted' where its value is 0. You can 
also add multiple filters on the command line, by specifying multiple 
<option>--filter</option> multiple times, however if you apply a second 
filter to an attribute it will override the first defined 
filter.</listitem>
</itemizedlist>
</para>
<para>Options for handling the results:
<itemizedlist>
<listitem><option>--limit &lt;count&gt;</option> (<option>-l 
count</option> for short) limits the total number of matches back to the 
number given. If a 'group' is specified, this will be the number of 
grouped results. This defaults to 20 results if not specified (as do the 
APIs)</listitem>
<listitem><option>--offset &lt;count&gt;</option> (<option>-o 
&lt;count&gt;</option> for short) offsets the result list by the number of 
places set by the count; this would be used for pagination through 
results, where if you have 20 results per 'page', the second page would 
begin at offset 20, the third page at offset 40, etc.</listitem>
<listitem><option>--group &lt;attr&gt;</option> (<option>-g 
&lt;attr&gt;</option> for short) specifies that results should be grouped 
together based on the attribute specified. Like the GROUP BY clause in 
SQL, it will combine all results where the attribute given matches, and 
returns a set of results where each returned result is the best from each 
group. Unless otherwise specified, this will be the best match on 
relevance.</listitem>
<listitem><option>--groupsort &lt;expr&gt;</option> (<option>-gs 
&lt;expr&gt;</option> for short) instructs that when results are grouped 
with <option>-group</option>, the expression given in &lt;expr&gt; shall 
determine the order of the groups. Note, this does not specify which is 
the best item within the group, only the order in which the groups 
themselves shall be returned.</listitem>
<listitem><option>--sortby &lt;clause&gt;</option> (<option>-s 
&lt;clause&gt;</option> for short) specifies that results should be sorted 
in the order listed in &lt;clause&gt;. This allows you to specify the 
order you wish results to be presented in, ordering by different columns. 
For example, you could say <option>--sortby "@weight DESC entrytime 
DESC"</option> to sort entries first by weight (or relevance) and where 
two or more entries have the same weight, to then sort by the time with 
the highest time (newest) first. You will usually need to put the items in 
quotes (<option>--sortby "@weight DESC"</option>) or use commas 
(<option>--sortby @weight,DESC</option>) to avoid the items being treated 
separately. Additionally, like the regular sorting modes, if 
<option>--group</option> (grouping) is being used, this will state how to 
establish the best match within each group.</listitem>
<listitem><option>--sortexpr expr</option> (<option>-S expr</option> for 
short) specifies that the search results should be presented in an order 
determined by an arithmetic expression, stated in expr. For example: 
<option>--sortexpr "@weight + ( user_karma + ln(pageviews) )*0.1"</option> 
(again noting that this will have to be quoted to avoid the shell dealing 
with the asterisk). Extended sort mode is discussed in more detail under 
the <option>SPH_SORT_EXTENDED</option> entry under the <link 
linkend="sorting-modes">Sorting modes</link> section of the 
manual.</listitem>
<listitem><option>--sort=date</option> specifies that the results should 
be sorted by descending (i.e. most recent first) date. This requires that 
there is an attribute in the index that is set as a timestamp.</listitem>
<listitem><option>--rsort=date</option> specifies that the results should 
be sorted by ascending (i.e. oldest first) date. This requires that there 
is an attribute in the index that is set as a timestamp.</listitem>
<listitem><option>--sort=ts</option> specifies that the results should be 
sorted by timestamp in groups; it will return all of the documents whose 
timestamp is within the last hour, then sorted within that bracket for 
relevance. After, it would return the documents from the last day, sorted 
by relevance, then the last week and then the last month. It is discussed 
in more detail under the <option>SPH_SORT_TIME_SEGMENTS</option> entry 
under the <link linkend="sorting-modes">Sorting modes</link> section of 
the manual.</listitem>
</itemizedlist>
</para>
<para>Other options:
<itemizedlist>
<listitem><option>--noinfo</option> (<option>-q</option> for short) 
instructs <filename>search</filename> not to look-up data in your SQL 
database. Specifically, for debugging with MySQL and 
<filename>search</filename>, you can provide it with a query to look up 
the full article based on the returned document ID. It is explained in 
more detail under the <link 
linkend="conf-sql-query-info">sql_query_info</link> directive.</listitem>
</itemizedlist>
</para>
</sect1>


<sect1 id="ref-spelldump"><title><filename>spelldump</filename> command reference</title>
<para><filename>spelldump</filename> is one of the helper tools within the Sphinx package.</para>
<para>It is used to extract the contents of a dictionary file that uses 
<filename>ispell</filename> or <filename>MySpell</filename> format, which 
can help build word lists for <glossterm>wordforms</glossterm> - all of 
the possible forms are pre-built for you.</para>
<para>Its general usage is:</para>
<programlisting>
spelldump [options] &lt;dictionary&gt; &lt;affix&gt; [result] [locale-name]
</programlisting>
<para>The two main parameters are the dictionary's main file and its affix 
file; usually these are named as 
<filename>[language-prefix].dict</filename> and 
<filename>[language-prefix].aff</filename> and will be available with most 
common Linux distributions, as well as various places online.</para>
<para><option>[result]</option> specifies where the dictionary data should 
be output to, and <option>[locale-name]</option> additionally specifies 
the locale details you wish to use.</para>
<para>There is an additional option, <option>-c [file]</option>, which 
specifies a file for case conversion details.</para>
<para>Examples of its usage are:</para>
<programlisting>
spelldump en.dict en.aff
spelldump ru.dict ru.aff ru.txt ru_RU.CP1251
spelldump ru.dict ru.aff ru.txt .1251
</programlisting>
<para>The results file will contain a list of all the words in the 
dictionary in alphabetical order, output in the format of a wordforms file, 
which you can use to customise for your specific circumstances. An example 
of the result file:</para>
<programlisting>
zone &gt; zone
zoned &gt; zoned
zoning &gt; zoning
</programlisting>
</sect1>


<sect1 id="ref-indextool"><title><filename>indextool</filename> command reference</title>
<para>
<filename>indextool</filename> is one of the helper tools within
the Sphinx package, introduced in version 0.9.9-rc2. It is used to
dump miscellaneous debug information about the physical index.
(Additional functionality such as index verification is planned
in the future, hence the indextool name rather than just indexdump.)
Its general usage is:
</para>
<programlisting>
indextool &lt;command&gt; [options]
</programlisting>
<para>
The only currently available option applies to all commands
and lets you specify the configuration file:
<itemizedlist>
<listitem><option>--config &lt;file&gt;</option> (<option>-c &lt;file&gt;</option> for short)
overrides the built-in config file names.
</listitem>
</itemizedlist>
</para>
<para>
The commands are as follows:
</para>
<itemizedlist>
<listitem><option>--dumpheader FILENAME.sph</option> quickly dumps
the provided index header file without touching any other index files
or even the configuration file. The report provides a breakdown of
all the index settings, in particular the entire attribute and
field list. Prior to 0.9.9-rc2, this command was present in
CLI search utility.
</listitem>
<listitem><option>--dumpconfig FILENAME.sph</option> dumps
the index definition from the given index header file in (almost)
compliant <filename>sphinx.conf</filename> file format.
Added in version 1.11-beta.
</listitem>
<listitem><option>--dumpheader INDEXNAME</option> dumps index header
by index name with looking up the header path in the configuration file.
</listitem>
<listitem><option>--dumpdocids INDEXNAME</option> dumps document IDs
by index name. It takes the data from attribute (.spa) file and therefore
requires docinfo=extern to work.
</listitem>
<listitem><option>--dumphitlist INDEXNAME KEYWORD</option> dumps all
the hits (occurences) of a given keyword in a given index, with keyword
specified as text.
</listitem>
<listitem><option>--dumphitlist INDEXNAME --wordid ID</option> dumps all
the hits (occurences) of a given keyword in a given index, with keyword
specified as internal numeric ID.
</listitem>
<listitem><option>--htmlstrip INDEXNAME</option> filters stdin using
HTML stripper settings for a given index, and prints the filtering 
results to stdout. Note that the settings will be taken from sphinx.conf,
and not the index header.
</listitem>
<listitem><option>--check INDEXNAME</option> checks the index data
files for consistency errors that might be introduced either by bugs
in <filename>indexer</filename> and/or hardware faults.
</listitem>
<listitem><option>--strip-path</option> strips the path names from
all the file names referenced from the index (stopwords, wordforms,
exceptions, etc). This is useful for checking indexes built on another
machine with possibly different path layouts.
</listitem>
</itemizedlist>
</sect1>


</chapter>
<chapter id="sphinxql-reference"><title>SphinxQL reference</title>


<para>
SphinxQL is our SQL dialect that exposes all of the search daemon
functionality using a standard SQL syntax with a few Sphinx-specific
extensions.  Everything available via the SphinxAPI is also available
SphinxQL but not vice versa; for instance, writes into RT indexes
are only available via SphinxQL.  This chapter documents supported
SphinxQL statements syntax.
</para>


<sect1 id="sphinxql-select"><title>SELECT syntax</title>
<programlisting>
SELECT
    select_expr [, select_expr ...]
    FROM index [, index2 ...]
    [WHERE where_condition]
    [GROUP BY {col_name | expr_alias}]
    [ORDER BY {col_name | expr_alias} {ASC | DESC} [, ...]]
    [WITHIN GROUP ORDER BY {col_name | expr_alias} {ASC | DESC}]
    [LIMIT offset, row_count]
    [OPTION opt_name = opt_value [, ...]]
</programlisting>
<para>
<b>SELECT</b> statement was introduced in version 0.9.9-rc2.
It's syntax is based upon regular SQL but adds several Sphinx-specific
extensions and has a few omissions (such as (currently) missing support for JOINs).
Specifically,
<itemizedlist>
<listitem>Column list clause. Column names, arbitrary expressions,
and star ('*') are all allowed (ie.
<code>SELECT @id, group_id*123+456 AS expr1 FROM test1</code>
will work). Unlike in regular SQL, all computed expressions must be aliased
with a valid identifier. Starting with version 1.11-beta, <code>AS</code>
is optional. Special names such as @id and @weight should currently
be used with leading at-sign. This at-sign requirement will be lifted in
the future.
</listitem>
<listitem>FROM clause. FROM clause should contain the list of indexes
to search through. Unlike in regular SQL, comma means enumeration of
full-text indexes as in <link linkend="api-func-query">Query()</link>
API call rather than JOIN.
</listitem>
<listitem>WHERE clause. This clause will map both to fulltext query
and filters. Comparison operators (=, !=, &lt;, &gt;, &lt;=, &gt;=), IN,
AND, NOT, and BETWEEN are all supported and map directly to filters.
OR is not supported yet but will be in the future. MATCH('query')
is supported and maps to fulltext query. Query will be interpreted
according to <link linkend="extended-syntax">full-text query language rules</link>.
There must be at most one MATCH() in the clause. Starting with version
1.11-beta, <code>{col_name | expr_alias} [NOT] IN @uservar</code>
condition syntax is supported. (Refer to <xref linkend="sphinxql-set"/>
for a discussion of global user variables.)
</listitem>
<listitem>GROUP BY clause. Currently only supports grouping by a single
column. The column however can be a computed expression:
<programlisting>
SELECT *, group_id*1000+article_type AS gkey FROM example GROUP BY gkey
</programlisting>
Aggregate functions (AVG(), MIN(), MAX(), SUM()) in column list
clause are supported. Arguments to aggregate functions can be either
plain attributes or arbitrary expressions. COUNT(*) is implicitly
supported as using GROUP BY will add @count column to result set.
Explicit support might be added in the future. COUNT(DISTINCT attr)
is supported. Currently there can be at most one COUNT(DISTINCT)
per query and an argument needs to be an attribute. Both current
restrictions on COUNT(DISTINCT) might be lifted in the future.
<programlisting>
SELECT *, AVG(price) AS avgprice, COUNT(DISTINCT storeid)
FROM products
WHERE MATCH('ipod')
GROUP BY vendorid
</programlisting>
Starting with 1.11-beta, GROUP BY on a string attribute is supported,
with respect for current collation (see <xref linkend="collations"/>).
</listitem>
<listitem>WITHIN GROUP ORDER BY clause. This is a Sphinx specific
extension that lets you control how the best row within a group
will to be selected. The syntax matches that of regular ORDER BY
clause:
<programlisting>
SELECT *, INTERVAL(posted,NOW()-7*86400,NOW()-86400) AS timeseg
FROM example WHERE MATCH('my search query')
GROUP BY siteid
WITHIN GROUP ORDER BY @weight DESC
ORDER BY timeseg DESC, @weight DESC
</programlisting>
Starting with 1.11-beta, WITHIN GROUP ORDER BY on a string attribute is supported,
with respect for current collation (see <xref linkend="collations"/>).
</listitem>
<listitem>ORDER BY clause. Unlike in regular SQL, only column names
(not expressions) are allowed and explicit ASC and DESC are required.
The columns however can be computed expressions:
<programlisting>
SELECT *, @weight*10+docboost AS skey FROM example ORDER BY skey
</programlisting>
Starting with 1.11-beta, ORDER BY on a string attribute is supported,
with respect for current collation (see <xref linkend="collations"/>).
</listitem>
<listitem>LIMIT clause. Both LIMIT N and LIMIT M,N forms are supported.
Unlike in regular SQL (but like in Sphinx API), an implicit LIMIT 0,20
is present by default.
</listitem>
<listitem>OPTION clause. This is a Sphinx specific extension that
lets you control a number of per-query options. The syntax is:
<programlisting>
OPTION &lt;optionname&gt;=&lt;value&gt; [ , ... ]
</programlisting>
Supported options and respectively allowed values are:
<itemizedlist>
<listitem>'ranker' - any of 'proximity_bm25', 'bm25', 'none', 'wordcount', 'proximity', 'matchany', or 'fieldmask'</listitem>
<listitem>'max_matches' - integer (per-query max matches value)</listitem>
<listitem>'cutoff' - integer (max found matches threshold)</listitem>
<listitem>'max_query_time' - integer (max search time threshold, msec)</listitem>
<listitem>'retry_count' - integer (distributed retries count)</listitem>
<listitem>'retry_delay' - integer (distributed retry delay, msec)</listitem>
<listitem>'field_weights' - a named integer list (per-field user weights for ranking)</listitem>
<listitem>'index_weights' - a named integer list (per-index user weights for ranking)</listitem>
<listitem>'reverse_scan' - 0 or 1, lets you control the order in which full-scan query processes the rows</listitem>
</itemizedlist>
Example:
<programlisting>
SELECT * FROM test WHERE MATCH('@title hello @body world')
OPTION ranker=bm25, max_matches=3000,
    field_weights=(title=10, body=3)
</programlisting>
</listitem>
</itemizedlist>
</para>
</sect1>


<sect1 id="sphinxql-show-meta"><title>SHOW META syntax</title>
<programlisting>
SHOW META
</programlisting>
<para><b>SHOW META</b> shows additional meta-information about the latest
query such as query time and keyword statistics:
<programlisting>
mysql> SELECT * FROM test1 WHERE MATCH('test|one|two');
+------+--------+----------+------------+
| id   | weight | group_id | date_added |
+------+--------+----------+------------+
|    1 |   3563 |      456 | 1231721236 |
|    2 |   2563 |      123 | 1231721236 |
|    4 |   1480 |        2 | 1231721236 |
+------+--------+----------+------------+
3 rows in set (0.01 sec)

mysql> SHOW META;
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| total         | 3     |
| total_found   | 3     |
| time          | 0.005 |
| keyword[0]    | test  |
| docs[0]       | 3     |
| hits[0]       | 5     |
| keyword[1]    | one   |
| docs[1]       | 1     |
| hits[1]       | 2     |
| keyword[2]    | two   |
| docs[2]       | 1     |
| hits[2]       | 2     |
+---------------+-------+
12 rows in set (0.00 sec)
</programlisting>
</para>
</sect1>


<sect1 id="sphinxql-show-warnings"><title>SHOW WARNINGS syntax</title>
<programlisting>
SHOW WARNINGS
</programlisting>
<para><b>SHOW WARNINGS</b> statement, introduced in version 0.9.9-rc2,
can be used to retrieve the warning
produced by the latest query. The error message will be returned along with
the query itself:
<programlisting>
mysql&gt; SELECT * FROM test1 WHERE MATCH('@@title hello') \G
ERROR 1064 (42000): index test1: syntax error, unexpected TOK_FIELDLIMIT
near '@title hello'

mysql&gt; SELECT * FROM test1 WHERE MATCH('@title -hello') \G
ERROR 1064 (42000): index test1: query is non-computable (single NOT operator)

mysql&gt; SELECT * FROM test1 WHERE MATCH('"test doc"/3') \G
*************************** 1. row ***************************
        id: 4
    weight: 2500
  group_id: 2
date_added: 1231721236
1 row in set, 1 warning (0.00 sec)

mysql> SHOW WARNINGS \G
*************************** 1. row ***************************
  Level: warning
   Code: 1000
Message: quorum threshold too high (words=2, thresh=3); replacing quorum operator
         with AND operator
1 row in set (0.00 sec)
</programlisting>
</para>
</sect1>


<sect1 id="sphinxql-show-status"><title>SHOW STATUS syntax</title>
<para><b>SHOW STATUS</b>, introduced in version 0.9.9-rc2,
displays a number of useful performance counters. IO and CPU
counters will only be available if searchd was started with --iostats and --cpustats
switches respectively.
<programlisting>
mysql> SHOW STATUS;
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| uptime             | 216   |
| connections        | 3     |
| maxed_out          | 0     |
| command_search     | 0     |
| command_excerpt    | 0     |
| command_update     | 0     |
| command_keywords   | 0     |
| command_persist    | 0     |
| command_status     | 0     |
| agent_connect      | 0     |
| agent_retry        | 0     |
| queries            | 10    |
| dist_queries       | 0     |
| query_wall         | 0.075 |
| query_cpu          | OFF   |
| dist_wall          | 0.000 |
| dist_local         | 0.000 |
| dist_wait          | 0.000 |
| query_reads        | OFF   |
| query_readkb       | OFF   |
| query_readtime     | OFF   |
| avg_query_wall     | 0.007 |
| avg_query_cpu      | OFF   |
| avg_dist_wall      | 0.000 |
| avg_dist_local     | 0.000 |
| avg_dist_wait      | 0.000 |
| avg_query_reads    | OFF   |
| avg_query_readkb   | OFF   |
| avg_query_readtime | OFF   |
+--------------------+-------+
29 rows in set (0.00 sec)
</programlisting>
</para>
</sect1>


<sect1 id="sphinxql-insert"><title>INSERT and REPLACE syntax</title>
<programlisting>
{INSERT | REPLACE} INTO index [(column, ...)]
	VALUES (value, ...)
	[, (...)]
</programlisting>
<para>
INSERT statement, introduced in version 1.10-beta, is only supported for RT indexes.
It inserts new rows (documents) into an existing index, with the provided column values.
</para>
<para>
ID column must be present in all cases. Rows with duplicate IDs will <b>not</b>
be overwritten by INSERT; use REPLACE to do that.
</para>
<para>
<option>index</option> is the name of RT index into which the new row(s)
should be inserted. The optional column names list lets you only explicitly specify
values for some of the columns present in the index. All the other columns will be
filled with their default values (0 for scalar types, empty string for text types).
</para>
<para>
Expressions are not currently supported in INSERT and values should be explicitly
specified.
</para>
<para>
Multiple rows can be inserted using a single INSERT statement by providing
several comma-separated, parens-enclosed lists of rows values.
</para>
</sect1>


<sect1 id="sphinxql-delete"><title>DELETE syntax</title>
<programlisting>
DELETE FROM index WHERE {id = value | id IN (val1 [, val2 [, ...]])}
</programlisting>
<para>
DELETE statement, introduced in version 1.10-beta, is only supported for RT indexes.
It deletes existing rows (documents) from an existing index based on ID.
</para>
<para>
<option>index</option> is the name of RT index from which the row should be deleted.
<option>value</option> is the row ID to be deleted. Support for batch
<code>id IN (2,3,5)</code> syntax was added in version 1.11-beta.
</para>
<para>
Additional types of WHERE conditions (such as conditions on attributes, etc)
are planned, but not supported yet as of 1.10-beta.
</para>
</sect1>


<sect1 id="sphinxql-set"><title>SET syntax</title>
<programlisting>
SET [GLOBAL] server_variable_name = value
SET GLOBAL @user_variable_name = (int_val1 [, int_val2, ...])
</programlisting>
<para>
SET statement, introduced in version 1.10-beta, modifies a server variable value.
The variable names are case-insensitive. No variable value changes survive
server restart. There are the following classes of the variables:
<orderedlist>
<listitem>per-session server variable (1.10-beta and above)</listitem>
<listitem>global server variable (1.11-beta and above)</listitem>
<listitem>global user variable (1.11-beta and above)</listitem>
</orderedlist>
</para>
<para>
Global user variables are shared between concurrent sessions. Currently,
the only supported value type is the list of BIGINTs, and these variables
can only be used along with IN() for filtering purpose. The intended usage
scenario is uploading huge lists of values to <filename>searchd</filename>
(once) and reusing them (many times) later, saving on network overheads.
Example:
<programlisting>
// in session 1
mysql> SET GLOBAL @myfilter=(2,3,5,7,11,13);
Query OK, 0 rows affected (0.00 sec)

// later in session 2
mysql> SELECT * FROM test1 WHERE group_id IN @myfilter;
+------+--------+----------+------------+-----------------+------+
| id   | weight | group_id | date_added | title           | tag  |
+------+--------+----------+------------+-----------------+------+
|    3 |      1 |        2 | 1299338153 | another doc     | 15   |
|    4 |      1 |        2 | 1299338153 | doc number four | 7,40 |
+------+--------+----------+------------+-----------------+------+
2 rows in set (0.02 sec)
</programlisting>
</para>
<para>
Per-session and global server variables affect certain server settings in the respective scope.
Known per-session server variables are:
<variablelist>
<varlistentry>
<term><code>AUTOCOMMIT = {0 | 1}</code></term>
<listitem>
Whether any data modification statement should be implicitly
wrapped by BEGIN and COMMIT.
Introduced in version 1.10-beta.
</listitem>
</varlistentry>
<varlistentry>
<term><code>COLLATION_CONNECTION = collation_name</code></term>
<listitem>
Selects the collation to be used for ORDER BY or GROUP BY on string
values in the subsequent queries. Refer to <xref linkend="collations"/>
for a list of known collation names.
Introduced in version 1.11-beta.
</listitem>
</varlistentry>
<varlistentry>
<term><code>CHARACTER_SET_RESULTS = charset_name</code></term>
<listitem>
Does nothing; a placeholder to support frameworks, clients, and
connectors that attempt to automatically enforce a charset when
connecting to a Sphinx server.
Introduced in version 1.11-beta.
</listitem>
</varlistentry>
</variablelist>
</para>
<para>
Known global server variables are:
<variablelist>
<varlistentry>
<term><code>QUERY_LOG_FORMAT = {plain | sphinxql}</code></term>
<listitem>
Changes the current log format.
Introduced in version 1.11-beta.
</listitem>
</varlistentry>
<varlistentry>
<term><code>LOG_LEVEL = {info | debug | debugv | debugvv}</code></term>
<listitem>
Changes the current log verboseness level.
Introduced in version 1.11-beta.
</listitem>
</varlistentry>
</variablelist>
</para>
<para>
Examples:
<programlisting>
mysql> SET autocommit=0;
Query OK, 0 rows affected (0.00 sec)

mysql> SET GLOBAL query_log_format=sphinxql;
Query OK, 0 rows affected (0.00 sec)
</programlisting>
</para>
</sect1>


<sect1 id="sphinxql-commit"><title>BEGIN, COMMIT, and ROLLBACK syntax</title>
<programlisting>
START TRANSACTION | BEGIN
COMMIT
ROLLBACK
SET AUTOCOMMIT = {0 | 1}
</programlisting>
<para>
BEGIN, COMMIT, and ROLLBACK statements were introduced in version 1.10-beta.
BEGIN statement (or its START TRANSACTION alias) forcibly commits pending
transaction, if any, and begins a new one. COMMIT statement commits the current
transaction, making all its changes permanent. ROLLBACK statement rolls back the
current transaction, canceling all its changes. SET AUTOCOMMIT controls the
autocommit mode in the active session.
</para>
<para>
AUTOCOMMIT is set to 1 by default, meaning that every statement that perfoms
any changes on any index is implicitly wrapped in BEGIN and COMMIT.
</para>
<para>
Transactions are limited to a single RT index, and also limited in size.
They are atomic, consistent, overly isolated, and durable. Overly isolated
means that the changes are not only invisible to the concurrent transactions
but even to the current session itself.
</para>
</sect1>


<sect1 id="sphinxql-call-snippets"><title>CALL SNIPPETS syntax</title>
<programlisting>
CALL SNIPPETS(data, index, query[, opt_value AS opt_name[, ...]])
</programlisting>
<para>
CALL SNIPPETS statement, introduced in version 1.10-beta, builds a snippet
from provided data and query, using specified index settings.
</para>
<para>
<option>data</option> is the source data string to extract a snippet from.
<option>index</option> is the name of the index from which to take the text
processing settings. <option>query</option> is the full-text query to build
snippets for. Additional options are documented in
<xref linkend="api-func-buildexcerpts"/>. Usage example:
</para>
<programlisting>
CALL SNIPPETS('this is my document text', 'test1', 'hello world',
    5 AS around, 200 AS limit)
</programlisting>
</sect1>


<sect1 id="sphinxql-call-keywords"><title>CALL KEYWORDS syntax</title>
<programlisting>
CALL KEYWORDS(text, index, [hits])
</programlisting>
<para>
CALL KEYWORDS statement, introduced in version 1.10-beta, splits text
into particular keywords. It returns tokenized and normalized forms
of the keywords, and, optionally, keyword statistics.
</para>
<para>
<option>text</option> is the text to break down to keywords.
<option>index</option> is the name of the index from which to take the text
processing settings. <option>hits</option> is an optional boolean parameter
that specifies whether to return document and hit occurrence statistics.
</para>
</sect1>


<sect1 id="sphinxql-show-tables"><title>SHOW TABLES syntax</title>
<programlisting>
SHOW TABLES
</programlisting>
<para>
SHOW TABLES statement, introduced in version 1.11-beta, enumerates
all currently active indexes along with their types. As of 1.11-beta,
existing index types are <option>local</option>, <option>distributed</option>,
and <option>rt</option> respectively.
Example:
<programlisting>
mysql> SHOW TABLES;
+-------+-------------+
| Index | Type        |
+-------+-------------+
| dist1 | distributed |
| rt    | rt          |
| test1 | local       |
| test2 | local       |
+-------+-------------+
4 rows in set (0.00 sec)
</programlisting>
</para>
</sect1>


<sect1 id="sphinxql-describe"><title>DESCRIBE syntax</title>
<programlisting>
{DESC | DESCRIBE} index
</programlisting>
<para>
DESCRIBE statement, introduced in version 1.11-beta, lists
index columns and their associated types. Columns are document ID,
full-text fields, and attributes. The order matches that in which
fields and attributes are expected by INSERT and REPLACE statements.
As of 1.11-beta, column types are <option>field</option>,
<option>integer</option>, <option>timestamp</option>,
<option>ordinal</option>, <option>bool</option>,
<option>float</option>, <option>bigint</option>,
<option>string</option>, and <option>mva</option>.
ID column will be typed either <option>integer</option>
or <option>bigint</option> based on whether the binaries
were built with 32-bit or 64-bit document ID support.
Example:
</para>
<programlisting>
mysql> DESC rt;
+---------+---------+
| Field   | Type    |
+---------+---------+
| id      | integer |
| title   | field   |
| content | field   |
| gid     | integer |
+---------+---------+
4 rows in set (0.00 sec)
</programlisting>
</sect1>


<sect1 id="sphinxql-create-function"><title>CREATE FUNCTION syntax</title>
<programlisting>
CREATE FUNCTION udf_name
	RETURNS {INT | BIGINT | FLOAT}
	SONAME 'udf_lib_file'
</programlisting>
<para>
CREATE FUNCTION statement, introduced in version 1.11-beta,
installs a <link linkend="udf">user-defined function (UDF)</link>
with the given name and type from the given library file.
The library file must reside in a trusted
<link linkend="conf-plugin-dir">plugin_dir</link> directory.
On success, the function is available for use in all subsequent
queries that the server receives. Example:
</para>
<programlisting>
mysql> CREATE FUNCTION avgmva RETURNS INT SONAME 'udfexample.dll';
Query OK, 0 rows affected (0.03 sec)

mysql> SELECT *, AVGMVA(tag) AS q from test1;
+------+--------+---------+-----------+
| id   | weight | tag     | q         |
+------+--------+---------+-----------+
|    1 |      1 | 1,3,5,7 | 4.000000  |
|    2 |      1 | 2,4,6   | 4.000000  |
|    3 |      1 | 15      | 15.000000 |
|    4 |      1 | 7,40    | 23.500000 |
+------+--------+---------+-----------+
</programlisting>
</sect1>


<sect1 id="sphinxql-drop-function"><title>DROP FUNCTION syntax</title>
<programlisting>
DROP FUNCTION udf_name
</programlisting>
<para>
DROP FUNCTION statement, introduced in version 1.11-beta,
deinstalls a <link linkend="udf">user-defined function (UDF)</link>
with the given name. On success, the function is no longer available
for use in subsequent queries. Pending concurrent queries will not be
affected and the library unload, if necessary, will be postponed
until those queries complete. Example:
</para>
<programlisting>
mysql> DROP FUNCTION avgmva;
Query OK, 0 rows affected (0.00 sec)
</programlisting>
</sect1>


<sect1 id="sphinxql-show-variables"><title>SHOW VARIABLES syntax</title>
<programlisting>
SHOW VARIABLES
</programlisting>
<para>
Added in version 1.11-beta, this is currently a placeholder 
query that does nothing and reports success. That is in order
to keep compatibility with frameworks and connectors that
automatically execute this statement.
</para>
<programlisting>
mysql> SHOW VARIABLES;
Query OK, 0 rows affected (0.00 sec)
</programlisting>
</sect1>


<sect1 id="sphinxql-show-variables"><title>SHOW COLLATION syntax</title>
<programlisting>
SHOW COLLATION
</programlisting>
<para>
Added in version 1.11-beta, this is currently a placeholder 
query that does nothing and reports success. That is in order
to keep compatibility with frameworks and connectors that
automatically execute this statement.
</para>
<programlisting>
mysql> SHOW COLLATION;
Query OK, 0 rows affected (0.00 sec)
</programlisting>
</sect1>


<sect1 id="sphinxql-update"><title>UPDATE syntax</title>
<programlisting>
UPDATE index SET col1 = newval1 [, ...] WHERE ID = docid
</programlisting>
<para>
UPDATE statement was added in version 1.11-beta. It can currently
update 32-bit integer attributes only. Multiple attributes and values
can be specified. Both RT and disk indexes are supported.
Updates on other attribute types are also planned.
</para>
<programlisting>
mysql> UPDATE myindex SET enabled=0 WHERE id=123;
Query OK, 1 rows affected (0.00 sec)
</programlisting>
</sect1>


<sect1 id="sphinxql-multi-queries">
<title>Multi-statement queries</title>
<para>
Starting version 1.11-beta, SphinxQL supports multi-statement
queries, or batches. Possible inter-statement optimizations described
in <xref linkend="multi-queries"/> do apply to SphinxQL just as well.
The batched queries should be separated by a semicolon. Your MySQL
client library needs to support MySQL multi-query mechanism and
multiple result set. For instance, mysqli interface in PHP
and DBI/DBD libraries in Perl are known to work.
</para>
<para>
Here's a PHP sample showing how to utilize mysqli interface
with Sphinx.
<programlisting><![CDATA[
<?php

$link = mysqli_connect ( "127.0.0.1", "root", "", "", 9306 );
if ( mysqli_connect_errno() )
    die ( "connect failed: " . mysqli_connect_error() );

$batch = "SELECT * FROM test1 ORDER BY group_id ASC;";
$batch .= "SELECT * FROM test1 ORDER BY group_id DESC";

if ( !mysqli_multi_query ( $link, $batch ) )
    die ( "query failed" );

do
{
    // fetch and print result set
    if ( $result = mysqli_store_result($link) )
    {
        while ( $row = mysqli_fetch_row($result) )
            printf ( "id=%s\n", $row[0] );
        mysqli_free_result($result);
    }

    // print divider
    if ( mysqli_more_results($link) )
        printf ( "------\n" );

} while ( mysqli_next_result($link) );
]]></programlisting>
Its output with the sample <code>test1</code> index included
with Sphinx is as follows.
<programlisting>
$ php test_multi.php
id=1
id=2
id=3
id=4
------
id=3
id=4
id=1
id=2
</programlisting>
</para>
<para>
The following statements can currently be used in a batch:
SELECT, SHOW WARNINGS, SHOW STATUS, and SHOW META. Arbitrary
sequence of these statements are allowed. The results sets
returned should match those that would be returned if the
batched queries were sent one by one.
</para>
</sect1>


<sect1 id="sphinxql-comment-syntax">
<title>Comment syntax</title>
<para>
Since version 1.11-beta, SphinxQL supports C-style comment syntax.
Everything from an opening <code>/*</code> sequence to a closing
<code>*/</code> sequence is ignored. Comments can span multiple lines,
can not nest, and should not get logged. MySQL specific
<code>/*! ... */</code> comments are also currently ignored.
(As the comments support was rather added for better compatibility
with <filename>mysqldump</filename> produced dumps, rather than
improving generaly query interoperability between Sphinx and MySQL.)
<programlisting>
SELECT /*! SQL_CALC_FOUND_ROWS */ col1 FROM table1 WHERE ...
</programlisting>
</para>
</sect1>


<sect1 id="sphinxql-reserved-keywords">
<title>List of SphinxQL reserved keywords</title>
<para>A complete alphabetical list of keywords that are currently reserved
in SphinxQL syntax (and therefore can not be used as identifiers).
<programlisting>
AND
AS
ASC
AVG
BEGIN
BETWEEN
BY
CALL
COLLATION
COMMIT
COUNT
DELETE
DESC
DESCRIBE
DISTINCT
FALSE
FROM
GLOBAL
GROUP
ID
IN
INSERT
INTO
LIMIT
MATCH
MAX
META
MIN
NOT
NULL
OPTION
OR
ORDER
REPLACE
ROLLBACK
SELECT
SET
SHOW
START
STATUS
SUM
TABLES
TRANSACTION
TRUE
UPDATE
VALUES
VARIABLES
WARNINGS
WEIGHT
WHERE
WITHIN
</programlisting></para>
</sect1>


</chapter>
<chapter id="api-reference"><title>API reference</title>


<para>
There is a number of native searchd client API implementations
for Sphinx. As of time of this writing, we officially support our own
PHP, Python, and Java implementations. There also are third party
free, open-source API implementations for Perl, Ruby, and C++.
</para>
<para>
The reference API implementation is in PHP, because (we believe)
Sphinx is most widely used with PHP than any other language.
This reference documentation is in turn based on reference PHP API,
and all code samples in this section will be given in PHP.
</para>
<para>
However, all other APIs provide the same methods and implement
the very same network protocol. Therefore the documentation does
apply to them as well. There might be minor differences as to the
method naming conventions or specific data structures used.
But the provided functionality must not differ across languages.
</para>


<sect1 id="api-funcgroup-general"><title>General API functions</title>


<sect2 id="api-func-getlasterror"><title>GetLastError</title>
<para><b>Prototype:</b> function GetLastError()</para>
<para>
Returns last error message, as a string, in human readable format.
If there were no errors during the previous API call, empty string is returned.
</para>
<para>
You should call it when any other function (such as <link linkend="api-func-query">Query()</link>)
fails (typically, the failing function returns false). The returned string will
contain the error description.
</para>
<para>
The error message is <emphasis>not</emphasis> reset by this call; so you can safely
call it several times if needed.
</para>
</sect2>

<sect2 id="api-func-getlastwarning"><title>GetLastWarning</title>
<para><b>Prototype:</b> function GetLastWarning ()</para>
<para>
Returns last warning message, as a string, in human readable format.
If there were no warnings during the previous API call, empty string is returned.
</para>
<para>
You should call it to verify whether your request
(such as <link linkend="api-func-query">Query()</link>) was completed but with warnings.
For instance, search query against a distributed index might complete
succesfully even if several remote agents timed out. In that case,
a warning message would be produced.
</para>
<para>
The warning message is <emphasis>not</emphasis> reset by this call; so you can safely
call it several times if needed.
</para>
</sect2>

<sect2 id="api-func-setserver"><title>SetServer</title>
<para><b>Prototype:</b> function SetServer ( $host, $port )</para>
<para>
Sets <filename>searchd</filename> host name and TCP port.
All subsequent requests will use the new host and port settings.
Default host and port are 'localhost' and 9312, respectively.
</para>
</sect2>

<sect2 id="api-func-setretries"><title>SetRetries</title>
<para><b>Prototype:</b> function SetRetries ( $count, $delay=0 )</para>
<para>
Sets distributed retry count and delay.
</para>
<para>
On temporary failures <filename>searchd</filename> will attempt up to
<code>$count</code> retries per agent. <code>$delay</code> is the delay
between the retries, in milliseconds. Retries are disabled by default.
Note that this call will <b>not</b> make the API itself retry on
temporary failure; it only tells <filename>searchd</filename> to do so.
Currently, the list of temporary failures includes all kinds of connect()
failures and maxed out (too busy) remote agents.
</para>
</sect2>

<sect2 id="api-func-setconnecttimeout"><title>SetConnectTimeout</title>
<para><b>Prototype:</b> function SetConnectTimeout ( $timeout )</para>
<para>
Sets the time allowed to spend connecting to the server before giving up.
</para>
<para>Under some circumstances, the server can be delayed in responding, either
due to network delays, or a query backlog. In either instance, this allows
the client application programmer some degree of control over how their
program interacts with <filename>searchd</filename> when not available,
and can ensure that the client application does not fail due to exceeding
the script execution limits (especially in PHP).
</para>
<para>In the event of a failure to connect, an appropriate error code should
be returned back to the application in order for application-level error handling
to advise the user.
</para>
</sect2>

<sect2 id="api-func-setarrayresult"><title>SetArrayResult</title>
<para><b>Prototype:</b> function SetArrayResult ( $arrayresult )</para>
<para>
PHP specific. Controls matches format in the search results set
(whether matches should be returned as an array or a hash).
</para>
<para>
<code>$arrayresult</code> argument must be boolean. If <code>$arrayresult</code> is <code>false</code>
(the default mode), matches will returned in PHP hash format with
document IDs as keys, and other information (weight, attributes)
as values. If <code>$arrayresult</code> is true, matches will be returned
as a plain array with complete per-match information including
document ID.
</para>
<para>
Introduced along with GROUP BY support on MVA attributes.
Group-by-MVA result sets may contain duplicate document IDs.
Thus they need to be returned as plain arrays, because hashes
will only keep one entry per document ID.
</para>
</sect2>


<sect2 id="api-func-isconnecterror"><title>IsConnectError</title>
<para><b>Prototype:</b> function IsConnectError ()</para>
<para>
Checks whether the last error was a network error on API side, or a remote error
reported by searchd. Returns true if the last connection attempt to searchd failed on API side,
false otherwise (if the error was remote, or there were no connection attempts at all).
Introduced in version 0.9.9-rc1.
</para>
</sect2>


</sect1>
<sect1 id="api-funcgroup-general-query-settings"><title>General query settings</title>


<sect2 id="api-func-setlimits"><title>SetLimits</title>
<para><b>Prototype:</b> function SetLimits ( $offset, $limit, $max_matches=0, $cutoff=0 )</para>
<para>
Sets offset into server-side result set (<code>$offset</code>) and amount of matches
to return to client starting from that offset (<code>$limit</code>). Can additionally
control maximum server-side result set size for current query (<code>$max_matches</code>)
and the threshold amount of matches to stop searching at (<code>$cutoff</code>).
All parameters must be non-negative integers.
</para>
<para>
First two parameters to SetLimits() are identical in behavior to MySQL
LIMIT clause. They instruct <filename>searchd</filename> to return at
most <code>$limit</code> matches starting from match number <code>$offset</code>.
The default offset and limit settings are 0 and 20, that is, to return
first 20 matches.
</para>
<para>
<code>max_matches</code> setting controls how much matches <filename>searchd</filename>
will keep in RAM while searching. <b>All</b> matching documents will be normally
processed, ranked, filtered, and sorted even if <code>max_matches</code> is set to 1.
But only best N documents are stored in memory at any given moment for performance
and RAM usage reasons, and this setting controls that N. Note that there are
<b>two</b> places where <code>max_matches</code> limit is enforced. Per-query
limit is controlled by this API call, but there also is per-server limit
controlled by <code>max_matches</code> setting in the config file. To prevent
RAM usage abuse, server will not allow to set per-query limit
higher than the per-server limit.
</para>
<para>
You can't retrieve more than <code>max_matches</code> matches to the client application.
The default limit is set to 1000. Normally, you must not have to go over
this limit. One thousand records is enough to present to the end user.
And if you're thinking about pulling the results to application
for further sorting or filtering, that would be <b>much</b> more efficient
if performed on Sphinx side.
</para>
<para>
<code>$cutoff</code> setting is intended for advanced performance control.
It tells <filename>searchd</filename> to forcibly stop search query
once <code>$cutoff</code> matches had been found and processed.
</para>
</sect2>

<sect2 id="api-func-setmaxquerytime"><title>SetMaxQueryTime</title>
<para><b>Prototype:</b> function SetMaxQueryTime ( $max_query_time )</para>
<para>
Sets maximum search query time, in milliseconds. Parameter must be
a non-negative integer. Default valus is 0 which means "do not limit".
</para>
<para>Similar to <code>$cutoff</code> setting from <link linkend="api-func-setlimits">SetLimits()</link>,
but limits elapsed query time instead of processed matches count. Local search queries
will be stopped once that much time has elapsed. Note that if you're performing
a search which queries several local indexes, this limit applies to each index
separately.
</para>
</sect2>

<sect2 id="api-func-setoverride"><title>SetOverride</title>
<para><b>Prototype:</b> function SetOverride ( $attrname, $attrtype, $values )</para>
<para>
Sets temporary (per-query) per-document attribute value overrides.
Only supports scalar attributes. $values must be a hash that maps document
IDs to overridden attribute values. Introduced in version 0.9.9-rc1.
</para>
<para>
Override feature lets you "temporary" update attribute values for some documents
within a single query, leaving all other queries unaffected. This might be useful
for personalized data. For example, assume you're implementing a personalized
search function that wants to boost the posts that the user's friends recommend.
Such data is not just dynamic, but also personal; so you can't simply put it
in the index because you don't want everyone's searches affected. Overrides,
on the other hand, are local to a single query and invisible to everyone else.
So you can, say, setup a "friends_weight" value for every document, defaulting to 0,
then temporary override it with 1 for documents 123, 456 and 789 (recommended by
exactly the friends of current user), and use that value when ranking.
</para>
</sect2>


<sect2 id="api-func-setselect"><title>SetSelect</title>
<para><b>Prototype:</b> function SetSelect ( $clause )</para>
<para>
Sets the select clause, listing specific attributes to fetch, and <link linkend="sort-expr">expressions</link>
to compute and fetch. Clause syntax mimics SQL. Introduced in version 0.9.9-rc1.</para>
<para>
SetSelect() is very similar to the part of a typical SQL query between SELECT and FROM.
It lets you choose what attributes (columns) to fetch, and also what expressions
over the columns to compute and fetch. A certain difference from SQL is that expressions
<b>must</b> always be aliased to a correct identifier (consisting of letters and digits)
using 'AS' keyword. SQL also lets you do that but does not require to. Sphinx enforces
aliases so that the computation results can always be returned under a "normal" name
in the result set, used in other clauses, etc. 
</para>
<para>
Everything else is basically identical to SQL. Star ('*') is supported.
Functions are supported. Arbitrary amount of expressions is supported.
Computed expressions can be used for sorting, filtering, and grouping,
just as the regular attributes.
</para>
<para>
Starting with version 0.9.9-rc2, aggregate functions (AVG(), MIN(),
MAX(), SUM()) are supported when using GROUP BY.
</para>
<para>
Expression sorting (<xref linkend="sort-expr"/>) and geodistance functions
(<xref linkend="api-func-setgeoanchor"/>) are now internally implemented using
this computed expressions mechanism, using magic names '@expr' and '@geodist'
respectively.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
$cl->SetSelect ( "*, @weight+(user_karma+ln(pageviews))*0.1 AS myweight" );
$cl->SetSelect ( "exp_years, salary_gbp*{$gbp_usd_rate} AS salary_usd,
   IF(age>40,1,0) AS over40" );
$cl->SetSelect ( "*, AVG(price) AS avgprice" );
</programlisting>
</sect2>


</sect1>
<sect1 id="api-funcgroup-fulltext-query-settings"><title>Full-text search query settings</title>


<sect2 id="api-func-setmatchmode"><title>SetMatchMode</title>
<para><b>Prototype:</b> function SetMatchMode ( $mode )</para>
<para>
Sets full-text query matching mode, as described in <xref linkend="matching-modes"/>.
Parameter must be a constant specifying one of the known modes.
</para>
<para>
<b>WARNING:</b> (PHP specific) you <b>must not</b> take the matching mode
constant name in quotes, that syntax specifies a string and is incorrect:
<programlisting>
$cl->SetMatchMode ( "SPH_MATCH_ANY" ); // INCORRECT! will not work as expected
$cl->SetMatchMode ( SPH_MATCH_ANY ); // correct, works OK
</programlisting>
</para>
</sect2>

<sect2 id="api-func-setrankingmode"><title>SetRankingMode</title>
<para><b>Prototype:</b> function SetRankingMode ( $ranker )</para>
<para>
Sets ranking mode. Only available in SPH_MATCH_EXTENDED2 matching
mode at the time of this writing. Parameter must be a constant
specifying one of the known modes.
</para>
<para>
By default, Sphinx computes two factors which contribute to the final
match weight. The major part is query phrase proximity to document text.
The minor part is so-called BM25 statistical function, which varies
from 0 to 1 depending on the keyword frequency within document
(more occurrences yield higher weight) and within the whole index
(more rare keywords yield higher weight).
</para>
<para>
However, in some cases you'd want to compute weight differently -
or maybe avoid computing it at all for performance reasons because
you're sorting the result set by something else anyway. This can be
accomplished by setting the appropriate ranking mode.
</para>
<para>
Currently implemented modes are:
<itemizedlist>
<listitem>
SPH_RANK_PROXIMITY_BM25, default ranking mode which uses and combines
both phrase proximity and BM25 ranking.
</listitem>
<listitem>
SPH_RANK_BM25, statistical ranking mode which uses BM25 ranking only (similar to
most other full-text engines). This mode is faster but may result in worse quality
on queries which contain more than 1 keyword.
</listitem>
<listitem>
SPH_RANK_NONE, disabled ranking mode. This mode is the fastest.
It is essentially equivalent to boolean searching. A weight of 1 is assigned
to all matches.
</listitem>
<listitem>SPH_RANK_WORDCOUNT, ranking by keyword occurrences count. This ranker
computes the amount of per-field keyword occurrences, then multiplies the
amounts by field weights, then sums the resulting values for the final result.
</listitem>
<listitem>
SPH_RANK_PROXIMITY, added in version 0.9.9-rc1, returns raw phrase proximity
value as a result. This mode is internally used to emulate SPH_MATCH_ALL queries.
</listitem>
<listitem>
SPH_RANK_MATCHANY, added in version 0.9.9-rc1, returns rank as it was computed
in SPH_MATCH_ANY mode ealier, and is internally used to emulate SPH_MATCH_ANY queries.
</listitem>
<listitem>
SPH_RANK_FIELDMASK, added in version 0.9.9-rc2, returns a 32-bit mask with
N-th bit corresponding to N-th fulltext field, numbering from 0. The bit will
only be set when the respective field has any keyword occurences satisfiying
the query.
</listitem>
<listitem>
SPH_RANK_SPH04, added in version 1.10-beta, is generally based on the default
SPH_RANK_PROXIMITY_BM25 ranker, but additionally boosts the matches when
they occur in the very beginning or the very end of a text field. Thus,
if a field equals the exact query, SPH04 should rank it higher than a field
that contains the exact query but is not equal to it. (For instance, when
the query is "Hyde Park", a document entitled "Hyde Park" should be ranked
higher than a one entitled "Hyde Park, London" or "The Hyde Park Cafe".)
</listitem>
</itemizedlist>
</para>
</sect2>


<sect2 id="api-func-setsortmode"><title>SetSortMode</title>
<para><b>Prototype:</b> function SetSortMode ( $mode, $sortby="" )</para>
<para>
Set matches sorting mode, as described in <xref linkend="sorting-modes"/>.
Parameter must be a constant specifying one of the known modes.
</para>
<para>
<b>WARNING:</b> (PHP specific) you <b>must not</b> take the matching mode
constant name in quotes, that syntax specifies a string and is incorrect:
<programlisting>
$cl->SetSortMode ( "SPH_SORT_ATTR_DESC" ); // INCORRECT! will not work as expected
$cl->SetSortMode ( SPH_SORT_ATTR_ASC ); // correct, works OK
</programlisting>
</para>
</sect2>

<sect2 id="api-func-setweights"><title>SetWeights</title>
<para><b>Prototype:</b> function SetWeights ( $weights )</para>
<para>
Binds per-field weights in the order of appearance in the index.
<b>DEPRECATED</b>, use <link linkend="api-func-setfieldweights">SetFieldWeights()</link> instead.
</para>
</sect2>

<sect2 id="api-func-setfieldweights"><title>SetFieldWeights</title>
<para><b>Prototype:</b> function SetFieldWeights ( $weights )</para>
<para>
Binds per-field weights by name. Parameter must be a hash (associative array)
mapping string field names to integer weights.
</para>
<para>
Match ranking can be affected by per-field weights. For instance,
see <xref linkend="weighting"/> for an explanation how phrase proximity
ranking is affected. This call lets you specify what non-default
weights to assign to different full-text fields.
</para>
<para>
The weights must be positive 32-bit integers. The final weight
will be a 32-bit integer too. Default weight value is 1. Unknown
field names will be silently ignored.
</para>
<para>
There is no enforced limit on the maximum weight value at the
moment. However, beware that if you set it too high you can start
hitting 32-bit wraparound issues. For instance, if you set
a weight of 10,000,000 and search in extended mode, then
maximum possible weight will be equal to 10 million (your weight)
by 1 thousand (internal BM25 scaling factor, see <xref linkend="weighting"/>)
by 1 or more (phrase proximity rank). The result is at least 10 billion
that does not fit in 32 bits and will be wrapped around, producing
unexpected results.
</para>
</sect2>

<sect2 id="api-func-setindexweights"><title>SetIndexWeights</title>
<para><b>Prototype:</b> function SetIndexWeights ( $weights )</para>
<para>
Sets per-index weights, and enables weighted summing of match weights
across different indexes. Parameter must be a hash (associative array)
mapping string index names to integer weights. Default is empty array
that means to disable weighting summing.
</para>
<para>
When a match with the same document ID is found in several different
local indexes, by default Sphinx simply chooses the match from the index
specified last in the query. This is to support searching through
partially overlapping index partitions.
</para>
<para>
However in some cases the indexes are not just partitions, and you
might want to sum the weights across the indexes instead of picking one.
<code>SetIndexWeights()</code> lets you do that. With summing enabled,
final match weight in result set will be computed as a sum of match
weight coming from the given index multiplied by respective per-index
weight specified in this call. Ie. if the document 123 is found in
index A with the weight of 2, and also in index B with the weight of 3,
and you called <code>SetIndexWeights ( array ( "A"=>100, "B"=>10 ) )</code>,
the final weight return to the client will be 2*100+3*10 = 230.
</para>
</sect2>


</sect1>
<sect1 id="api-funcgroup-filtering"><title>Result set filtering settings</title>


<sect2 id="api-func-setidrange"><title>SetIDRange</title>
<para><b>Prototype:</b> function SetIDRange ( $min, $max )</para>
<para>
Sets an accepted range of document IDs. Parameters must be integers.
Defaults are 0 and 0; that combination means to not limit by range.
</para>
<para>
After this call, only those records that have document ID
between <code>$min</code> and <code>$max</code> (including IDs
exactly equal to <code>$min</code> or <code>$max</code>)
will be matched.
</para>
</sect2>

<sect2 id="api-func-setfilter"><title>SetFilter</title>
<para><b>Prototype:</b> function SetFilter ( $attribute, $values, $exclude=false )</para>
<para>
Adds new integer values set filter. 
</para>
<para>
On this call, additional new filter is added to the existing
list of filters. <code>$attribute</code> must be a string with
attribute name. <code>$values</code> must be a plain array
containing integer values. <code>$exclude</code> must be a boolean
value; it controls whether to accept the matching documents
(default mode, when <code>$exclude</code> is false) or reject them.
</para>
<para>
Only those documents where <code>$attribute</code> column value
stored in the index matches any of the values from <code>$values</code>
array will be matched (or rejected, if <code>$exclude</code> is true).
</para>
</sect2>

<sect2 id="api-func-setfilterrange"><title>SetFilterRange</title>
<para><b>Prototype:</b> function SetFilterRange ( $attribute, $min, $max, $exclude=false )</para>
<para>
Adds new integer range filter.
</para>
<para>
On this call, additional new filter is added to the existing
list of filters. <code>$attribute</code> must be a string with
attribute name. <code>$min</code> and <code>$max</code> must be
integers that define the acceptable attribute values range
(including the boundaries). <code>$exclude</code> must be a boolean
value; it controls whether to accept the matching documents
(default mode, when <code>$exclude</code> is false) or reject them.
</para>
<para>
Only those documents where <code>$attribute</code> column value
stored in the index is between <code>$min</code> and <code>$max</code>
(including values that are exactly equal to <code>$min</code> or <code>$max</code>)
will be matched (or rejected, if <code>$exclude</code> is true).
</para>
</sect2>

<sect2 id="api-func-setfilterfloatrange"><title>SetFilterFloatRange</title>
<para><b>Prototype:</b> function SetFilterFloatRange ( $attribute, $min, $max, $exclude=false )</para>
<para>
Adds new float range filter.
</para>
<para>
On this call, additional new filter is added to the existing
list of filters. <code>$attribute</code> must be a string with
attribute name. <code>$min</code> and <code>$max</code> must be
floats that define the acceptable attribute values range
(including the boundaries). <code>$exclude</code> must be a boolean
value; it controls whether to accept the matching documents
(default mode, when <code>$exclude</code> is false) or reject them.
</para>
<para>
Only those documents where <code>$attribute</code> column value
stored in the index is between <code>$min</code> and <code>$max</code>
(including values that are exactly equal to <code>$min</code> or <code>$max</code>)
will be matched (or rejected, if <code>$exclude</code> is true).
</para>
</sect2>

<sect2 id="api-func-setgeoanchor"><title>SetGeoAnchor</title>
<para><b>Prototype:</b> function SetGeoAnchor ( $attrlat, $attrlong, $lat, $long )</para>
<para>
Sets anchor point for and geosphere distance (geodistance) calculations, and enable them.
</para>
<para>
<code>$attrlat</code> and <code>$attrlong</code> must be strings that contain the names
of latitude and longitude attributes, respectively. <code>$lat</code> and <code>$long</code>
are floats that specify anchor point latitude and longitude, in radians.
</para>
<para>
Once an anchor point is set, you can use magic <code>"@geodist"</code> attribute
name in your filters and/or sorting expressions. Sphinx will compute geosphere distance
between the given anchor point and a point specified by latitude and lognitude
attributes from each full-text match, and attach this value to the resulting match.
The latitude and longitude values both in <code>SetGeoAnchor</code> and the index
attribute data are expected to be in radians. The result will be returned in meters,
so geodistance value of 1000.0 means 1 km. 1 mile is approximately 1609.344 meters.
</para>
</sect2>


</sect1>
<sect1 id="api-funcgroup-groupby"><title>GROUP BY settings</title>


<sect2 id="api-func-setgroupby"><title>SetGroupBy</title>
<para><b>Prototype:</b> function SetGroupBy ( $attribute, $func, $groupsort="@group desc" )</para>
<para>
Sets grouping attribute, function, and groups sorting mode; and enables grouping
(as described in <xref linkend="clustering"/>).
</para>
<para>
<code>$attribute</code> is a string that contains group-by attribute name.
<code>$func</code> is a constant that chooses a function applied to the attribute value in order to compute group-by key.
<code>$groupsort</code> is a clause that controls how the groups will be sorted. Its syntax is similar
to that described in <xref linkend="sort-extended"/>.
</para>
<para>
Grouping feature is very similar in nature to GROUP BY clause from SQL.
Results produces by this function call are going to be the same as produced
by the following pseudo code:
<programlisting>
SELECT ... GROUP BY $func($attribute) ORDER BY $groupsort
</programlisting>
Note that it's <code>$groupsort</code> that affects the order of matches
in the final result set. Sorting mode (see <xref linkend="api-func-setsortmode"/>)
affect the ordering of matches <emphasis>within</emphasis> group, ie.
what match will be selected as the best one from the group.
So you can for instance order the groups by matches count
and select the most relevant match within each group at the same time.
</para>
<para>
Starting with version 0.9.9-rc2, aggregate functions (AVG(), MIN(),
MAX(), SUM()) are supported through <link linkend="api-func-setselect">SetSelect()</link> API call
when using GROUP BY.
</para>
<para>
Starting with version 1.11-beta, grouping on string attributes
is supported, with respect to current collation.
</para>
</sect2>

<sect2 id="api-func-setgroupdistinct"><title>SetGroupDistinct</title>
<para><b>Prototype:</b> function SetGroupDistinct ( $attribute )</para>
<para>
Sets attribute name for per-group distinct values count calculations.
Only available for grouping queries.
</para>
<para>
<code>$attribute</code> is a string that contains the attribute name.
For each group, all values of this attribute will be stored (as RAM limits
permit), then the amount of distinct values will be calculated and returned
to the client. This feature is similar to <code>COUNT(DISTINCT)</code>
clause in standard SQL; so these Sphinx calls:
<programlisting>
$cl->SetGroupBy ( "category", SPH_GROUPBY_ATTR, "@count desc" );
$cl->SetGroupDistinct ( "vendor" );
</programlisting>
can be expressed using the following SQL clauses:
<programlisting>
SELECT id, weight, all-attributes,
	COUNT(DISTINCT vendor) AS @distinct,
	COUNT(*) AS @count
FROM products
GROUP BY category
ORDER BY @count DESC
</programlisting>
In the sample pseudo code shown just above, <code>SetGroupDistinct()</code> call
corresponds to <code>COUNT(DISINCT vendor)</code> clause only.
<code>GROUP BY</code>, <code>ORDER BY</code>, and  <code>COUNT(*)</code>
clauses are all an equivalent of <code>SetGroupBy()</code> settings. Both queries
will return one matching row for each category. In addition to indexed attributes,
matches will also contain total per-category matches count, and the count
of distinct vendor IDs within each category.
</para>
</sect2>


</sect1>
<sect1 id="api-funcgroup-querying"><title>Querying</title>


<sect2 id="api-func-query"><title>Query</title>
<para><b>Prototype:</b> function Query ( $query, $index="*", $comment="" )</para>
<para>
Connects to <filename>searchd</filename> server, runs given search query
with current settings, obtains and returns the result set.
</para>
<para>
<code>$query</code> is a query string. <code>$index</code> is an index name (or names) string.
Returns false and sets <code>GetLastError()</code> message on general error. 
Returns search result set on success.
Additionally, the contents of <code>$comment</code> are sent to the query log, marked in square brackets, just before the search terms, which can be very useful for debugging.

Currently, the comment is limited to 128 characters.
</para>
<para>
Default value for <code>$index</code> is <code>"*"</code> that means
to query all local indexes. Characters allowed in index names include
Latin letters (a-z), numbers (0-9), minus sign (-), and underscore (_);
everything else is considered a separator. Therefore, all of the
following samples calls are valid and will search the same
two indexes:
<programlisting>
$cl->Query ( "test query", "main delta" );
$cl->Query ( "test query", "main;delta" );
$cl->Query ( "test query", "main, delta" );
</programlisting>
Index specification order matters. If document with identical IDs are found
in two or more indexes, weight and attribute values from the very last matching 
index will be used for sorting and returning to client (unless explicitly
overridden with <link linkend="api-func-setindexweights">SetIndexWeights()</link>). Therefore,
in the example above, matches from "delta" index will always win over
matches from "main".
</para>
<para>
On success, <code>Query()</code> returns a result set that contains
some of the found matches (as requested by <link linkend="api-func-setlimits">SetLimits()</link>)
and additional general per-query statistics. The result set is a hash
(PHP specific; other languages might utilize other structures instead
of hash) with the following keys and values:
<variablelist>
<varlistentry>
	<term>"matches":</term>
	<listitem>Hash which maps found document IDs to another small hash containing document weight and attribute values
		(or an array of the similar small hashes if <link linkend="api-func-setarrayresult">SetArrayResult()</link> was enabled).
	</listitem>
</varlistentry>
<varlistentry>
	<term>"total":</term>
	<listitem>Total amount of matches retrieved <emphasis>on server</emphasis> (ie. to the server side result set) by this query.
		You can retrieve up to this amount of matches from server for this query text with current query settings.
	</listitem>
</varlistentry>
<varlistentry>
	<term>"total_found":</term>
	<listitem>Total amount of matching documents in index (that were found and procesed on server).</listitem>
</varlistentry>
<varlistentry>
	<term>"words":</term>
	<listitem>Hash which maps query keywords (case-folded, stemmed, and otherwise processed) to a small hash with per-keyword statitics ("docs", "hits").</listitem>
</varlistentry>
<varlistentry>
	<term>"error":</term>
	<listitem>Query error message reported by <filename>searchd</filename> (string, human readable). Empty if there were no errors.</listitem>
</varlistentry>
<varlistentry>
	<term>"warning":</term>
	<listitem>Query warning message reported by <filename>searchd</filename> (string, human readable). Empty if there were no warnings.</listitem>
</varlistentry>
</variablelist>
</para>
<para>
It should be noted that <code>Query()</code> carries out the same actions as
<code>AddQuery()</code> and <code>RunQueries()</code> without the intermediate steps;
it is analoguous to a single <code>AddQuery()</code> call, followed by a corresponding
<code>RunQueries()</code>, then returning the first array element of matches
(from the first, and only, query.)
</para>
</sect2>

<sect2 id="api-func-addquery"><title>AddQuery</title>
<para><b>Prototype:</b> function AddQuery ( $query, $index="*", $comment="" )</para>
<para>
Adds additional query with current settings to multi-query batch.
<code>$query</code> is a query string. <code>$index</code> is an index name (or names) string.
Additionally if provided, the contents of <code>$comment</code> are sent to the query log,
marked in square brackets, just before the search terms, which can be very useful for debugging.
Currently, this is limited to 128 characters.
Returns index to results array returned from <link linkend="api-func-runqueries">RunQueries()</link>.
</para>
<para>
Batch queries (or multi-queries) enable <filename>searchd</filename> to perform internal
optimizations if possible. They also reduce network connection overheads and search process
creation overheads in all cases. They do not result in any additional overheads compared
to simple queries. Thus, if you run several different queries from your web page,
you should always consider using multi-queries.
</para>
<para>
For instance, running the same full-text query but with different
sorting or group-by settings will enable <filename>searchd</filename>
to perform expensive full-text search and ranking operation only once,
but compute multiple group-by results from its output.
</para>
<para>
This can be a big saver when you need to display not just plain
search results but also some per-category counts, such as the amount of
products grouped by vendor. Without multi-query, you would have to run several
queries which perform essentially the same search and retrieve the
same matches, but create result sets differently. With multi-query,
you simply pass all these querys in a single batch and Sphinx
optimizes the redundant full-text search internally.
</para>
<para>
<code>AddQuery()</code> internally saves full current settings state
along with the query, and you can safely change them afterwards for subsequent
<code>AddQuery()</code> calls. Already added queries will not be affected;
there's actually no way to change them at all. Here's an example:
<programlisting>
$cl->SetSortMode ( SPH_SORT_RELEVANCE );
$cl->AddQuery ( "hello world", "documents" );

$cl->SetSortMode ( SPH_SORT_ATTR_DESC, "price" );
$cl->AddQuery ( "ipod", "products" );

$cl->AddQuery ( "harry potter", "books" );

$results = $cl->RunQueries ();
</programlisting>
With the code above, 1st query will search for "hello world" in "documents" index
and sort results by relevance, 2nd query will search for "ipod" in "products"
index and sort results by price, and 3rd query will search for "harry potter"
in "books" index while still sorting by price. Note that 2nd <code>SetSortMode()</code> call
does not affect the first query (because it's already added) but affects both other
subsequent queries.
</para>
<para>
Additionally, any filters set up before an <code>AddQuery()</code> will fall through to subsequent
queries. So, if <code>SetFilter()</code> is called before the first query, the same filter
will be in place for the second (and subsequent) queries batched through <code>AddQuery()</code>
unless you call <code>ResetFilters()</code> first. Alternatively, you can add additional filters
as well.</para>
<para>This would also be true for grouping options and sorting options; no current sorting,
filtering, and grouping settings are affected by this call; so subsequent queries will reuse
current query settings.
</para>
<para>
<code>AddQuery()</code> returns an index into an array of results
that will be returned from <code>RunQueries()</code> call. It is simply
a sequentially increasing 0-based integer, ie. first call will return 0,
second will return 1, and so on. Just a small helper so you won't have
to track the indexes manualy if you need then.
</para>
</sect2>

<sect2 id="api-func-runqueries"><title>RunQueries</title>
<para><b>Prototype:</b> function RunQueries ()</para>
<para>
Connect to searchd, runs a batch of all queries added using <code>AddQuery()</code>,
obtains and returns the result sets. Returns false and sets <code>GetLastError()</code>
message on general error (such as network I/O failure). Returns a plain array
of result sets on success.
</para>
<para>
Each result set in the returned array is exactly the same as
the result set returned from <link linkend="api-func-query"><code>Query()</code></link>.
</para>
<para>
Note that the batch query request itself almost always succeds -
unless there's a network error, blocking index rotation in progress,
or another general failure which prevents the whole request from being
processed.
</para>
<para>
However individual queries within the batch might very well fail.
In this case their respective result sets will contain non-empty <code>"error"</code> message,
but no matches or query statistics. In the extreme case all queries within the batch
could fail. There still will be no general error reported, because API was able to
succesfully connect to <filename>searchd</filename>, submit the batch, and receive
the results - but every result set will have a specific error message.
</para>
</sect2>

<sect2 id="api-func-resetfilters"><title>ResetFilters</title>
<para><b>Prototype:</b> function ResetFilters ()</para>
<para>
Clears all currently set filters.
</para>
<para>
This call is only normally required when using multi-queries. You might want
to set different filters for different queries in the batch. To do that,
you should call <code>ResetFilters()</code> and add new filters using
the respective calls.
</para>
</sect2>

<sect2 id="api-func-resetgroupby"><title>ResetGroupBy</title>
<para><b>Prototype:</b> function ResetGroupBy ()</para>
<para>
Clears all currently group-by settings, and disables group-by.
</para>
<para>
This call is only normally required when using multi-queries.
You can change individual group-by settings using <code>SetGroupBy()</code>
and <code>SetGroupDistinct()</code> calls, but you can not disable
group-by using those calls. <code>ResetGroupBy()</code>
fully resets previous group-by settings and disables group-by mode
in the current state, so that subsequent <code>AddQuery()</code>
calls can perform non-grouping searches.
</para>
</sect2>


</sect1>
<sect1 id="api-funcgroup-additional-functionality"><title>Additional functionality</title>


<sect2 id="api-func-buildexcerpts"><title>BuildExcerpts</title>
<para><b>Prototype:</b> function BuildExcerpts ( $docs, $index, $words, $opts=array() )</para>
<para>
Excerpts (snippets) builder function. Connects to <filename>searchd</filename>,
asks it to generate excerpts (snippets) from given documents, and returns the results.
</para>
<para>
<code>$docs</code> is a plain array of strings that carry the documents' contents.
<code>$index</code> is an index name string. Different settings (such as charset,
morphology, wordforms) from given index will be used.
<code>$words</code> is a string that contains the keywords to highlight. They will
be processed with respect to index settings. For instance, if English stemming
is enabled in the index, "shoes" will be highlighted even if keyword is "shoe".
Starting with version 0.9.9-rc1, keywords can contain wildcards, that work similarly to
<link linkend="conf-enable-star">star-syntax</link> available in queries.
<code>$opts</code> is a hash which contains additional optional highlighting parameters:
<variablelist>
<varlistentry>
	<term>"before_match":</term>
	<listitem>A string to insert before a keyword match. Starting with version 1.10-beta,
	a %PASSAGE_ID% macro can be used in this string. The macro is replaced with an incrementing
	passage number within a current snippet. Numbering starts at 1 by default but can be
	overridden with "start_passage_id" option. In a multi-document call, %PASSAGE_ID% would
	restart at every given document. Default is "&lt;b&gt;".</listitem>
</varlistentry>
<varlistentry>
	<term>"after_match":</term>
	<listitem>A string to insert after a keyword match. Starting with version 1.10-beta,
	a %PASSAGE_ID% macro can be used in this string. Default is "&lt;b&gt;".</listitem>
</varlistentry>
<varlistentry>
	<term>"chunk_separator":</term>
	<listitem>A string to insert between snippet chunks (passages). Default is "&#160;...&#160;".</listitem>
</varlistentry>
<varlistentry>
	<term>"limit":</term>
	<listitem>Maximum snippet size, in symbols (codepoints). Integer, default is 256.</listitem>
</varlistentry>
<varlistentry>
	<term>"around":</term>
	<listitem>How much words to pick around each matching keywords block. Integer, default is 5.</listitem>
</varlistentry>
<varlistentry>
	<term>"exact_phrase":</term>
	<listitem>Whether to highlight exact query phrase matches only instead of individual keywords. Boolean, default is false.</listitem>
</varlistentry>
<varlistentry>
	<term>"single_passage":</term>
	<listitem>Whether to extract single best passage only. Boolean, default is false.</listitem>
</varlistentry>
<varlistentry>
	<term>"use_boundaries":</term>
	<listitem>Whether to additionaly break passages by phrase 
	boundary characters, as configured in index settings with
	<link linkend="conf-phrase-boundary">phrase_boundary</link>
	directive. Boolean, default is false.
	</listitem>
</varlistentry>
<varlistentry>
	<term>"weight_order":</term>
	<listitem>Whether to sort the extracted passages in order of relevance (decreasing weight),
	or in order of appearance in the document (increasing position). Boolean, default is false.</listitem>
</varlistentry>
<varlistentry>
	<term>"query_mode":</term>
	<listitem>Added in version 1.10-beta. Whether to handle $words as a query in
	<link linkend="extended-syntax">extended syntax</link>, or as a bag of words
	(default behavior). For instance, in query mode ("one two" | "three four") will
	only highlight and include those occurrences "one two" or "three four" when 
	the two words from each pair are adjacent to each other. In default mode,
	any single occurrence of "one", "two", "three", or "four" would be
	highlighted. Boolean, default is false.
	</listitem>
</varlistentry>
<varlistentry>
	<term>"force_all_words":</term>
	<listitem>Added in version 1.10-beta. Ignores the snippet length limit until it
	includes all the keywords. Boolean, default is false.
	</listitem>
</varlistentry>
<varlistentry>
	<term>"limit_passages":</term>
	<listitem>Added in version 1.10-beta. Limits the maximum number of passages
	that can be included into the snippet. Integer, default is 0 (no limit).
	</listitem>
</varlistentry>
<varlistentry>
	<term>"limit_words":</term>
	<listitem>Added in version 1.10-beta. Limits the maximum number of keywords
	that can be included into the snippet. Integer, default is 0 (no limit).
	</listitem>
</varlistentry>
<varlistentry>
	<term>"start_passage_id":</term>
	<listitem>Added in version 1.10-beta. Specifies the starting value of
	%PASSAGE_ID% macro (that gets detected and expanded in <option>before_match</option>,
	<option>after_match</option> strings). Integer, default is 1.
	</listitem>
</varlistentry>
<varlistentry>
	<term>"load_files":</term>
	<listitem>Added in version 1.10-beta. Whether to handle $docs as data
	to extract snippets from (default behavior), or to treat it as file names,
	and load data from specified files on the server side. Starting with
	version 1.11-beta, up to <link linkend="conf-dist-threads">dist_threads</link>
	worker threads per request will be created to parallelize the work
	when this flag is enabled. Boolean, default is false.
	</listitem>
</varlistentry>
<varlistentry>
	<term>"html_strip_mode":</term>
	<listitem>Added in version 1.10-beta. HTML stripping mode setting.
	Defaults to "index", which means that index settings will be used.
	The other values are "none" and "strip", that forcibly skip or apply
	stripping irregardless of index settings; and "retain", that retains
	HTML markup and protects it from highlighting. The "retain" mode can
	only be used when highlighting full documents and thus requires that
	no snippet size limits are set. String, allowed values are "none",
	"strip", "index", and "retain".
	</listitem>
</varlistentry>
<varlistentry>
	<term>"allow_empty":</term>
	<listitem>Added in version 1.10-beta. Allows empty string to be
	returned as highlighting result when a snippet could not be generated
	(no keywords match, or no passages fit the limit). By default,
	the beginning of original text would be returned instead of an empty
	string. Boolean, default is false.
	</listitem>
</varlistentry>
<varlistentry>
	<term>"passage_boundary":</term>
	<listitem>Added in version 1.11-beta. Ensures that passages do not
	cross a sentence, paragraph, or zone boundary (when used with an index
	that has the respective indexing settings enabled). String, allowed
	values are "sentence", "paragraph", and "zone".
	</listitem>
</varlistentry>
<varlistentry>
	<term>"passage_boundary":</term>
	<listitem>Added in version 1.11-beta. Ensures that passages do not
	cross a sentence, paragraph, or zone boundary (when used with an index
	that has the respective indexing settings enabled). String, allowed
	values are "sentence", "paragraph", and "zone".
	</listitem>
</varlistentry>
<varlistentry>
	<term>"emit_zones":</term>
	<listitem>Added in version 1.11-beta. Emits an HTML tag with
	an enclosing zone name before each passage. Boolean, default is false.
	</listitem>
</varlistentry>
</variablelist>
</para>
<para>
Snippets extraction algorithm currently favors better passages
(with closer phrase matches), and then passages with keywords not
yet in snippet. Generally, it will try to highlight the best match
with the query, and it will also to highlight all the query keywords,
as made possible by the limtis. In case the document does not match
the query, beginning of the document trimmed down according to the
limits will be return by default. Starting with 1.10-beta, you can
also return an empty snippet instead case by setting "allow_empty"
option to true.
</para>
<para>
Returns false on failure. Returns a plain array of strings with excerpts (snippets) on success.
</para>
</sect2>


<sect2 id="api-func-updateatttributes"><title>UpdateAttributes</title>
<para><b>Prototype:</b> function UpdateAttributes ( $index, $attrs, $values )</para>
<para>
Instantly updates given attribute values in given documents.
Returns number of actually updated documents (0 or more) on success, or -1 on failure.
</para>
<para>
<code>$index</code> is a name of the index (or indexes) to be updated.
<code>$attrs</code> is a plain array with string attribute names, listing attributes that are updated.
<code>$values</code> is a hash where key is document ID, and value is a plain array of new attribute values.
</para>
<para>
<code>$index</code> can be either a single index name or a list, like in <code>Query()</code>.
Unlike <code>Query()</code>, wildcard is not allowed and all the indexes
to update must be specified explicitly. The list of indexes can include
distributed index names. Updates on distributed indexes will be pushed
to all agents.
</para>
<para>
The updates only work with <code>docinfo=extern</code> storage strategy.
They are very fast because they're working fully in RAM, but they can also
be made persistent: updates are saved on disk on clean <filename>searchd</filename>
shutdown initiated by SIGTERM signal. With additional restrictions, updates
are also possible on MVA attributes; refer to <link linkend="conf-mva-updates-pool">mva_updates_pool</link>
directive for details.
</para>
<para>
Usage example:
<programlisting>
$cl->UpdateAttributes ( "test1", array("group_id"), array(1=>array(456)) );
$cl->UpdateAttributes ( "products", array ( "price", "amount_in_stock" ),
	array ( 1001=>array(123,5), 1002=>array(37,11), 1003=>(25,129) ) );
</programlisting>
The first sample statement will update document 1 in index "test1", setting "group_id" to 456.
The second one will update documents 1001, 1002 and 1003 in index "products". For document 1001,
the new price will be set to 123 and the new amount in stock to 5; for document 1002, the new price
will be 37 and the new amount will be 11; etc.
</para>
</sect2>


<sect2 id="api-func-buildkeywords"><title>BuildKeywords</title>
<para><b>Prototype:</b> function BuildKeywords ( $query, $index, $hits )</para>
<para>
Extracts keywords from query using tokenizer settings for given index, optionally with per-keyword occurrence statistics.
Returns an array of hashes with per-keyword information.
</para>
<para>
<code>$query</code> is a query to extract keywords from.
<code>$index</code> is a name of the index to get tokenizing settings and keyword occurrence statistics from.
<code>$hits</code> is a boolean flag that indicates whether keyword occurrence statistics are required.
</para>
<para>
Usage example:
</para>
<programlisting>
$keywords = $cl->BuildKeywords ( "this.is.my query", "test1", false );
</programlisting>
</sect2>


<sect2 id="api-func-escapestring"><title>EscapeString</title>
<para><b>Prototype:</b> function EscapeString ( $string )</para>
<para>
Escapes characters that are treated as special operators by the query language parser.
Returns an escaped string.
</para>
<para>
<code>$string</code> is a string to escape.
</para>
<para>
This function might seem redundant because it's trivial to implement in any calling
application. However, as the set of special characters might change over time, it makes
sense to have an API call that is guaranteed to escape all such characters at all times.
</para>
<para>
Usage example:
</para>
<programlisting>
$escaped = $cl->EscapeString ( "escaping-sample@query/string" );
</programlisting>
</sect2>


<sect2 id="api-func-status"><title>Status</title>
<para><b>Prototype:</b> function Status ()</para>
<para>
Queries searchd status, and returns an array of status variable name and value pairs.
</para>
<para>
Usage example:
</para>
<programlisting>
$status = $cl->Status ();
foreach ( $status as $row )
	print join ( ": ", $row ) . "\n";
</programlisting>
</sect2>


<sect2 id="api-func-flushattributes"><title>FlushAttributes</title>
<para><b>Prototype:</b> function FlushAttributes ()</para>
<para>
Forces <filename>searchd</filename> to flush pending attribute updates
to disk, and blocks until completion. Returns a non-negative internal
"flush tag" on success. Returns -1 and sets an error message on error.
Introduced in version 1.10-beta. 
</para>
<para>
Attribute values updated using <link linkend="api-func-updateatttributes">UpdateAttributes()</link>
API call are only kept in RAM until a so-called flush (which writes
the current, possibly updated attribute values back to disk). FlushAttributes()
call lets you enforce a flush.  The call will block until <filename>searchd</filename>
finishes writing the data to disk, which might take seconds or even minutes
depending on the total data size (.spa file size). All the currently updated
indexes will be flushed.
</para>
<para>
Flush tag should be treated as an ever growing magic number that does not
mean anything. It's guaranteed to be non-negative. It is guaranteed to grow over
time, though not necessarily in a sequential fashion; for instance, two calls that
return 10 and then 1000 respectively are a valid situation. If two calls to
FlushAttrs() return the same tag, it means that there were no actual attribute
updates in between them, and therefore current flushed state remained the same
(for all indexes).
</para>
<para>
Usage example:
</para>
<programlisting>
$status = $cl->FlushAttributes ();
if ( $status&lt;0 )
	print "ERROR: " . $cl->GetLastError(); 
</programlisting>
</sect2>


</sect1>

<sect1 id="api-funcgroup-pconn"><title>Persistent connections</title>
<para>
Persistent connections allow to use single network connection to run
multiple commands that would otherwise require reconnects.
</para>

<sect2 id="api-func-open"><title>Open</title>
<para><b>Prototype:</b> function Open ()</para>
<para>
Opens persistent connection to the server.
</para>
</sect2>

<sect2 id="api-func-close"><title>Close</title>
<para><b>Prototype:</b> function Close ()</para>
<para>
Closes previously opened persistent connection.
</para>
</sect2>

</sect1>

</chapter>
<chapter id="sphinxse"><title>MySQL storage engine (SphinxSE)</title>


<sect1 id="sphinxse-overview"><title>SphinxSE overview</title>
<para>
SphinxSE is MySQL storage engine which can be compiled
into MySQL server 5.x using its pluggable architecure.
It is not available for MySQL 4.x series. It also requires
MySQL 5.0.22 or higher in 5.0.x series, or MySQL 5.1.12
or higher in 5.1.x series.
</para>
<para>
Despite the name, SphinxSE does <emphasis>not</emphasis>
actually store any data itself. It is actually a built-in client
which allows MySQL server to talk to <filename>searchd</filename>,
run search queries, and obtain search results. All indexing and
searching happen outside MySQL.
</para>
<para>
Obvious SphinxSE applications include:
<itemizedlist>
<listitem>easier porting of MySQL FTS applications to Sphinx;</listitem>
<listitem>allowing Sphinx use with progamming languages for which native APIs are not available yet;</listitem>
<listitem>optimizations when additional Sphinx result set processing on MySQL side is required
	(eg. JOINs with original document tables, additional MySQL-side filtering, etc).</listitem>
</itemizedlist>
</para>
</sect1>


<sect1 id="sphinxse-installing"><title>Installing SphinxSE</title>
<para>
You will need to obtain a copy of MySQL sources, prepare those,
and then recompile MySQL binary.
MySQL sources (mysql-5.x.yy.tar.gz) could be obtained from 
<ulink url="http://dev.mysql.com">dev.mysql.com</ulink> Web site.
</para>
<para>
For some MySQL versions, there are delta tarballs with already
prepared source versions available from Sphinx Web site. After unzipping
those over original sources MySQL would be ready to be configured and
built with Sphinx support.
</para>
<para>
If such tarball is not available, or does not work for you for any
reason, you would have to prepare sources manually. You will need to
GNU Autotools framework (autoconf, automake and libtool) installed
to do that.
</para>


<sect2 id="sphinxse-mysql50"><title>Compiling MySQL 5.0.x with SphinxSE</title>
<para>
Skips steps 1-3 if using already prepared delta tarball.
</para>
<orderedlist>
<listitem><para>copy <filename>sphinx.5.0.yy.diff</filename> patch file
into MySQL sources directory and run
<programlisting>
patch -p1 &lt; sphinx.5.0.yy.diff
</programlisting>
If there's no .diff file exactly for the specific version you need
to build, try applying .diff with closest version numbers. It is important
that the patch should apply with no rejects.
</para></listitem>
<listitem>in MySQL sources directory, run
<programlisting>
sh BUILD/autorun.sh
</programlisting>
</listitem>
<listitem>in MySQL sources directory, create <filename>sql/sphinx</filename>
directory in and copy all files in <filename>mysqlse</filename> directory 
from Sphinx sources there. Example:
<programlisting>
cp -R /root/builds/sphinx-0.9.7/mysqlse /root/builds/mysql-5.0.24/sql/sphinx
</programlisting>
</listitem>
<listitem>
configure MySQL and enable Sphinx engine:
<programlisting>
./configure --with-sphinx-storage-engine
</programlisting>
</listitem>
<listitem>
build and install MySQL:
<programlisting>
make
make install
</programlisting>
</listitem>
</orderedlist>
</sect2>


<sect2 id="sphinxse-mysql51"><title>Compiling MySQL 5.1.x with SphinxSE</title>
<para>
Skip steps 1-2 if using already prepared delta tarball.
</para>
<orderedlist>
<listitem>in MySQL sources directory, create <filename>storage/sphinx</filename>
directory in and copy all files in <filename>mysqlse</filename> directory 
from Sphinx sources there. Example:
<programlisting>
cp -R /root/builds/sphinx-0.9.7/mysqlse /root/builds/mysql-5.1.14/storage/sphinx
</programlisting>
</listitem>
<listitem>in MySQL sources directory, run
<programlisting>
sh BUILD/autorun.sh
</programlisting>
</listitem>
<listitem>
configure MySQL and enable Sphinx engine:
<programlisting>
./configure --with-plugins=sphinx
</programlisting>
</listitem>
<listitem>
build and install MySQL:
<programlisting>
make
make install
</programlisting>
</listitem>
</orderedlist>
</sect2>


<sect2 id="sphinxse-checking"><title>Checking SphinxSE installation</title>
To check whether SphinxSE has been succesfully compiled
into MySQL, launch newly built servers, run mysql client and
issue <code>SHOW ENGINES</code> query. You should see a list
of all available engines. Sphinx should be present and "Support"
column should contain "YES":

<programlisting>
mysql> show engines;
+------------+----------+-------------------------------------------------------------+
| Engine     | Support  | Comment                                                     |
+------------+----------+-------------------------------------------------------------+
| MyISAM     | DEFAULT  | Default engine as of MySQL 3.23 with great performance      |
  ...
| SPHINX     | YES      | Sphinx storage engine                                       |
  ...
+------------+----------+-------------------------------------------------------------+
13 rows in set (0.00 sec)
</programlisting>     
</sect2>
</sect1>


<sect1 id="sphinxse-using"><title>Using SphinxSE</title>
<para>
To search via SphinxSE, you would need to create special ENGINE=SPHINX "search table",
and then SELECT from it with full text query put into WHERE clause for query column.
</para>
<para>
Let's begin with an example create statement and search query:
<programlisting>
CREATE TABLE t1
(
    id          INTEGER UNSIGNED NOT NULL,
    weight      INTEGER NOT NULL,
    query       VARCHAR(3072) NOT NULL,
    group_id    INTEGER,
    INDEX(query)
) ENGINE=SPHINX CONNECTION="sphinx://localhost:9312/test";

SELECT * FROM t1 WHERE query='test it;mode=any';
</programlisting>
</para>
<para>
First 3 columns of search table <emphasis>must</emphasis> have a types of
<code>INTEGER UNSINGED</code> or <code>BIGINT</code> for the 1st column (document id),
<code>INTEGER</code> or <code>BIGINT</code> for the 2nd column (match weight), and
<code>VARCHAR</code> or <code>TEXT</code> for the 3rd column (your query), respectively.
This mapping is fixed; you can not omit any of these three required columns,
or move them around, or change types. Also, query column must be indexed;
all the others must be kept unindexed. Columns' names are ignored so you
can use arbitrary ones.
</para>
<para>
Additional columns must be either <code>INTEGER</code>, <code>TIMESTAMP</code>,
<code>BIGINT</code>, <code>VARCHAR</code>, or <code>FLOAT</code>.
They will be bound to attributes provided in Sphinx result set by name, so their
names must match attribute names specified in <filename>sphinx.conf</filename>.
If there's no such attribute name in Sphinx search results, column will have
<code>NULL</code> values.
</para>
<para>
Special "virtual" attributes names can also be bound to SphinxSE columns.
<code>_sph_</code> needs to be used instead of <code>@</code> for that.
For instance, to obtain the values of <code>@groupby</code>, <code>@count</code>,
or <code>@distinct</code> virtual attributes, use <code>_sph_groupby</code>,
<code>_sph_count</code> or <code>_sph_distinct</code> column names, respectively.
</para>
<para>
<code>CONNECTION</code> string parameter can be used to specify default
searchd host, port and indexes for queries issued using this table.
If no connection string is specified in <code>CREATE TABLE</code>,
index name "*" (ie. search all indexes) and localhost:9312 are assumed.
Connection string syntax is as follows:
<programlisting>
CONNECTION="sphinx://HOST:PORT/INDEXNAME"
</programlisting>
You can change the default connection string later:
<programlisting>
ALTER TABLE t1 CONNECTION="sphinx://NEWHOST:NEWPORT/NEWINDEXNAME";
</programlisting>
You can also override all these parameters per-query.
</para>
<para>
As seen in example, both query text and search options should be put
into WHERE clause on search query column (ie. 3rd column); the options
are separated by semicolons; and their names from values by equality sign.
Any number of options can be specified. Available options are:
<itemizedlist>
<listitem>query - query text;</listitem>
<listitem>mode - matching mode. Must be one of "all", "any", "phrase",
	"boolean", or "extended". Default is "all";</listitem>
<listitem>sort - match sorting mode. Must be one of "relevance", "attr_desc",
"attr_asc", "time_segments", or "extended". In all modes besides "relevance"
attribute name (or sorting clause for "extended") is also required after a colon:
<programlisting>
... WHERE query='test;sort=attr_asc:group_id';
... WHERE query='test;sort=extended:@weight desc, group_id asc';
</programlisting>
</listitem>
<listitem>offset - offset into result set, default is 0;</listitem>
<listitem>limit - amount of matches to retrieve from result set, default is 20;</listitem>
<listitem>index - names of the indexes to search:
<programlisting>
... WHERE query='test;index=test1;';
... WHERE query='test;index=test1,test2,test3;';
</programlisting>
</listitem>
<listitem>minid, maxid - min and max document ID to match;</listitem>
<listitem>weights - comma-separated list of weights to be assigned to Sphinx full-text fields:
<programlisting>
... WHERE query='test;weights=1,2,3;';
</programlisting>
</listitem>
<listitem>filter, !filter - comma-separated attribute name and a set of values to match:
<programlisting>
# only include groups 1, 5 and 19
... WHERE query='test;filter=group_id,1,5,19;';

# exclude groups 3 and 11
... WHERE query='test;!filter=group_id,3,11;';
</programlisting>
</listitem>
<listitem>range, !range - comma-separated attribute name, min and max value to match:
<programlisting>
# include groups from 3 to 7, inclusive
... WHERE query='test;range=group_id,3,7;';

# exclude groups from 5 to 25
... WHERE query='test;!range=group_id,5,25;';
</programlisting>
</listitem>
<listitem>maxmatches - per-query max matches value, as in max_matches parameter to
<link linkend="api-func-setlimits">SetLimits()</link> API call:
<programlisting>
... WHERE query='test;maxmatches=2000;';
</programlisting>
</listitem>
<listitem>cutoff - maximum allowed matches, as in cutoff parameter to
<link linkend="api-func-setlimits">SetLimits()</link> API call:
<programlisting>
... WHERE query='test;cutoff=10000;';
</programlisting>
</listitem>
<listitem>maxquerytme - maximum allowed query time (in milliseconds), as in
<link linkend="api-func-setmaxquerytime">SetMaxQueryTime()</link> API call:
<programlisting>
... WHERE query='test;maxquerytime=1000;';
</programlisting>
</listitem>
<listitem>groupby - group-by function and attribute, corresponding to
<link linkend="api-func-setgroupby">SetGroupBy()</link> API call:
<programlisting>
... WHERE query='test;groupby=day:published_ts;';
... WHERE query='test;groupby=attr:group_id;';
</programlisting>
</listitem>
<listitem>groupsort - group-by sorting clause:
<programlisting>
... WHERE query='test;groupsort=@count desc;';
</programlisting>
</listitem>
<listitem>distinct - an attribute to compute COUNT(DISTINCT) for when doing group-by, as in
<link linkend="api-func-setgroupdistinct">SetGroupDistinct()</link> API call:
<programlisting>
... WHERE query='test;groupby=attr:country_id;distinct=site_id';
</programlisting>
</listitem>
<listitem>indexweights - comma-separated list of index names and weights
to use when searching through several indexes:
<programlisting>
... WHERE query='test;indexweights=idx_exact,2,idx_stemmed,1;';
</programlisting>
</listitem>
<listitem>comment - a string to mark this query in query log
(mapping to $comment parameter in <link linkend="api-func-query">Query()</link> API call):
<programlisting>
... WHERE query='test;comment=marker001;';
</programlisting>
</listitem>
<listitem>select - a string with expressions to compute
(mapping to <link linkend="api-func-setselect">SetSelect()</link> API call):
<programlisting>
... WHERE query='test;select=2*a+3*b as myexpr;';
</programlisting>
</listitem>
<listitem>host, port - remote <filename>searchd</filename> host name
and TCP port, respectively:
<programlisting>
... WHERE query='test;host=sphinx-test.loc;port=7312;';
</programlisting>
</listitem>
<listitem>ranker - a ranking function to use when matching mode is extended
(i.e. with query syntax), as in <link linkend="api-func-setrankingmode">SetRankingMode()</link> API call.
Known values are "proximity_bm25", "bm25", "none", "wordcount", "proximity",
"matchany", and "fieldmask".
<programlisting>
... WHERE query='test;ranker=bm25;';
</programlisting>
</listitem>
<listitem>geoanchor - geodistance anchor, as in
<link linkend="api-func-setgeoanchor">SetGeoAnchor()</link> API call.
Takes 4 parameters which are latitude and longiture attribute names,
and anchor point coordinates respectively:
<programlisting>
... WHERE query='test;geoanchor=latattr,lonattr,0.123,0.456';
</programlisting>
</listitem>
</itemizedlist>
</para>
<para>
One <emphasis role="bold">very important</emphasis> note that it is
<emphasis role="bold">much</emphasis> more efficient to allow Sphinx
to perform sorting, filtering and slicing the result set than to raise
max matches count and use WHERE, ORDER BY and LIMIT clauses on MySQL
side. This is for two reasons. First, Sphinx does a number of
optimizations and performs better than MySQL on these tasks.
Second, less data would need to be packed by searchd, transferred
and unpacked by SphinxSE.
</para>
<para>
Starting with version 0.9.9-rc1, additional query info besides result set could be
retrieved with <code>SHOW ENGINE SPHINX STATUS</code> statement:
<programlisting>
mysql> SHOW ENGINE SPHINX STATUS;
+--------+-------+-------------------------------------------------+
| Type   | Name  | Status                                          |
+--------+-------+-------------------------------------------------+
| SPHINX | stats | total: 25, total found: 25, time: 126, words: 2 | 
| SPHINX | words | sphinx:591:1256 soft:11076:15945                | 
+--------+-------+-------------------------------------------------+
2 rows in set (0.00 sec)
</programlisting>
This information can also be accessed through status variables. Note
that this method does not require super-user privileges.
<programlisting>
mysql> SHOW STATUS LIKE 'sphinx_%';
+--------------------+----------------------------------+
| Variable_name      | Value                            |
+--------------------+----------------------------------+
| sphinx_total       | 25                               | 
| sphinx_total_found | 25                               | 
| sphinx_time        | 126                              | 
| sphinx_word_count  | 2                                | 
| sphinx_words       | sphinx:591:1256 soft:11076:15945 | 
+--------------------+----------------------------------+
5 rows in set (0.00 sec)
</programlisting>
</para>
<para>
You could perform JOINs on SphinxSE search table and tables using
other engines. Here's an example with "documents" from example.sql:
<programlisting>
mysql> SELECT content, date_added FROM test.documents docs
-> JOIN t1 ON (docs.id=t1.id) 
-> WHERE query="one document;mode=any";
+-------------------------------------+---------------------+
| content                             | docdate             |
+-------------------------------------+---------------------+
| this is my test document number two | 2006-06-17 14:04:28 | 
| this is my test document number one | 2006-06-17 14:04:28 | 
+-------------------------------------+---------------------+
2 rows in set (0.00 sec)

mysql> SHOW ENGINE SPHINX STATUS;
+--------+-------+---------------------------------------------+
| Type   | Name  | Status                                      |
+--------+-------+---------------------------------------------+
| SPHINX | stats | total: 2, total found: 2, time: 0, words: 2 | 
| SPHINX | words | one:1:2 document:2:2                        | 
+--------+-------+---------------------------------------------+
2 rows in set (0.00 sec)
</programlisting>
</para>
</sect1>


<sect1 id="sphinxse-snippets"><title>Building snippets (excerpts) via MySQL</title>
<para>
Starting with version 0.9.9-rc2, SphinxSE also includes a UDF function
that lets you create snippets through MySQL. The functionality is fully
similar to <link linkend="api-func-buildexcerpts">BuildExcerprts</link>
API call but accesible through MySQL+SphinxSE.
</para>
<para>
The binary that provides the UDF is named <filename>sphinx.so</filename>
and should be automatically built and installed to proper location
along with SphinxSE itself. If it does not get installed automatically
for some reason, look for <filename>sphinx.so</filename> in the build
directory and copy it to the plugins directory of your MySQL instance.
After that, register the UDF using the following statement:
<programlisting>
CREATE FUNCTION sphinx_snippets RETURNS STRING SONAME 'sphinx.so';
</programlisting>
</para>
<para>
Function name <emphasis>must</emphasis> be sphinx_snippets,
you can not use an arbitrary name. Function arguments are as follows:
</para>
<para>
<b>Prototype:</b> function sphinx_snippets ( document, index, words, [options] );
</para>
<para>
Document and words arguments can be either strings or table columns.
Options must be specified like this: <code>'value' AS option_name</code>.
For a list of supported options, refer to
<link linkend="api-func-buildexcerpts">BuildExcerprts()</link> API call.
The only UDF-specific additional option is named <code>'sphinx'</code>
and lets you specify searchd location (host and port).
</para>
<para>
Usage examples:
<programlisting>
SELECT sphinx_snippets('hello world doc', 'main', 'world',
    'sphinx://192.168.1.1/' AS sphinx, true AS exact_phrase,
    '[b]' AS before_match, '[/b]' AS after_match)
FROM documents;

SELECT title, sphinx_snippets(text, 'index', 'mysql php') AS text
    FROM sphinx, documents
    WHERE query='mysql php' AND sphinx.id=documents.id;
</programlisting>
</para>
</sect1>


</chapter>
<chapter id="reporting-bugs"><title>Reporting bugs</title>


<para>
Unfortunately, Sphinx is not yet 100% bug free (even though I'm working hard
towards that), so you might occasionally run into some issues.
</para>
<para>
Reporting as much as possible about each bug is very important -
because to fix it, I need to be able either to reproduce and debug the bug,
or to deduce what's causing it from the information that you provide.
So here are some instructions on how to do that.
</para>


<bridgehead>Build-time issues</bridgehead>
<para>If Sphinx fails to build for some reason, please do the following:</para>
<orderedlist>
<listitem>check that headers and libraries for your DBMS are properly installed
(for instance, check that <filename>mysql-devel</filename> package is present);
</listitem>
<listitem>report Sphinx version and config file (be sure to remove the passwords!),
MySQL (or PostgreSQL) configuration info, gcc version, OS version and CPU type
(ie. x86, x86-64, PowerPC, etc):
<programlisting>
mysql_config
gcc --version
uname -a
</programlisting>
</listitem>
<listitem>
report the error message which is produced by <filename>configure</filename>
or <filename>gcc</filename> (it should be to include error message itself only,
not the whole build log).
</listitem>
</orderedlist>


<bridgehead>Run-time issues</bridgehead>
<para>
If Sphinx builds and runs, but there are any problems running it,
please do the following:
</para>
<orderedlist>
<listitem>describe the bug (ie. both the expected behavior and actual behavior)
and all the steps necessary to reproduce it;</listitem>
<listitem>include Sphinx version and config file (be sure to remove the passwords!),
MySQL (or PostgreSQL) version, gcc version, OS version and CPU type (ie. x86, x86-64,
PowerPC, etc):
<programlisting>
mysql --version
gcc --version
uname -a
</programlisting>
</listitem>
<listitem>build, install and run debug versions of all Sphinx programs (this is
to enable a lot of additional internal checks, so-called assertions):
<programlisting>
make distclean
./configure --with-debug
make install
killall -TERM searchd
</programlisting>
</listitem>
<listitem>reindex to check if any assertions are triggered (in this case,
it's likely that the index is corrupted and causing problems);
</listitem>
<listitem>if the bug does not reproduce with debug versions,
revert to non-debug and mention it in your report;
</listitem>
<listitem>if the bug could be easily reproduced with a small (1-100 record)
part of your database, please provide a gzipped dump of that part;
</listitem>
<listitem>if the problem is related to <filename>searchd</filename>, include
relevant entries from <filename>searchd.log</filename> and
<filename>query.log</filename> in your bug report;
</listitem>
<listitem>if the problem is related to <filename>searchd</filename>, try
running it in console mode and check if it dies with an assertion:
<programlisting>
./searchd --console
</programlisting>
</listitem>
<listitem>if any program dies with an assertion, provide the assertion message.</listitem>
</orderedlist>


<bridgehead>Debugging assertions, crashes and hangups</bridgehead>
<para>
If any program dies with an assertion, crashes without an assertion or hangs up,
you would additionally need to generate a core dump and examine it.
</para>
<orderedlist>
<listitem>
enable core dumps. On most Linux systems, this is done
using <filename>ulimit</filename>:
<programlisting>
ulimit -c 32768
</programlisting>
</listitem>
<listitem>
run the program and try to reproduce the bug;
</listitem>
<listitem>
if the program crashes (either with or without an assertion),
find the core file in current directory (it should typically print
out "Segmentation fault (core dumped)" message);
</listitem>
<listitem>
if the program hangs, use <filename>kill -SEGV</filename>
from another console to force it to exit and dump core:
<programlisting>
kill -SEGV HANGED-PROCESS-ID
</programlisting>
</listitem>
<listitem>
use <filename>gdb</filename> to examine the core file
and obtain a backtrace:
<programlisting>
gdb ./CRASHED-PROGRAM-FILE-NAME CORE-DUMP-FILE-NAME
(gdb) bt
(gdb) quit
</programlisting>
</listitem>
</orderedlist>
<para>
Note that HANGED-PROCESS-ID, CRASHED-PROGRAM-FILE-NAME and
CORE-DUMP-FILE-NAME must all be replaced with specific numbers
and file names. For example, hanged searchd debugging session
would look like:
<programlisting>
# kill -SEGV 12345
# ls *core*
core.12345
# gdb ./searchd core.12345
(gdb) bt
...
(gdb) quit
</programlisting>
</para>
<para>
Note that <filename>ulimit</filename> is not server-wide
and only affects current shell session. This means that you will not
have to restore any server-wide limits - but if you relogin,
you will have to set <filename>ulimit</filename> again.
</para>
<para>
Core dumps should be placed in current working directory
(and Sphinx programs do not change it), so this is where you
would look for them.
</para>
<para>
Please do not immediately remove the core file because there could
be additional helpful information which could be retrieved from it.
You do not need to send me this file (as the debug info there is
closely tied to your system) but I might need to ask
you a few additional questions about it.
</para>


</chapter>
<chapter id="conf-reference"><title><filename>sphinx.conf</filename> options reference</title>


<sect1 id="confgroup-source"><title>Data source configuration options</title>


<sect2 id="conf-source-type"><title>type</title>
<para>
Data source type.
Mandatory, no default value.
Known types are <option>mysql</option>, <option>pgsql</option>, <option>mssql</option>,
<option>xmlpipe</option> and <option>xmlpipe2</option>, and <option>odbc</option>.
</para>
<para>
All other per-source options depend on source type selected by this option.
Names of the options used for SQL sources (ie. MySQL, PostgreSQL, MS SQL) start with "sql_";
names of the ones used for xmlpipe and xmlpipe2 start with "xmlpipe_".
All source types except <option>xmlpipe</option> are conditional; they might or might
not be supported depending on your build settings, installed client libraries, etc.
<option>mssql</option> type is currently only available on Windows.
<option>odbc</option> type is available both on Windows natively and on
Linux through <ulink url="http://www.unixodbc.org/">UnixODBC library</ulink>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
type = mysql
</programlisting>
</sect2>


<sect2 id="conf-sql-host"><title>sql_host</title>
<para>
SQL server host to connect to.
Mandatory, no default value.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
In the simplest case when Sphinx resides on the same host with your MySQL
or PostgreSQL installation, you would simply specify "localhost". Note that
MySQL client library chooses whether to connect over TCP/IP or over UNIX
socket based on the host name. Specifically "localhost" will force it
to use UNIX socket (this is the default and generally recommended mode)
and "127.0.0.1" will force TCP/IP usage. Refer to
<ulink url="http://dev.mysql.com/doc/refman/5.0/en/mysql-real-connect.html">MySQL manual</ulink>
for more details.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_host = localhost
</programlisting>
</sect2>


<sect2 id="conf-sql-port"><title>sql_port</title>
<para>
SQL server IP port to connect to.
Optional, default is 3306 for <option>mysql</option> source type and 5432 for <option>pgsql</option> type.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Note that it depends on <link linkend="conf-sql-host">sql_host</link> setting whether this value will actually be used.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_port = 3306
</programlisting>
</sect2>


<sect2 id="conf-sql-user"><title>sql_user</title>
<para>
SQL user to use when connecting to <link linkend="conf-sql-host">sql_host</link>.
Mandatory, no default value.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_user = test
</programlisting>
</sect2>


<sect2 id="conf-sql-pass"><title>sql_pass</title>
<para>
SQL user password to use when connecting to <link linkend="conf-sql-host">sql_host</link>.
Mandatory, no default value.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_pass = mysecretpassword
</programlisting>
</sect2>


<sect2 id="conf-sql-db"><title>sql_db</title>
<para>
SQL database (in MySQL terms) to use after the connection and perform further queries within.
Mandatory, no default value.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_db = test
</programlisting>
</sect2>


<sect2 id="conf-sql-sock"><title>sql_sock</title>
<para>
UNIX socket name to connect to for local SQL servers.
Optional, default value is empty (use client library default settings).
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
On Linux, it would typically be <filename>/var/lib/mysql/mysql.sock</filename>.
On FreeBSD, it would typically be <filename>/tmp/mysql.sock</filename>.
Note that it depends on <link linkend="conf-sql-host">sql_host</link> setting whether this value will actually be used.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_sock = /tmp/mysql.sock
</programlisting>
</sect2>


<sect2 id="conf-mysql-connect-flags"><title>mysql_connect_flags</title>
<para>
MySQL client connection flags.
Optional, default value is 0 (do not set any flags).
Applies to <option>mysql</option> source type only.
</para>
<para>
This option must contain an integer value with the sum of the flags.
The value will be passed to <ulink url="http://dev.mysql.com/doc/refman/5.0/en/mysql-real-connect.html">mysql_real_connect()</ulink> verbatim.
The flags are enumerated in mysql_com.h include file.
Flags that are especially interesting in regard to indexing, with their respective values, are as follows:
<itemizedlist>
<listitem>CLIENT_COMPRESS = 32; can use compression protocol</listitem>
<listitem>CLIENT_SSL = 2048; switch to SSL after handshake</listitem>
<listitem>CLIENT_SECURE_CONNECTION = 32768; new 4.1 authentication</listitem>
</itemizedlist>
For instance, you can specify 2080 (2048+32) to use both compression and SSL,
or 32768 to use new authentication only. Initially, this option was introduced
to be able to use compression when the <filename>indexer</filename>
and <filename>mysqld</filename> are on different hosts. Compression on 1 Gbps
links is most likely to hurt indexing time though it reduces network traffic,
both in theory and in practice. However, enabling compression on 100 Mbps links
may improve indexing time significantly (upto 20-30% of the total indexing time
improvement was reported). Your mileage may vary.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
mysql_connect_flags = 32 # enable compression
</programlisting>
</sect2>


<sect2 id="conf-mysql-ssl"><title>mysql_ssl_cert, mysql_ssl_key, mysql_ssl_ca</title>
<para>
SSL certificate settings to use for connecting to MySQL server.
Optional, default values are empty strings (do not use SSL).
Applies to <option>mysql</option> source type only.
</para>
<para>
These directives let you set up secure SSL connection between
<filename>indexer</filename> and MySQL. The details on creating
the certificates and setting up MySQL server can be found in
MySQL documentation.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
mysql_ssl_cert = /etc/ssl/client-cert.pem
mysql_ssl_key = /etc/ssl/client-key.pem
mysql_ssl_ca = /etc/ssl/cacert.pem
</programlisting>
</sect2>


<sect2 id="conf-odbc-dsn"><title>odbc_dsn</title>
<para>
ODBC DSN to connect to.
Mandatory, no default value.
Applies to <option>odbc</option> source type only.
</para>
<para>
ODBC DSN (Data Source Name) specifies the credentials (host, user, password, etc)
to use when connecting to ODBC data source. The format depends on specific ODBC
driver used.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
odbc_dsn = Driver={Oracle ODBC Driver};Dbq=myDBName;Uid=myUsername;Pwd=myPassword
</programlisting>
</sect2>


<sect2 id="conf-sql-query-pre"><title>sql_query_pre</title>
<para>
Pre-fetch query, or pre-query.
Multi-value, optional, default is empty list of queries.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
Multi-value means that you can specify several pre-queries.
They are executed before <link linkend="conf-sql-query">the main fetch query</link>,
and they will be exectued exactly in order of appeareance in the configuration file.
Pre-query results are ignored.
</para>
<para>
Pre-queries are useful in a lot of ways. They are used to setup encoding,
mark records that are going to be indexed, update internal counters,
set various per-connection SQL server options and variables, and so on.
</para>
<para>
Perhaps the most frequent pre-query usage is to specify the encoding
that the server will use for the rows it returnes. It <b>must</b> match
the encoding that Sphinx expects (as specified by <link linkend="conf-charset-type">charset_type</link>
and <link linkend="conf-charset-table">charset_table</link> options).
Two MySQL specific examples of setting the encoding are:
<programlisting>
sql_query_pre = SET CHARACTER_SET_RESULTS=cp1251
sql_query_pre = SET NAMES utf8
</programlisting>
Also specific to MySQL sources, it is useful to disable query cache
(for indexer connection only) in pre-query, because indexing queries
are not going to be re-run frequently anyway, and there's no sense
in caching their results. That could be achieved with:
<programlisting>
sql_query_pre = SET SESSION query_cache_type=OFF
</programlisting>
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_query_pre = SET NAMES utf8
sql_query_pre = SET SESSION query_cache_type=OFF
</programlisting>
</sect2>


<sect2 id="conf-sql-query"><title>sql_query</title>
<para>
Main document fetch query.
Mandatory, no default value.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
There can be only one main query.
This is the query which is used to retrieve documents from SQL server.
You can specify up to 32 full-text fields (formally, upto SPH_MAX_FIELDS from sphinx.h), and an arbitrary amount of attributes.
All of the columns that are neither document ID (the first one) nor attributes will be full-text indexed.
</para>
<para>
Document ID <emphasis role="bold">MUST</emphasis> be the very first field,
and it <emphasis role="bold">MUST BE UNIQUE UNSIGNED POSITIVE (NON-ZERO, NON-NEGATIVE) INTEGER NUMBER</emphasis>.
It can be either 32-bit or 64-bit, depending on how you built Sphinx;
by default it builds with 32-bit IDs support but <option>--enable-id64</option> option
to <filename>configure</filename> allows to build with 64-bit document and word IDs support.
<!-- TODO: add more on zero, negative, duplicate ID handling -->
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_query = \
	SELECT id, group_id, UNIX_TIMESTAMP(date_added) AS date_added, \
		title, content \
	FROM documents
</programlisting>
</sect2>


<sect2 id="conf-sql-joined-field"><title>sql_joined_field</title>
<para>
Joined/payload field fetch query.
Multi-value, optional, default is empty list of queries.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
<option>sql_joined_field</option> lets you use two different features:
joined fields, and payloads (payload fields). It's syntax is as follows:
<programlisting>
sql_joined_field = FIELD-NAME 'from'  ( 'query' | 'payload-query' ); \
    QUERY [ ; RANGE-QUERY ]
</programlisting>
where
<itemizedlist>
<listitem>FIELD-NAME is a joined/payload field name;</listitem>
<listitem>QUERY is an SQL query that must fetch values to index.</listitem>
<listitem>RANGE-QUERY is an optional SQL query that fetches a range
of values to index. (Added in version 1.11-beta.)</listitem>
</itemizedlist>
</para>
<para>
<b>Joined fields</b> let you avoid JOIN and/or GROUP_CONCAT statements in the main
document fetch query (sql_query). This can be useful when SQL-side JOIN is slow,
or needs to be offloaded on Sphinx side, or simply to emulate MySQL-specific
GROUP_CONCAT funcionality in case your database server does not support it.
</para>
<para>
The query must return exactly 2 columns: document ID, and text to append
to a joined field. Document IDs can be duplicate, but they <b>must</b> be
in ascending order. All the text rows fetched for a given ID will be
concatented together, and the concatenation result will be indexed
as the entire contents of a joined field. Rows will be concatenated
in the order returned from the query, and separating whitespace
will be inserted between them. For instance, if joined field query
returns the following rows:
<programlisting>
( 1, 'red' )
( 1, 'right' )
( 1, 'hand' )
( 2, 'mysql' )
( 2, 'sphinx' )
</programlisting>
then the indexing results would be equivalent to that of adding
a new text field with a value of 'red right hand' to document 1 and
'mysql sphinx' to document 2.
</para>
<para>
Joined fields are only indexed differently. There are no other differences
between joined fields and regular text fields.
</para>
<para>
Starting with 1.11-beta, <b>ranged queries</b> can be used when
a single query is not efficient enough or does not work because of
the database driver limitations. It works similar to the ranged
queries in the main indexing loop, see <xref linkend="ranged-queries"/>.
The range will be queried for and fetched upfront once,
then multiple queries with different <code>$start</code>
and <code>$end</code> substitutions will be run to fetch
the actual data.
</para>
<para>
<b>Payloads</b> let you create a special field in which, instead of
keyword positions, so-called user payloads are stored. Payloads are
custom integer values attached to every keyword. They can then be used
in search time to affect the ranking.
</para>
<para>
The payload query must return exactly 3 columns: document ID; keyword;
and integer payload value. Document IDs can be duplicate, but they <b>must</b> be
in ascending order. Payloads must be unsigned integers within 24-bit range,
ie. from 0 to 16777215. For reference, payloads are currently internally
stored as in-field keyword positions, but that is not guaranteed
and might change in the future.
</para>
<para>
Currently, the only method to account for payloads is to use
SPH_RANK_PROXIMITY_BM25 ranker. On indexes with payload fields,
it will automatically switch to a variant that matches keywords
in those fields, computes a sum of matched payloads multiplied
by field wieghts, and adds that sum to the final rank.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_joined_field = \
	tagstext from query; \
	SELECT docid, CONCAT('tag',tagid) FROM tags ORDER BY docid ASC
</programlisting>
</sect2>


<sect2 id="conf-sql-query-range"><title>sql_query_range</title>
<para>
Range query setup.
Optional, default is empty.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
Setting this option enables ranged document fetch queries (see <xref linkend="ranged-queries"/>).
Ranged queries are useful to avoid notorious MyISAM table locks when indexing
lots of data. (They also help with other less notorious issues, such as reduced
performance caused by big result sets, or additional resources consumed by InnoDB
to serialize big read transactions.)
</para>
<para>
The query specified in this option must fetch min and max document IDs that will be
used as range boundaries. It must return exactly two integer fields, min ID first
and max ID second; the field names are ignored.
</para>
<para>
When ranged queries are enabled, <link linkend="conf-sql-query">sql_query</link>
will be required to contain <option>$start</option> and <option>$end</option> macros
(because it obviously would be a mistake to index the whole table many times over).
Note that the intervals specified by <option>$start</option>..<option>$end</option>
will not overlap, so you should <b>not</b> remove document IDs that are
exactly equal to <option>$start</option> or <option>$end</option> from your query.
The example in <xref linkend="ranged-queries"/>) illustrates that; note how it
uses greater-or-equal and less-or-equal comparisons.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_query_range = SELECT MIN(id),MAX(id) FROM documents
</programlisting>
</sect2>


<sect2 id="conf-sql-range-step"><title>sql_range_step</title>
<para>
Range query step.
Optional, default is 1024.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
Only used when <link linkend="ranged-queries">ranged queries</link> are enabled.
The full document IDs interval fetched by <link linkend="conf-sql-query-range">sql_query_range</link>
will be walked in this big steps. For example, if min and max IDs fetched
are 12 and 3456 respectively, and the step is 1000, indexer will call
<link linkend="conf-sql-query">sql_query</link> several times with the
following substitutions:
<itemizedlist>
<listitem>$start=12, $end=1011</listitem>
<listitem>$start=1012, $end=2011</listitem>
<listitem>$start=2012, $end=3011</listitem>
<listitem>$start=3012, $end=3456</listitem>
</itemizedlist>
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_range_step = 1000
</programlisting>
</sect2>


<sect2 id="conf-sql-query-killlist"><title>sql_query_killlist</title>
<para>
Kill-list query.
Optional, default is empty (no query).
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Introduced in version 0.9.9-rc1.
</para>
<para>
This query is expected to return a number of 1-column rows, each containing
just the document ID. The returned document IDs are stored within an index.
Kill-list for a given index suppresses results from <emphasis>other</emphasis>
indexes, depending on index order in the query. The intended use is to help
implement deletions and updates on existing indexes without rebuilding
(actually even touching them), and especially to fight phantom results
problem.
</para>
<para>
Let us dissect an example. Assume we have two indexes, 'main' and 'delta'.
Assume that documents 2, 3, and 5 were deleted since last reindex of 'main',
and documents 7 and 11 were updated (ie. their text contents were changed).
Assume that a keyword 'test' occurred in all these mentioned documents
when we were indexing 'main'; still occurs in document 7 as we index 'delta';
but does not occur in document 11 any more. We now reindex delta and then
search through both these indexes in proper (least to most recent) order:
<programlisting>
$res = $cl->Query ( "test", "main delta" );
</programlisting>
</para>
<para>
First, we need to properly handle deletions. The result set should not
contain documents 2, 3, or 5. Second, we also need to avoid phantom results.
Unless we do something about it, document 11 <emphasis>will</emphasis>
appear in search results! It will be found in 'main' (but not 'delta').
And it will make it to the final result set unless something stops it.
</para>
<para>
Kill-list, or K-list for short, is that something. Kill-list attached
to 'delta' will suppress the specified rows from <b>all</b> the preceding
indexes, in this case just 'main'. So to get the expected results,
we should put all the updated <emphasis>and</emphasis> deleted
document IDs into it.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_query_killlist = \
	SELECT id FROM documents WHERE updated_ts>=@last_reindex UNION \
	SELECT id FROM documents_deleted WHERE deleted_ts>=@last_reindex
</programlisting>
</sect2>


<sect2 id="conf-sql-attr-uint"><title>sql_attr_uint</title>
<para>
Unsigned integer <link linkend="attributes">attribute</link> declaration.
Multi-value (there might be multiple attributes declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
The column value should fit into 32-bit unsigned integer range.
Values outside this range will be accepted but wrapped around.
For instance, -1 will be wrapped around to 2^32-1 or 4,294,967,295.
</para>
<para>
You can specify bit count for integer attributes by appending
':BITCOUNT' to attribute name (see example below).  Attributes with
less than default 32-bit size, or bitfields, perform slower.
But they require less RAM when using <link linkend="conf-docinfo">extern storage</link>:
such bitfields are packed together in 32-bit chunks in <filename>.spa</filename>
attribute data file. Bit size settings are ignored if using
<link linkend="conf-docinfo">inline storage</link>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_attr_uint = group_id
sql_attr_uint = forum_id:9 # 9 bits for forum_id
</programlisting>
</sect2>


<sect2 id="conf-sql-attr-bool"><title>sql_attr_bool</title>
<para>
Boolean <link linkend="attributes">attribute</link> declaration.
Multi-value (there might be multiple attributes declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Equivalent to <link linkend="conf-sql-attr-uint">sql_attr_uint</link> declaration with a bit count of 1.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_attr_bool = is_deleted # will be packed to 1 bit
</programlisting>
</sect2>


<sect2 id="conf-sql-attr-bigint"><title>sql_attr_bigint</title>
<para>
64-bit signed integer <link linkend="attributes">attribute</link> declaration.
Multi-value (there might be multiple attributes declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Note that unlike <link linkend="conf-sql-attr-uint">sql_attr_uint</link>,
these values are <b>signed</b>.
Introduced in version 0.9.9-rc1.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_attr_bigint = my_bigint_id
</programlisting>
</sect2>


<sect2 id="conf-sql-attr-timestamp"><title>sql_attr_timestamp</title>
<para>
UNIX timestamp <link linkend="attributes">attribute</link> declaration.
Multi-value (there might be multiple attributes declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
Timestamps can store date and time in the range of Jan 01, 1970
to Jan 19, 2038 with a precision of one second.
The expected column value should be a timestamp in UNIX format, ie. 32-bit unsigned
integer number of seconds elapsed since midnight, January 01, 1970, GMT.
Timestamps are internally stored and handled as integers everywhere.
But in addition to working with timestamps as integers, it's also legal
to use them along with different date-based functions, such as time segments
sorting mode, or day/week/month/year extraction for GROUP BY.
</para>
<para>
Note that DATE or DATETIME column types in MySQL can <b>not</b> be directly
used as timestamp attributes in Sphinx; you need to explicitly convert such
columns using UNIX_TIMESTAMP function (if data is in range).
</para>
<para>
Note timestamps can not represent dates before January 01, 1970,
and UNIX_TIMESTAMP() in MySQL will not return anything expected.
If you only needs to work with dates, not times, consider TO_DAYS()
function in MySQL instead.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_attr_timestamp = UNIX_TIMESTAMP(added_datetime) AS added_ts
</programlisting>
</sect2>


<sect2 id="conf-sql-attr-str2ordinal"><title>sql_attr_str2ordinal</title>
<para>
Ordinal string number <link linkend="attributes">attribute</link> declaration.
Multi-value (there might be multiple attributes declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
This attribute type (so-called ordinal, for brevity) is intended
to allow sorting by string values, but without storing the strings
themselves. When indexing ordinals, string values are fetched from
database, temporarily stored, sorted, and then replaced by their
respective ordinal numbers in the array of sorted strings.
So, the ordinal number is an integer such that sorting by it
produces the same result as if lexicographically sorting by original strings.
by string values lexicographically.
</para>
<para>
Earlier versions could consume a lot of RAM for indexing ordinals.
Starting with revision r1112, ordinals accumulation and sorting
also runs in fixed memory (at the cost of using additional temporary
disk space), and honors
<link linkend="conf-mem-limit">mem_limit</link> settings.
</para>
<para>
Ideally the strings should be sorted differently, depending
on the encoding and locale. For instance, if the strings are known
to be Russian text in KOI8R encoding, sorting the bytes 0xE0, 0xE1,
and 0xE2 should produce 0xE1, 0xE2 and 0xE0, because in KOI8R
value 0xE0 encodes a character that is (noticeably) after
characters encoded by 0xE1 and 0xE2. Unfortunately, Sphinx
does not support that at the moment and will simply sort
the strings bytewise.
</para>
<para>
Note that the ordinals are by construction local to each index,
and it's therefore impossible to merge ordinals while retaining
the proper order. The processed strings are replaced by their
sequential number in the index they occurred in, but different
indexes have different sets of strings. For instance, if 'main' index
contains strings "aaa", "bbb", "ccc", and so on up to "zzz",
they'll be assigned numbers 1, 2, 3, and so on up to 26,
respectively. But then if 'delta' only contains "zzz" the assigned
number will be 1. And after the merge, the order will be broken.
Unfortunately, this is impossible to workaround without storing
the original strings (and once Sphinx supports storing the
original strings, ordinals will not be necessary any more). 
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_attr_str2ordinal = author_name
</programlisting>
</sect2>


<sect2 id="conf-sql-attr-float"><title>sql_attr_float</title>
<para>
Floating point <link linkend="attributes">attribute</link> declaration.
Multi-value (there might be multiple attributes declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
The values will be stored in single precision, 32-bit IEEE 754 format.
Represented range is approximately from 1e-38 to 1e+38. The amount
of decimal digits that can be stored precisely is approximately 7.
One important usage of the float attributes is storing latitude
and longitude values (in radians), for further usage in query-time
geosphere distance calculations.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_attr_float = lat_radians
sql_attr_float = long_radians
</programlisting>
</sect2>


<sect2 id="conf-sql-attr-multi"><title>sql_attr_multi</title>
<para>
<link linkend="mva">Multi-valued attribute</link> (MVA) declaration.
Multi-value (ie. there may be more than one such attribute declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
Plain attributes only allow to attach 1 value per each document.
However, there are cases (such as tags or categories) when it is
desired to attach multiple values of the same attribute and be able
to apply filtering or grouping to value lists.
</para>
<para>
The declaration format is as follows (backslashes are for clarity only;
everything can be declared in a single line as well):
<programlisting>
sql_attr_multi = ATTR-TYPE ATTR-NAME 'from' SOURCE-TYPE \
	[;QUERY] \
	[;RANGE-QUERY]
</programlisting>
where
<itemizedlist>
<listitem>ATTR-TYPE is 'uint' or 'timestamp'</listitem>
<listitem>SOURCE-TYPE is 'field', 'query', or 'ranged-query'</listitem>
<listitem>QUERY is SQL query used to fetch all ( docid, attrvalue ) pairs</listitem>
<listitem>RANGE-QUERY is SQL query used to fetch min and max ID values, similar to 'sql_query_range'</listitem>
</itemizedlist>
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_attr_multi = uint tag from query; SELECT id, tag FROM tags
sql_attr_multi = uint tag from ranged-query; \
	SELECT id, tag FROM tags WHERE id&gt;=$start AND id&lt;=$end; \
	SELECT MIN(id), MAX(id) FROM tags
</programlisting>
</sect2>


<sect2 id="conf-sql-attr-string"><title>sql_attr_string</title>
<para>
String attribute declaration.
Multi-value (ie. there may be more than one such attribute declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Introduced in version 1.10-beta.
</para>
<para>
String attributes can store arbitrary strings attached to every document.
There's a fixed size limit of 4 MB per value. Also, <filename>searchd</filename>
will currently cache all the values in RAM, which is an additional implicit limit.
</para>
<para>
As of 1.10-beta, strings can only be used for storage and retrieval.
They can not participate in expressions, be used for filtering, sorting,
or grouping (ie. in WHERE, ORDER or GROUP clauses). Note that attributes
declared using <option>sql_attr_string</option> will <b>not</b> be full-text
indexed; you can use <link linkend="conf-sql-field-string">sql_field_string</link>
directive for that.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_attr_string = title # will be stored but will not be indexed
</programlisting>
</sect2>


<sect2 id="conf-sql-attr-str2wordcount"><title>sql_attr_str2wordcount</title>
<para>
Word-count attribute declaration.
Multi-value (ie. there may be more than one such attribute declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Introduced in version 1.10-beta.
</para>
<para>
Word-count attribute takes a string column, tokenizes it according
to index settings, and stores the resulting number of tokens in an attribute.
This number of tokens ("word count") is a normal integer that can be later
used, for instance, in custom ranking expressions (boost shorter titles,
help identify exact field matches, etc).
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_attr_str2wordcount = title_wc
</programlisting>
</sect2>


<sect2 id="conf-sql-column-buffers"><title>sql_column_buffers</title>
<para>
Per-column buffer sizes.
Optional, default is empty (deduce the sizes automatically).
Applies to <option>odbc</option>, <option>mssql</option> source types only.
Introduced in version 1.11-beta.
</para>
<para>
ODBC and MS SQL drivers sometimes can not return the maximum
actual column size to be expected. For instance, NVARCHAR(MAX) columns
always report their length as 2147483647 bytes to
<filename>indexer</filename> even though the actually used length
is likely considerably less. However, the receiving buffers still
need to be allocated upfront, and their sizes have to be determined.
When the driver does not report the column length at all, Sphinx
allocates default 1 KB buffers for each non-char column, and 1 MB
buffers for each char column. Driver-reported column length
also gets clamped by an upper limie of 8 MB, so in case the
driver reports (almost) a 2 GB column length, it will be clamped
and a 8 MB buffer will be allocated instead for that column.
These hard-coded limits can be overridden using the
<code>sql_column_buffers</code> directive, either in order
to save memory on actually shorter columns, or overcome
the 8 MB limit on actually longer columns. The directive values
must be a comma-separated lists of selected column names and sizes:
<programlisting>
sql_column_buffers = &lt;colname&gt;=&lt;size&gt;[K|M] [, ...]
</programlisting>
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_query = SELECT id, mytitle, mycontent FROM documents
sql_column_buffers = mytitle=64K, mycontent=10M
</programlisting>
</sect2>



<sect2 id="conf-sql-field-string"><title>sql_field_string</title>
<para>
Combined string attribute and full-text field declaration.
Multi-value (ie. there may be more than one such attribute declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Introduced in version 1.10-beta.
</para>
<para>
<link linkend="conf-sql-attr-string">sql_attr_string</link> only stores the column
value but does not full-text index it.  In some cases it might be desired to both full-text
index the column and store it as attribute.  <option>sql_field_string</option> lets you do
exactly that. Both the field and the attribute will be named the same.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_field_string = title # will be both indexed and stored
</programlisting>
</sect2>


<sect2 id="conf-sql-field-str2wordcount"><title>sql_field_str2wordcount</title>
<para>
Combined word-count attribute and full-text field declaration.
Multi-value (ie. there may be more than one such attribute declared), optional.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Introduced in version 1.10-beta.
</para>
<link linkend="conf-sql-attr-str2wordcount">sql_attr_str2wordcount</link> only stores the column
word count but does not full-text index it.  In some cases it might be desired to both full-text
index the column and also have the count.  <option>sql_field_str2wordcount</option> lets you do
exactly that. Both the field and the attribute will be named the same.
<bridgehead>Example:</bridgehead>
<programlisting>
sql_field_str2wordcount = title # will be indexed, and counted/stored
</programlisting>
</sect2>


<sect2 id="conf-sql-file-field"><title>sql_file_field</title>
<para>
File based field declaration.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Introduced in version 1.10-beta.
</para>
<para>
This directive makes <filename>indexer</filename> interpret field contents
as a file name, and load and index the referred file.  Files larger than
<link linkend="conf-max-file-field-buffer">max_file_field_buffer</link>
in size are skipped.  Any errors during the file loading (IO errors, missed
limits, etc) will be reported as indexing warnings and will <b>not</b> early
terminate the indexing.  No content will be indexed for such files.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_file_field = my_file_path # load and index files referred to by my_file_path
</programlisting>
</sect2>


<sect2 id="conf-sql-query-post"><title>sql_query_post</title>
<para>
Post-fetch query.
Optional, default value is empty.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
This query is executed immediately after <link linkend="conf-sql-query">sql_query</link>
completes successfully. When post-fetch query produces errors,
they are reported as warnings, but indexing is <b>not</b> terminated.
It's result set is ignored. Note that indexing is <b>not</b> yet completed
at the point when this query gets executed, and further indexing still may fail.
Therefore, any permanent updates should not be done from here.
For instance, updates on helper table that permanently change
the last successfully indexed ID should not be run from post-fetch
query; they should be run from <link linkend="conf-sql-query-post-index">post-index query</link> instead.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_query_post = DROP TABLE my_tmp_table
</programlisting>
</sect2>


<sect2 id="conf-sql-query-post-index"><title>sql_query_post_index</title>
<para>
Post-index query.
Optional, default value is empty.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
This query is executed when indexing is fully and succesfully completed.
If this query produces errors, they are reported as warnings,
but indexing is <b>not</b> terminated. It's result set is ignored.
<code>$maxid</code> macro can be used in its text; it will be
expanded to maximum document ID which was actually fetched
from the database during indexing. If no documents were indexed,
$maxid will be expanded to 0.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_query_post_index = REPLACE INTO counters ( id, val ) \
    VALUES ( 'max_indexed_id', $maxid )
</programlisting>
</sect2>


<sect2 id="conf-sql-ranged-throttle"><title>sql_ranged_throttle</title>
<para>
Ranged query throttling period, in milliseconds.
Optional, default is 0 (no throttling).
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
</para>
<para>
Throttling can be useful when indexer imposes too much load on the
database server. It causes the indexer to sleep for given amount of
milliseconds once per each ranged query step. This sleep is unconditional,
and is performed before the fetch query.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_ranged_throttle = 1000 # sleep for 1 sec before each query step
</programlisting>
</sect2>


<sect2 id="conf-sql-query-info"><title>sql_query_info</title>
<para>
Document info query.
Optional, default is empty.
Applies to <option>mysql</option> source type only.
</para>
<para>
Only used by CLI search to fetch and display document information,
only works with MySQL at the moment, and only intended for debugging purposes.
This query fetches the row that will be displayed by CLI search utility
for each document ID. It is required to contain <code>$id</code> macro
that expands to the queried document ID.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
sql_query_info = SELECT * FROM documents WHERE id=$id
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-command"><title>xmlpipe_command</title>
<para>
Shell command that invokes xmlpipe stream producer.
Mandatory.
Applies to <option>xmlpipe</option> and <option>xmlpipe2</option> source types only.
</para>
<para>
Specifies a command that will be executed and which output
will be parsed for documents. Refer to <xref linkend="xmlpipe"/>
or <xref linkend="xmlpipe2"/> for specific format description.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_command = cat /home/sphinx/test.xml
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-field"><title>xmlpipe_field</title>
<para>
xmlpipe field declaration.
Multi-value, optional.
Applies to <option>xmlpipe2</option> source type only. Refer to <xref linkend="xmlpipe2"/>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_field = subject
xmlpipe_field = content
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-field-string"><title>xmlpipe_field_string</title>
<para>
xmlpipe field and string attribute declaration.
Multi-value, optional.
Applies to <option>xmlpipe2</option> source type only. Refer to <xref linkend="xmlpipe2"/>.
Introduced in version 1.10-beta.
</para>
<para>
Makes the specified XML element indexed as both a full-text field and a string attribute.
Equivalent to <![CDATA[<sphinx:field name="field" attr="string"/>]]> declaration within the XML file.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_field_string = subject
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-field-wordcount"><title>xmlpipe_field_wordcount</title>
<para>
xmlpipe field and word count attribute declaration.
Multi-value, optional.
Applies to <option>xmlpipe2</option> source type only. Refer to <xref linkend="xmlpipe2"/>.
Introduced in version 1.10-beta.
</para>
<para>
Makes the specified XML element indexed as both a full-text field and a word count attribute.
Equivalent to <![CDATA[<sphinx:field name="field" attr="wordcount"/>]]> declaration within the XML file.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_field_wordcount = subject
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-attr-uint"><title>xmlpipe_attr_uint</title>
<para>
xmlpipe integer attribute declaration.
Multi-value, optional.
Applies to <option>xmlpipe2</option> source type only.
Syntax fully matches that of <link linkend="conf-sql-attr-uint">sql_attr_uint</link>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_attr_uint = author
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-attr-bool"><title>xmlpipe_attr_bool</title>
<para>
xmlpipe boolean attribute declaration.
Multi-value, optional.
Applies to <option>xmlpipe2</option> source type only.
Syntax fully matches that of <link linkend="conf-sql-attr-bool">sql_attr_bool</link>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_attr_bool = is_deleted # will be packed to 1 bit
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-attr-timestamp"><title>xmlpipe_attr_timestamp</title>
<para>
xmlpipe UNIX timestamp attribute declaration.
Multi-value, optional.
Applies to <option>xmlpipe2</option> source type only.
Syntax fully matches that of <link linkend="conf-sql-attr-timestamp">sql_attr_timestamp</link>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_attr_timestamp = published
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-attr-str2ordinal"><title>xmlpipe_attr_str2ordinal</title>
<para>
xmlpipe string ordinal attribute declaration.
Multi-value, optional.
Applies to <option>xmlpipe2</option> source type only.
Syntax fully matches that of <link linkend="conf-sql-attr-str2ordinal">sql_attr_str2ordinal</link>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_attr_str2ordinal = author_sort
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-attr-float"><title>xmlpipe_attr_float</title>
<para>
xmlpipe floating point attribute declaration.
Multi-value, optional.
Applies to <option>xmlpipe2</option> source type only.
Syntax fully matches that of <link linkend="conf-sql-attr-float">sql_attr_float</link>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_attr_float = lat_radians
xmlpipe_attr_float = long_radians
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-attr-multi"><title>xmlpipe_attr_multi</title>
<para>
xmlpipe MVA attribute declaration.
Multi-value, optional.
Applies to <option>xmlpipe2</option> source type only.
</para>
<para>
This setting declares an MVA attribute tag in xmlpipe2 stream.
The contents of the specified tag will be parsed and a list of integers
that will constitute the MVA will be extracted, similar to how
<link linkend="conf-sql-attr-multi">sql_attr_multi</link> parses
SQL column contents when 'field' MVA source type is specified.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_attr_multi = taglist
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-attr-string"><title>xmlpipe_attr_string</title>
<para>
xmlpipe string declaration.
Multi-value, optional.
Applies to <option>xmlpipe2</option> source type only.
Introduced in version 1.10-beta.
</para>
<para>
This setting declares a string attribute tag in xmlpipe2 stream.
The contents of the specified tag will be parsed and stored as a string value.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_attr_string = subject
</programlisting>
</sect2>


<sect2 id="conf-xmlpipe-fixup-utf8"><title>xmlpipe_fixup_utf8</title>
<para>
Perform Sphinx-side UTF-8 validation and filtering to prevent XML parser from choking on non-UTF-8 documents.
Optional, default is 0.
Applies to <option>xmlpipe2</option> source type only.
</para>
<para>
Under certain occasions it might be hard or even impossible to guarantee
that the incoming XMLpipe2 document bodies are in perfectly valid and
conforming UTF-8 encoding.  For instance, documents with national
single-byte encodings could sneak into the stream. libexpat XML parser
is fragile, meaning that it will stop processing in such cases.
UTF8 fixup feature lets you avoid that. When fixup is enabled,
Sphinx will preprocess the incoming stream before passing it to the
XML parser and replace invalid UTF-8 sequences with spaces.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
xmlpipe_fixup_utf8 = 1
</programlisting>
</sect2>


<sect2 id="conf-mssql-winauth"><title>mssql_winauth</title>
<para>
MS SQL Windows authentication flag.
Boolean, optional, default value is 0 (false).
Applies to <option>mssql</option> source type only.
Introduced in version 0.9.9-rc1.
</para>
<para>
Whether to use currently logged in Windows account credentials for
authentication when connecting to MS SQL Server. Note that when running
<filename>searchd</filename> as a service, account user can differ
from the account you used to install the service.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
mssql_winauth = 1
</programlisting>
</sect2>


<sect2 id="conf-mssql-unicode"><title>mssql_unicode</title>
<para>
MS SQL encoding type flag.
Boolean, optional, default value is 0 (false).
Applies to <option>mssql</option> source type only.
Introduced in version 0.9.9-rc1.
</para>
<para>
Whether to ask for Unicode or single-byte data when querying MS SQL Server.
This flag <b>must</b> be in sync with <link linkend="conf-charset-type">charset_type</link> directive;
that is, to index Unicode data, you must set both <option>charset_type</option> in the index
(to 'utf-8') and <option>mssql_unicode</option> in the source (to 1).
For reference, MS SQL will actually return data in UCS-2 encoding instead of UTF-8,
but Sphinx will automatically handle that.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
mssql_unicode = 1
</programlisting>
</sect2>


<sect2 id="conf-unpack-zlib"><title>unpack_zlib</title>
<para>
Columns to unpack using zlib (aka deflate, aka gunzip).
Multi-value, optional, default value is empty list of columns.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Introduced in version 0.9.9-rc1.
</para>
<para>
Columns specified using this directive will be unpacked by <filename>indexer</filename>
using standard zlib algorithm (called deflate and also implemented by <filename>gunzip</filename>).
When indexing on a different box than the database, this lets you offload the database, and save on network traffic.
The feature is only available if zlib and zlib-devel were both available during build time.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
unpack_zlib = col1
unpack_zlib = col2
</programlisting>
</sect2>


<sect2 id="conf-unpack-mysqlcompress"><title>unpack_mysqlcompress</title>
<para>
Columns to unpack using MySQL UNCOMPRESS() algorithm.
Multi-value, optional, default value is empty list of columns.
Applies to SQL source types (<option>mysql</option>, <option>pgsql</option>, <option>mssql</option>) only.
Introduced in version 0.9.9-rc1.
</para>
<para>
Columns specified using this directive will be unpacked by <filename>indexer</filename>
using modified zlib algorithm used by MySQL COMPRESS() and UNCOMPRESS() functions.
When indexing on a different box than the database, this lets you offload the database, and save on network traffic.
The feature is only available if zlib and zlib-devel were both available during build time.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
unpack_mysqlcompress = body_compressed
unpack_mysqlcompress = description_compressed
</programlisting>
</sect2>


<sect2 id="conf-unpack-mysqlcompress-maxsize"><title>unpack_mysqlcompress_maxsize</title>
<para>
Buffer size for UNCOMPRESS()ed data.
Optional, default value is 16M.
Introduced in version 0.9.9-rc1.
</para>
<para>
When using <link linkend="conf-unpack-mysqlcompress">unpack_mysqlcompress</link>,
due to implementation intrincacies it is not possible to deduce the required buffer size
from the compressed data. So the buffer must be preallocated in advance, and unpacked
data can not go over the buffer size. This option lets you control the buffer size,
both to limit <filename>indexer</filename> memory use, and to enable unpacking
of really long data fields if necessary.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
unpack_mysqlcompress_maxsize = 1M
</programlisting>
</sect2>


</sect1>
<sect1 id="confgroup-index"><title>Index configuration options</title>


<sect2 id="conf-index-type"><title>type</title>
<para>
Index type.
Known values are 'plain', 'distributed', and 'rt'.
Optional, default is 'plain' (plain local index).
</para>
<para>
Sphinx supports several different types of indexes.
Versions 0.9.x supported two index types: plain local indexes
that are stored and processed on the local machine; and distributed indexes,
that involve not only local searching but querying remote <filename>searchd</filename>
instances over the network as well (see <xref linkend="distributed"/>).
Version 1.10-beta also adds support
for so-called real-time indexes (or RT indexes for short) that
are also stored and processed locally, but additionally allow
for on-the-fly updates of the full-text index (see <xref linkend="rt-indexes"/>).
Note that <emphasis>attributes</emphasis> can be updated on-the-fly using
either plain local indexes or RT ones.
</para>
<para>
Index type setting lets you choose the needed type.
By default, plain local index type will be assumed.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
type = distributed
</programlisting>
</sect2>


<sect2 id="conf-source"><title>source</title>
<para>
Adds document source to local index.
Multi-value, mandatory.
</para>
<para>
Specifies document source to get documents from when the current
index is indexed. There must be at least one source. There may be multiple
sources, without any restrictions on the source types: ie. you can pull
part of the data from MySQL server, part from PostgreSQL, part from
the filesystem using xmlpipe2 wrapper.
</para>
<para>
However, there are some restrictions on the source data. First,
document IDs must be globally unique across all sources. If that
condition is not met, you might get unexpected search results.
Second, source schemas must be the same in order to be stored
within the same index.
</para>
<para>
No source ID is stored automatically. Therefore, in order to be able
to tell what source the matched document came from, you will need to
store some additional information yourself. Two typical approaches
include:
<orderedlist>
<listitem>mangling document ID and encoding source ID in it:
<programlisting>
source src1
{
	sql_query = SELECT id*10+1, ... FROM table1
	...
}

source src2
{
	sql_query = SELECT id*10+2, ... FROM table2
	...
}
</programlisting>
</listitem>
<listitem>
storing source ID simply as an attribute:
<programlisting>
source src1
{
	sql_query = SELECT id, 1 AS source_id FROM table1
	sql_attr_uint = source_id
	...
}

source src2
{
	sql_query = SELECT id, 2 AS source_id FROM table2
	sql_attr_uint = source_id
	...
}
</programlisting>
</listitem>
</orderedlist>
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
source = srcpart1
source = srcpart2
source = srcpart3
</programlisting>
</sect2>


<sect2 id="conf-path"><title>path</title>
<para>
Index files path and file name (without extension).
Mandatory.
</para>
<para>
Path specifies both directory and file name, but without extension.
<filename>indexer</filename> will append different extensions
to this path when generating final names for both permanent and
temporary index files. Permanent data files have several different
extensions starting with '.sp'; temporary files' extensions
start with '.tmp'. It's safe to remove <filename>.tmp*</filename>
files is if indexer fails to remove them automatically.
</para>
<para>
For reference, different index files store the following data:
<itemizedlist>
<listitem><filename>.spa</filename> stores document attributes (used in <link linkend="conf-docinfo">extern docinfo</link> storage mode only);</listitem>
<listitem><filename>.spd</filename> stores matching document ID lists for each word ID;</listitem>
<listitem><filename>.sph</filename> stores index header information;</listitem>
<listitem><filename>.spi</filename> stores word lists (word IDs and pointers to <filename>.spd</filename> file);</listitem>
<listitem><filename>.spk</filename> stores kill-lists;</listitem>
<listitem><filename>.spm</filename> stores MVA data;</listitem>
<listitem><filename>.spp</filename> stores hit (aka posting, aka word occurence) lists for each word ID;</listitem>
<listitem><filename>.sps</filename> stores string attribute data.</listitem>
</itemizedlist>
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
path = /var/data/test1
</programlisting>
</sect2>


<sect2 id="conf-docinfo"><title>docinfo</title>
<para>
Document attribute values (docinfo) storage mode.
Optional, default is 'extern'.
Known values are 'none', 'extern' and 'inline'.
</para>
<para>
Docinfo storage mode defines how exactly docinfo will be
physically stored on disk and RAM. "none" means that there will be
no docinfo at all (ie. no attributes). Normally you need not to set
"none" explicitly because Sphinx will automatically select "none"
when there are no attributes configured. "inline" means that the
docinfo will be stored in the <filename>.spd</filename> file,
along with the document ID lists. "extern" means that the docinfo
will be stored separately (externally) from document ID lists,
in a special <filename>.spa</filename> file.
</para>
<para>
Basically, externally stored docinfo must be kept in RAM when querying.
for performance reasons. So in some cases "inline" might be the only option.
However, such cases are infrequent, and docinfo defaults to "extern".
Refer to <xref linkend="attributes"/> for in-depth discussion
and RAM usage estimates.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
docinfo = inline
</programlisting>
</sect2>


<sect2 id="conf-mlock"><title>mlock</title>
<para>
Memory locking for cached data.
Optional, default is 0 (do not call mlock()).
</para>
<para>
For search performance, <filename>searchd</filename> preloads
a copy of <filename>.spa</filename> and <filename>.spi</filename>
files in RAM, and keeps that copy in RAM at all times. But if there
are no searches on the index for some time, there are no accesses
to that cached copy, and OS might decide to swap it out to disk.
First queries to such "cooled down" index will cause swap-in
and their latency will suffer.
</para>
<para>
Setting mlock option to 1 makes Sphinx lock physical RAM used
for that cached data using mlock(2) system call, and that prevents
swapping (see man 2 mlock for details). mlock(2) is a privileged call,
so it will require <filename>searchd</filename> to be either run
from root account, or be granted enough privileges otherwise.
If mlock() fails, a warning is emitted, but index continues
working.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
mlock = 1
</programlisting>
</sect2>


<sect2 id="conf-morphology"><title>morphology</title>
<para>
A list of morphology preprocessors to apply.
Optional, default is empty (do not apply any preprocessor).
</para>
<para>
Morphology preprocessors can be applied to the words being
indexed to replace different forms of the same word with the base,
normalized form. For instance, English stemmer will normalize
both "dogs" and "dog" to "dog", making search results for
both searches the same.
</para>
<para>
Built-in preprocessors include English stemmer, Russian stemmer
(that supports UTF-8 and Windows-1251 encodings), Soundex,
and Metaphone. The latter two replace the words with special
phonetic codes that are equal is words are phonetically close.
Additional stemmers provided by <ulink url="http://snowball.tartarus.org/">Snowball</ulink>
project <ulink url="http://snowball.tartarus.org/dist/libstemmer_c.tgz">libstemmer</ulink> library
can be enabled at compile time using <option>--with-libstemmer</option> <filename>configure</filename> option.
Built-in English and Russian stemmers should be faster than their
libstemmer counterparts, but can produce slightly different results,
because they are based on an older version. Metaphone implementation
is based on Double Metaphone algorithm and indexes the primary code.
</para>
<para>
Built-in values that are available for use in <option>morphology</option>
option are as follows:
<itemizedlist>
<listitem>none - do not perform any morphology processing;</listitem>
<listitem>stem_en - apply Porter's English stemmer;</listitem>
<listitem>stem_ru - apply Porter's Russian stemmer;</listitem>
<listitem>stem_enru - apply Porter's English and Russian stemmers;</listitem>
<listitem>stem_cz - apply Czech stemmer;</listitem>
<listitem>soundex - replace keywords with their SOUNDEX code;</listitem>
<listitem>soundex - replace keywords with their METAPHONE code.</listitem>
</itemizedlist>
Additional values provided by libstemmer are in 'libstemmer_XXX' format,
where XXX is libstemmer algorithm codename (refer to
<filename>libstemmer_c/libstemmer/modules.txt</filename> for a complete list).
</para>
<para>
Several stemmers can be specified (comma-separated). They will be applied
to incoming words in the order they are listed, and the processing will stop
once one of the stemmers actually modifies the word.
Also when <link linkend="conf-wordforms">wordforms</link> feature is enabled
the word will be looked up in word forms dictionary first, and if there is
a matching entry in the dictionary, stemmers will not be applied at all.
Or in other words, <link linkend="conf-wordforms">wordforms</link> can be
used to implement stemming exceptions.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
morphology = stem_en, libstemmer_sv
</programlisting>
</sect2>


<sect2 id="conf-dict"><title>dict</title>
<para>
The keywords dictionary type.
Known values are 'crc' and 'keywords'.
Optional, default is 'crc'.
Introduced in version 1.11-beta.
</para>
<para>
The default dictionary type in Sphinx, and the only one available
until version 1.11-beta, is a so-called CRC dictionary which never
stores the original keyword text in the index. Instead, keywords are
replaced with their control sum value (either CRC32 or FNV64, depending
whether Sphinx was built with <option>--enable-id64</option>) both
when searching and indexing, and that value is used internally
in the index.
</para>
<para>
That approach has two drawbacks. First, in CRC32 case there is
a chance of control sum collision between several pairs of different
keywords, growing quadratically with the number of unique keywords
in the index. (FNV64 case is unaffected in practice, as a chance
of a single FNV64 collision in a dictionary of 1 billion entries
is approximately 1:16, or 6.25 percent. And most dictionaries
will be much more compact that a billion keywords, as a typical
spoken human language has in the region of 1 to 10 million word
forms.) Second, and more importantly, substring searches are not
directly possible with control sums. Sphinx alleviated that by
pre-indexing all the possible substrings as separate keywords
(see <xref linkend="conf-min-prefix-len"/>, <xref linkend="conf-min-infix-len"/>
directives). That actually has an added benefit of matching
substrings in the quickest way possible. But at the same time
pre-indexing all substrings grows the index size a lot (factors
of 3-10x and even more would not be unusual) and impacts the
indexing time respectively, rendering substring searches
on big indexes rather impractical.
</para>
<para>
Keywords dictionary, introduced in 1.11-beta, fixes both these
drawbacks. It stores the keywords in the index and performs
search-time wildcard expansion. For example, a search for a
'test*' prefix could internally expand to 'test|tests|testing'
query based on the dictionary contents. That expansion is fully
transparent to the application, except that the separate
per-keyword statistics for all the actually matched keywords 
would now also be reported.
</para>
<para>
Indexing with keywords dictionary should be 1.1x to 1.3x slower
compared to regular, non-substring indexing - but times faster
compared to substring indexing (either prefix or infix). Index size
should only be slightly bigger that than of the regular non-substring
index, with a 1..10% percent total difference
Regular keyword searching time must be very close or identical across
all three discussed index kinds (CRC non-substring, CRC substring,
keywords). Substring searching time can vary greatly depending
on how many actual keywords match the given substring (in other
words, into how many keywords does the search term expand).
The maximum number of keywords matched is restricted by the
<link linkend="conf-expansion-limit">expansion_limit</link>
directive.
</para>
<para>
Essentially, keywords and CRC dictionaries represent the two
different trade-off substring searching decisions. You can choose
to either sacrifice indexing time and index size in favor of
top-speed worst-case searches (CRC dictionary), or only slightly
impact indexing time but sacrifice worst-case searching time when
the prefix expands into very many keywords (keywords dictionary).
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
dict = keywords
</programlisting>
</sect2>


<sect2 id="conf-index-sp"><title>index_sp</title>
<para>
Whether to detect and index sentence and paragraph boundaries.
Optional, default is 0 (do not detect and index).
Introduced in version 1.11-beta.
</para>
<para>
This directive enables sentence and paragraph boundary indexing.
It's required for the SENTENCE and PARAGRAPH operators to work.
Sentence boundary detection is based on plain text analysis, so you
only need to set <code>index_sp = 1</code> to enable it. Paragraph
detection is however based on HTML markup, and happens in the
<link linkend="conf-html-strip">HTML stripper</link>.
So to index paragraph locations you also need to enable the stripper
by specifying <code>html_strip = 1</code>. Both types of boundaries
are detected based on a few built-in rules enumerated just below.
</para>
<para>
Sentence boundary detection rules are as follows.
<itemizedlist>
<listitem>Question and excalamation signs (? and !) are always a sentence boundary.</listitem>
<listitem>Trailing dot (.) is a sentence boundary, except:
	<itemizedlist>
	<listitem>When followed by a letter. That's considered a part of an abbreviation (as in "S.T.A.L.K.E.R" or "Goldman Sachs S.p.A.").</listitem>
	<listitem>When followed by a comma. That's considered an abbreviation followed by a comma (as in "Telecom Italia S.p.A., founded in 1994").</listitem>
	<listitem>When followed by a space and a small letter. That's considered an abbreviation within a sentence (as in "News Corp. announced in Februrary").</listitem>
	<listitem>When preceded by a space and a capital letter, and followed by a space. That's considered a middle initial (as in "John D. Doe").</listitem>
	</itemizedlist>
</listitem>
</itemizedlist>
</para>
<para>
Paragraph boundaries are inserted at every block-level HTML tag.
Namely, those are (as taken from HTML 4 standard) ADDRESS, BLOCKQUOTE,
CAPTION, CENTER, DD, DIV, DL, DT, H1, H2, H3, H4, H5, LI, MENU, OL, P,
PRE, TABLE, TBODY, TD, TFOOT, TH, THEAD, TR, and UL.
</para>
<para>
Both sentences and paragraphs increment the keyword position counter by 1.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
index_sp = 1
</programlisting>
</sect2>


<sect2 id="conf-index-zones"><title>index_zones</title>
<para>
A list of in-field HTML/XML zones to index.
Optional, default is empty (do not index zones).
Introduced in version 1.11-beta.
</para>
<para>
Zones can be formally defined as follows. Everything between
an opening and a matching closing tag is called a span, and
the aggregate of all spans corresponding sharing the same
tag name is called a zone. For instance, everything between
the occurrences of &lt;H1&gt; and &lt;/H1&gt; in the document
field belongs to H1 zone.
</para>
<para>
Zone indexing, enabled by <code>index_zones</code> directive,
is an optional extension of the HTML stripper. So it will also
require that the <link linkend="conf-html-strip">stripper</link>
is enabled (with <code>html_strip = 1</code>). The value of the
<code>index_zones</code> should be a comma-separated list of
those tag names and wildcards (ending with a star) that should
be indexed as zones.
</para>
<para>
Zones can nest and overlap arbitrarily. The only requirement
is that every opening tag has a matching tag. You can also have
an arbitrary number of both zones (as in unique zone names,
such as H1) and spans (all the occurrences of those H1 tags)
in a document.
Once indexed, zones can then be used for matching with
the ZONE operator, see <xref linkend="extended-syntax"/>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
index_zones = h*, th, title
</programlisting>
</sect2>


<sect2 id="conf-min-stemming-len"><title>min_stemming_len</title>
<para>
Minimum word length at which to enable stemming.
Optional, default is 1 (stem everything).
Introduced in version 0.9.9-rc1.
</para>
<para>
Stemmers are not perfect, and might sometimes produce undesired results.
For instance, running "gps" keyword through Porter stemmer for English
results in "gp", which is not really the intent. <option>min_stemming_len</option>
feature lets you suppress stemming based on the source word length,
ie. to avoid stemming too short words. Keywords that are shorter than
the given threshold will not be stemmed. Note that keywords that are
exactly as long as specified <b>will</b> be stemmed. So in order to avoid
stemming 3-character keywords, you should specify 4 for the value.
For more finely grained control, refer to <link linkend="conf-wordforms">wordforms</link> feature.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
min_stemming_len = 4
</programlisting>
</sect2>


<sect2 id="conf-stopwords"><title>stopwords</title>
<para>
Stopword files list (space separated).
Optional, default is empty.
</para>
<para>
Stopwords are the words that will not be indexed. Typically you'd
put most frequent words in the stopwords list because they do not add
much value to search results but consume a lot of resources to process.
</para>
<para>
You can specify several file names, separated by spaces. All the files
will be loaded. Stopwords file format is simple plain text. The encoding
must match index encoding specified in <link linkend="conf-charset-type">charset_type</link>.
File data will be tokenized with respect to <link linkend="conf-charset-table">charset_table</link>
settings, so you can use the same separators as in the indexed data.
The <link linkend="conf-morphology">stemmers</link> will also be
applied when parsing stopwords file.
</para>
<para>
While stopwords are not indexed, they still do affect the keyword positions.
For instance, assume that "the" is a stopword, that document 1 contains the line
"in office", and that document 2 contains "in the office". Searching for "in office"
as for exact phrase will only return the first document, as expected, even though
"the" in the second one is stopped.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
stopwords = /usr/local/sphinx/data/stopwords.txt
stopwords = stopwords-ru.txt stopwords-en.txt
</programlisting>
</sect2>


<sect2 id="conf-wordforms"><title>wordforms</title>
<para>
Word forms dictionary.
Optional, default is empty.
</para>
<para>
Word forms are applied after tokenizing the incoming text
by <link linkend="conf-charset-table">charset_table</link> rules.
They essentialy let you replace one word with another. Normally,
that would be used to bring different word forms to a single
normal form (eg. to normalize all the variants such as "walks",
"walked", "walking" to the normal form "walk"). It can also be used
to implement stemming exceptions, because stemming is not applied
to words found in the forms list.
</para>
<para>
Dictionaries are used to normalize incoming words both during indexing
and searching. Therefore, to pick up changes in wordforms file
it's required to reindex and restart <filename>searchd</filename>.
</para>
<para>
Word forms support in Sphinx is designed to support big dictionaries well.
They moderately affect indexing speed: for instance, a dictionary with 1 million
entries slows down indexing about 1.5 times. Searching speed is not affected at all.
Additional RAM impact is roughly equal to the dictionary file size,
and dictionaries are shared across indexes: ie. if the very same 50 MB wordforms
file is specified for 10 different indexes, additional <filename>searchd</filename>
RAM usage will be about 50 MB.
</para>
<para>
Dictionary file should be in a simple plain text format. Each line 
should contain source and destination word forms, in exactly the same
encoding as specified in <link linkend="conf-charset-type">charset_type</link>,
separated by "greater" sign. Rules from the
<link linkend="conf-charset-table">charset_table</link> will be
applied when the file is loaded. So basically it's as case sensitive
as your other full-text indexed data, ie. typically case insensitive.
Here's the file contents sample:
<programlisting>
walks &gt; walk
walked &gt; walk
walking &gt; walk
</programlisting>
</para>
<para>
There is a bundled <filename>spelldump</filename> utility that
helps you create a dictionary file in the format Sphinx can read
from source <filename>.dict</filename> and <filename>.aff</filename>
dictionary files in <filename>ispell</filename> or <filename>MySpell</filename>
format (as bundled with OpenOffice).
</para>
<para>
Starting with version 0.9.9-rc1, you can map several source words
to a single destination word. Because the work happens on tokens,
not the source text, differences in whitespace and markup are ignored.
<programlisting>
core 2 duo &gt; c2d
e6600 &gt; c2d
core 2duo &gt; c2d
</programlisting>
</para>
<para>
Notice however that the <emphasis>destination</emphasis> wordforms
are still always interpreted as a <emphasis>single</emphasis> keyword!
Having a mapping like "St John > Saint John" will result in <b>not</b>
matching "St John" when searching for "Saint" or "John", because the
destination keyword will be "Saint John" with a space character in it
(and it's barely possible to input a destination keyword with a space).
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
wordforms = /usr/local/sphinx/data/wordforms.txt
</programlisting>
</sect2>


<sect2 id="conf-exceptions"><title>exceptions</title>
<para>
Tokenizing exceptions file.
Optional, default is empty.
</para>
<para>
Exceptions allow to map one or more tokens (including tokens with
characters that would normally be excluded) to a single keyword.
They are similar to <link linkend="conf-wordforms">wordforms</link>
in that they also perform mapping, but have a number of important
differences.
</para>
<para>
Short summary of the differences is as follows:
<itemizedlist>
<listitem>exceptions are case sensitive, wordforms are not;</listitem>
<listitem>exceptions allow to detect sequences of tokens, wordforms work with single words only;</listitem>
<listitem>exceptions can use special characters that are <b>not</b> in charset_table, wordforms fully obey charset_table;</listitem>
<listitem>exceptions can underperform on huge dictionaries, wordforms handle millions of entries well.</listitem>
</itemizedlist>
</para>
<para>
The expected file format is also plain text, with one line per exception,
and the line format is as follows:
<programlisting>
map-from-tokens => map-to-token
</programlisting>
Example file:
<programlisting>
AT &amp; T => AT&amp;T
AT&amp;T => AT&amp;T
Standarten   Fuehrer => standartenfuhrer
Standarten Fuhrer => standartenfuhrer
MS Windows => ms windows
Microsoft Windows => ms windows
C++ => cplusplus
c++ => cplusplus
C plus plus => cplusplus
</programlisting>
All tokens here are case sensitive: they will <b>not</b> be processed by
<link linkend="conf-charset-table">charset_table</link> rules. Thus, with
the example exceptions file above, "At&amp;t" text will be tokenized as two
keywords "at" and "t", because of lowercase letters. On the other hand,
"AT&amp;T" will match exactly and produce single "AT&amp;T" keyword.
</para>
<para>
Note that this map-to keyword is a) always interpereted
as a <emphasis>single</emphasis> word, and b) is both case and space
sensitive! In our sample, "ms windows" query will <emphasis>not</emphasis>
match the document with "MS Windows" text. The query will be interpreted
as a query for two keywords, "ms" and "windows". And what "MS Windows"
gets mapped to is a <emphasis>single</emphasis> keyword "ms windows",
with a space in the middle. On the other hand, "standartenfuhrer"
will retrieve documents with "Standarten Fuhrer" or "Standarten Fuehrer"
contents (capitalized exactly like this), or any capitalization variant
of the keyword itself, eg. "staNdarTenfUhreR". (It won't catch
"standarten fuhrer", however: this text does not match any of the
listed exceptions because of case sensitivity, and gets indexed
as two separate keywords.)
</para>
<para>
Whitespace in the map-from tokens list matters, but its amount does not.
Any amount of the whitespace in the map-form list will match any other amount
of whitespace in the indexed document or query. For instance, "AT&#160;&amp;&#160;T"
map-from token will match "AT&#160;&#160;&#160;&#160;&amp;&#160;&#160;T" text,
whatever the amount of space in both map-from part and the indexed text.
Such text will therefore be indexed as a special "AT&amp;T" keyword,
thanks to the very first entry from the sample.
</para>
<para>
Exceptions also allow to capture special characters (that are exceptions
from general <link linkend="conf-charset-table">charset_table</link> rules;
hence the name). Assume that you generally do not want to treat '+'
as a valid character, but still want to be able search for some exceptions
from this rule such as 'C++'. The sample above will do just that, totally
independent of what characters are in the table and what are not.
</para>
<para>
Exceptions are applied to raw incoming document and query data
during indexing  and searching respectively. Therefore, to pick up
changes in the file it's required to reindex and restart
<filename>searchd</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
exceptions = /usr/local/sphinx/data/exceptions.txt
</programlisting>
</sect2>


<sect2 id="conf-min-word-len"><title>min_word_len</title>
<para>
Minimum indexed word length.
Optional, default is 1 (index everything).
</para>
<para>
Only those words that are not shorter than this minimum will be indexed.
For instance, if min_word_len is 4, then 'the' won't be indexed, but 'they' will be.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
min_word_len = 4
</programlisting>
</sect2>


<sect2 id="conf-charset-type"><title>charset_type</title>
<para>
Character set encoding type.
Optional, default is 'sbcs'.
Known values are 'sbcs' and 'utf-8'.
</para>
<para>
Different encodings have different methods for mapping their internal
characters codes into specific byte sequences. Two most common methods
in use today are single-byte encoding and UTF-8. Their corresponding
charset_type values are 'sbcs' (stands for Single Byte Character Set)
and 'utf-8'. The selected encoding type will be used everywhere where
the index is used: when indexing the data, when parsing the query
against this index, when generating snippets, etc.
</para>
<para>
Note that while 'utf-8' implies that the decoded values must be treated
as Unicode codepoint numbers, there's a family of 'sbcs' encodings that
may in turn treat different byte values differently, and that should be
properly reflected in your <link linkend="conf-charset-table">charset_table</link> settings.
For example, the same byte value of 224 (0xE0 hex) maps to different Russian letters
depending on whether koi-8r or windows-1251 encoding is used.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
charset_type = utf-8
</programlisting>
</sect2>


<sect2 id="conf-charset-table"><title>charset_table</title>
<para>
Accepted characters table, with case folding rules.
Optional, default value depends on <link linkend="conf-charset-type">charset_type</link> value.
</para>
<para>
charset_table is the main workhorse of Sphinx tokenizing process,
ie. the process of extracting keywords from document text or query txet.
It controls what characters are accepted as valid and what are not,
and how the accepted characters should be transformed (eg. should
the case be removed or not).
</para>
<para>
You can think of charset_table as of a big table that has a mapping
for each and every of 100K+ characters in Unicode (or as of a small
256-character table if you're using SBCS). By default, every character
maps to 0, which means that it does not occur within keywords and
should be treated as a separator. Once mentioned in the table,
character is mapped to some other character (most frequently,
either to itself or to a lowercase letter), and is treated
as a valid keyword part.
</para>
<para>
The expected value format is a commas-separated list of mappings.
Two simplest mappings simply declare a character as valid, and map
a single character to another single character, respectively.
But specifying the whole table in such form would result 
in bloated and barely manageable specifications. So there are
several syntax shortcuts that let you map ranges of characters
at once. The complete list is as follows:
<variablelist>
<varlistentry>
	<term>A-&gt;a</term>
	<listitem>Single char mapping, declares source char 'A' as allowed
		to occur within keywords and maps it to destination char 'a'
		(but does <emphasis>not</emphasis> declare 'a' as allowed).
	</listitem>
</varlistentry>
<varlistentry>
	<term>A..Z-&gt;a..z</term>
	<listitem>Range mapping, declares all chars in source range
		as allowed and maps them to the destination range. Does <emphasis>not</emphasis>
		declare destination range as allowed. Also checks ranges' lengths
		(the lengths must be equal).
	</listitem>
</varlistentry>
<varlistentry>
	<term>a</term>
	<listitem>Stray char mapping, declares a character as allowed
		and maps it to itself. Equivalent to a-&gt;a single char mapping.
	</listitem>
</varlistentry>
<varlistentry>
	<term>a..z</term>
	<listitem>Stray range mapping, declares all characters in range
		as allowed and maps them to themselves. Equivalent to
		a..z-&gt;a..z range mapping.
	</listitem>
</varlistentry>
<varlistentry>
	<term>A..Z/2</term>
	<listitem>Checkerboard range map. Maps every pair of chars
		to the second char. More formally, declares odd characters
		in range as allowed and maps them to the even ones; also
		declares even characters as allowed and maps them to themselves.
		For instance, A..Z/2 is equivalent to A-&gt;B, B-&gt;B, C-&gt;D, D-&gt;D,
		..., Y-&gt;Z, Z-&gt;Z. This mapping shortcut is helpful for
		a number of Unicode blocks where uppercase and lowercase
		letters go in such interleaved order instead of contiguous
		chunks.
	</listitem>
</varlistentry>
</variablelist>
</para>
<para>
Control characters with codes from 0 to 31 are always treated as separators.
Characters with codes 32 to 127, ie. 7-bit ASCII characters, can be used
in the mappings as is. To avoid configuration file encoding issues,
8-bit ASCII characters and Unicode characters must be specified in U+xxx form,
where 'xxx' is hexadecimal codepoint number. This form can also be used
for 7-bit ASCII characters to encode special ones: eg. use U+20 to
encode space, U+2E to encode dot, U+2C to encode comma.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
# 'sbcs' defaults for English and Russian
charset_table = 0..9, A..Z-&gt;a..z, _, a..z, \
	U+A8-&gt;U+B8, U+B8, U+C0..U+DF-&gt;U+E0..U+FF, U+E0..U+FF

# 'utf-8' defaults for English and Russian
charset_table = 0..9, A..Z-&gt;a..z, _, a..z, \
	U+410..U+42F-&gt;U+430..U+44F, U+430..U+44F
</programlisting>
</sect2>


<sect2 id="conf-ignore-chars"><title>ignore_chars</title>
<para>
Ignored characters list.
Optional, default is empty.
</para>
<para>
Useful in the cases when some characters, such as soft hyphenation mark (U+00AD),
should be not just treated as separators but rather fully ignored.
For example, if '-' is simply not in the charset_table,
"abc-def" text will be indexed as "abc" and "def" keywords.
On the contrary, if '-' is added to ignore_chars list, the same
text will be indexed as a single "abcdef" keyword.
</para>
<para>
The syntax is the same as for <link linkend="conf-charset-table">charset_table</link>,
but it's only allowed to declare characters, and not allowed to map them. Also,
the ignored characters must not be present in charset_table.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
ignore_chars = U+AD
</programlisting>
</sect2>


<sect2 id="conf-min-prefix-len"><title>min_prefix_len</title>
<para>
Minimum word prefix length to index.
Optional, default is 0 (do not index prefixes).
</para>
<para>
Prefix indexing allows to implement wildcard searching by 'wordstart*' wildcards
(refer to <link linkend="conf-enable-star">enable_star</link> option for details on wildcard syntax).
When mininum prefix length is set to a positive number, indexer will index
all the possible keyword prefixes (ie. word beginnings) in addition to the keywords
themselves. Too short prefixes (below the minimum allowed length) will not
be indexed.
</para>
<para>
For instance, indexing a keyword "example" with min_prefix_len=3
will result in indexing "exa", "exam", "examp", "exampl" prefixes along
with the word itself. Searches against such index for "exam" will match
documents that contain "example" word, even if they do not contain "exam"
on itself. However, indexing prefixes will make the index grow significantly
(because of many more indexed keywords), and will degrade both indexing
and searching times.
</para>
<para>
There's no automatic way to rank perfect word matches higher
in a prefix index, but there's a number of tricks to achieve that.
First, you can setup two indexes, one with prefix indexing and one
without it, search through both, and use <link linkend="api-func-setindexweights">SetIndexWeights()</link>
call to combine weights. Second, you can enable star-syntax and rewrite
your extended-mode queries:
<programlisting>
# in sphinx.conf
enable_star = 1

// in query
$cl->Query ( "( keyword | keyword* ) other keywords" );
</programlisting>
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
min_prefix_len = 3
</programlisting>
</sect2>


<sect2 id="conf-min-infix-len"><title>min_infix_len</title>
<para>
Minimum infix prefix length to index.
Optional, default is 0 (do not index infixes).
</para>
Infix indexing allows to implement wildcard searching by 'start*', '*end', and '*middle*' wildcards
(refer to <link linkend="conf-enable-star">enable_star</link> option for details on wildcard syntax).
When mininum infix length is set to a positive number, indexer will index all the possible keyword infixes
(ie. substrings) in addition to the keywords themselves. Too short infixes
(below the minimum allowed length) will not be indexed.
<para>
For instance, indexing a keyword "test" with min_infix_len=2
will result in indexing "te", "es", "st", "tes", "est" infixes along
with the word itself. Searches against such index for "es" will match
documents that contain "test" word, even if they do not contain "es"
on itself. However, indexing infixes will make the index grow significantly
(because of many more indexed keywords), and will degrade both indexing
and searching times.
</para>
<para>
There's no automatic way to rank perfect word matches higher
in an infix index, but the same tricks as with <link linkend="conf-min-prefix-len">prefix indexes</link>
can be applied.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
min_infix_len = 3
</programlisting>
</sect2>


<sect2 id="conf-prefix-fields"><title>prefix_fields</title>
<para>
The list of full-text fields to limit prefix indexing to.
Optional, default is empty (index all fields in prefix mode).
</para>
<para>
Because prefix indexing impacts both indexing and searching performance,
it might be desired to limit it to specific full-text fields only:
for instance, to provide prefix searching through URLs, but not through
page contents. prefix_fields specifies what fields will be prefix-indexed;
all other fields will be indexed in normal mode. The value format is a
comma-separated list of field names.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
prefix_fields = url, domain
</programlisting>
</sect2>


<sect2 id="conf-infix-fields"><title>infix_fields</title>
<para>
The list of full-text fields to limit infix indexing to.
Optional, default is empty (index all fields in infix mode).
</para>
<para>
Similar to <link linkend="conf-prefix-fields">prefix_fields</link>,
but lets you limit infix-indexing to given fields.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
infix_fields = url, domain
</programlisting>
</sect2>


<sect2 id="conf-enable-star"><title>enable_star</title>
<para>
Enables star-syntax (or wildcard syntax) when searching through prefix/infix indexes.
Optional, default is is 0 (do not use wildcard syntax), for compatibility with 0.9.7.
Known values are 0 and 1.
</para>
<para>
This feature enables "star-syntax", or wildcard syntax, when searching
through indexes which were created with prefix or infix indexing enabled.
It only affects searching; so it can be changed without reindexing
by simply restarting <filename>searchd</filename>.
</para>
<para>
The default value is 0, that means to disable star-syntax
and treat all keywords as prefixes or infixes respectively,
depending on indexing-time <link linkend="conf-min-prefix-len">min_prefix_len</link>
and <link linkend="conf-min-infix-len">min_infix_len settings</link>.
The value of 1 means that star ('*') can be used at the start
and/or the end of the keyword. The star will match zero or more characters.
</para>
<para>
For example, assume that the index was built with infixes and
that enable_star is 1. Searching should work as follows:
<orderedlist>
<listitem>"abcdef" query will match only those documents that contain the exact "abcdef" word in them.</listitem>
<listitem>"abc*" query will match those documents that contain
any words starting with "abc" (including the documents which
contain the exact "abc" word only);</listitem>
<listitem>"*cde*" query will match those documents that contain
any words which have "cde" characters in any part of the word
(including the documents which contain the exact "cde" word only).</listitem>
<listitem>"*def" query will match those documents that contain
any words ending with "def" (including the documents that
contain the exact "def" word only).</listitem>
</orderedlist>
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
enable_star = 1
</programlisting>
</sect2>


<sect2 id="conf-ngram-len"><title>ngram_len</title>
<para>
N-gram lengths for N-gram indexing.
Optional, default is 0 (disable n-gram indexing).
Known values are 0 and 1 (other lengths to be implemented).
</para>
<para>
N-grams provide basic CJK (Chinese, Japanese, Korean) support for
unsegmented texts. The issue with CJK searching is that there could be no
clear separators between the words. Ideally, the texts would be filtered
through a special program called segmenter that would insert separators
in proper locations. However, segmenters are slow and error prone,
and it's common to index contiguous groups of N characters, or n-grams,
instead.
</para>
<para>
When this feature is enabled, streams of CJK characters are indexed
as N-grams. For example, if incoming text is "ABCDEF" (where A to F represent
some CJK characters) and length is 1, in will be indexed as if
it was "A B C D E F". (With length equal to 2, it would produce "AB BC CD DE EF";
but only 1 is supported at the moment.) Only those characters that are
listed in <link linkend="conf-ngram-chars">ngram_chars</link> table
will be split this way; other ones will not be affected.
</para>
<para>
Note that if search query is segmented, ie. there are separators between
individual words, then wrapping the words in quotes and using extended mode
will resut in proper matches being found even if the text was <b>not</b>
segmented. For instance, assume that the original query is BC&#160;DEF.
After wrapping in quotes on the application side, it should look
like "BC"&#160;"DEF" (<emphasis>with</emphasis> quotes). This query
will be passed to Sphinx and internally split into 1-grams too,
resulting in "B&#160;C"&#160;"D&#160;E&#160;F" query, still with
quotes that are the phrase matching operator. And it will match
the text even though there were no separators in the text.
</para>
<para>
Even if the search query is not segmented, Sphinx should still produce
good results, thanks to phrase based ranking: it will pull closer phrase
matches (which in case of N-gram CJK words can mean closer multi-character
word matches) to the top.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
ngram_len = 1
</programlisting>
</sect2>


<sect2 id="conf-ngram-chars"><title>ngram_chars</title>
<para>
N-gram characters list.
Optional, default is empty.
</para>
<para>
To be used in conjunction with in <link linkend="conf-ngram-len">ngram_len</link>,
this list defines characters, sequences of which are subject to N-gram extraction.
Words comprised of other characters will not be affected by N-gram indexing
feature. The value format is identical to <link linkend="conf-charset-table">charset_table</link>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
ngram_chars = U+3000..U+2FA1F
</programlisting>
</sect2>


<sect2 id="conf-phrase-boundary"><title>phrase_boundary</title>
<para>
Phrase boundary characters list.
Optional, default is empty.
</para>
<para>
This list controls what characters will be treated as phrase boundaries,
in order to adjust word positions and enable phrase-level search
emulation through proximity search. The syntax is similar
to <link linkend="conf-charset-table">charset_table</link>.
Mappings are not allowed and the boundary characters must not
overlap with anything else.
</para>
<para>
On phrase boundary, additional word position increment (specified by
<link linkend="conf-phrase-boundary-step">phrase_boundary_step</link>)
will be added to current word position. This enables phrase-level
searching through proximity queries: words in different phrases
will be guaranteed to be more than phrase_boundary_step distance
away from each other; so proximity search within that distance
will be equivalent to phrase-level search.
</para>
<para>
Phrase boundary condition will be raised if and only if such character
is followed by a separator; this is to avoid abbreviations such as
S.T.A.L.K.E.R or URLs being treated as several phrases.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
phrase_boundary = ., ?, !, U+2026 # horizontal ellipsis
</programlisting>
</sect2>


<sect2 id="conf-phrase-boundary-step"><title>phrase_boundary_step</title>
<para>
Phrase boundary word position increment.
Optional, default is 0.
</para>
<para>
On phrase boundary, current word position will be additionally incremented
by this number. See <link linkend="conf-phrase-boundary">phrase_boundary</link> for details.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
phrase_boundary_step = 100
</programlisting>
</sect2>


<sect2 id="conf-html-strip"><title>html_strip</title>
<para>
Whether to strip HTML markup from incoming full-text data.
Optional, default is 0.
Known values are 0 (disable stripping) and 1 (enable stripping).
</para>
<para>
Both HTML tags and entities and considered markup and get processed.
</para>
<para>HTML tags are removed, their contents (i.e., everything between
&lt;P&gt; and &lt;/P&gt;) are left intact by default. You can choose
to keep and index attributes of the tags (e.g., HREF attribute in
an A tag, or ALT in an IMG one). Several well-known inline tags are
completely removed, all other tags are treated as block level and
replaced with whitespace. For example, 'te&lt;B&gt;st&lt;/B&gt;'
text will be indexed as a single keyword 'test', however,
'te&lt;P&gt;st&lt;/P&gt;' will be indexed as two keywords
'te' and 'st'. Known inline tags are as follows: A, B, I, S, U, BASEFONT,
BIG, EM, FONT, IMG, LABEL, SMALL, SPAN, STRIKE, STRONG, SUB, SUP, TT.
</para>
<para>
HTML entities get decoded and replaced with corresponding UTF-8
characters. Stripper supports both numeric forms (such as &amp;#239;)
and text forms (such as &amp;oacute; or &amp;nbsp;). All entities
as specified by HTML4 standard are supported.
</para>
<para>
Stripping does not work with <option>xmlpipe</option> source type
(it's suggested to upgrade to xmlpipe2 anyway). It should work with
properly formed HTML and XHTML, but, just as most browsers, may produce
unexpected results on malformed input (such as HTML with stray &lt;'s
or unclosed &gt;'s).
</para>
<para>
Only the tags themselves, and also HTML comments, are stripped.
To strip the contents of the tags too (eg. to strip embedded scripts),
see <link linkend="conf-html-remove-elements">html_remove_elements</link> option.
There are no restrictions on tag names; ie. everything
that looks like a valid tag start, or end, or a comment
will be stripped.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
html_strip = 1
</programlisting>
</sect2>


<sect2 id="conf-html-index-attrs"><title>html_index_attrs</title>
<para>
A list of markup attributes to index when stripping HTML.
Optional, default is empty (do not index markup attributes).
</para>
<para>
Specifies HTML markup attributes whose contents should be retained and indexed
even though other HTML markup is stripped. The format is per-tag enumeration of
indexable attributes, as shown in the example below.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
html_index_attrs = img=alt,title; a=title;
</programlisting>
</sect2>


<sect2 id="conf-html-remove-elements"><title>html_remove_elements</title>
<para>
A list of HTML elements for which to strip contents along with the elements themselves.
Optional, default is empty string (do not strip contents of any elements).
</para>
<para>
This feature allows to strip element contents, ie. everything that
is between the opening and the closing tags. It is useful to remove
embedded scripts, CSS, etc.	Short tag form for empty elements
(ie. &lt;br /&gt;) is properly supported; ie. the text that
follows such tag will <b>not</b> be removed.
</para>
<para>
The value is a comma-separated list of element (tag) names whose
contents should be removed. Tag names are case insensitive.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
html_remove_elements = style, script
</programlisting>
</sect2>


<sect2 id="conf-local"><title>local</title>
<para>
Local index declaration in the <link linkend="distributed">distributed index</link>.
Multi-value, optional, default is empty.
</para>
<para>
This setting is used to declare local indexes that will be searched when
given distributed index is searched. All local indexes will be searched
<b>sequentially</b>, utilizing only 1 CPU or core; to parallelize processing,
you can configure <filename>searchd</filename> to query itself (refer to
<xref linkend="conf-agent"/> for the details). There might be several local
indexes declared per each distributed index. Any local index can be mentioned
several times in other distributed indexes.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
local = chunk1
local = chunk2
</programlisting>
</sect2>


<sect2 id="conf-agent"><title>agent</title>
<para>
Remote agent declaration in the <link linkend="distributed">distributed index</link>.
Multi-value, optional, default is empty.
</para>
<para>
This setting is used to declare remote agents that will be searched
when given distributed index is searched. The agents can be thought of
as network pointers that specify host, port, and index names. In the basic
case agents would correspond to remote physical machines. More formally,
that is not always correct: you can point several agents to the
same remote machine; or you can even point agents to the very same
single instance of <filename>searchd</filename> (in order to utilize
many CPUs or cores).
</para>
<para>
The value format is as follows:
<programlisting>
agent = specification:remote-indexes-list
specification = hostname ":" port | path
</programlisting>
Where 'hostname' is remote host name; 'port' is remote TCP port; 'path'
is Unix-domain socket path and 'remote-indexes-list' is a
comma-separated list of remote index names.
</para>
<para>
All agents will be searched in parallel. However, all indexes
specified for a given agent will be searched sequentially
in this agent. This lets you fine-tune the configuration
to the hardware. For instance, if two remote indexes are stored
on the same physical HDD, it's better to configure one agent
with several sequentially searched indexes to avoid HDD steping.
If they are stored on different HDDs, having two agents will be
advantageous, because the work will be fully parallelized.
The same applies to CPUs; though CPU performance impact caused
by two processes stepping on each other is somewhat smaller
and frequently can be ignored at all.
</para>
<para>
On machines with many CPUs and/or HDDs, agents can be pointed
to the same machine to utilize all of the hardware in parallel
and reduce query latency. There is no need to setup several
<filename>searchd</filename> instances for that; it's legal
to configure the instance to contact itself. Here's an example
setup, intended for a 4-CPU machine, that will use up to
4 CPUs in parallel to process each query:
<programlisting>
index dist
{
	type = distributed
	local = chunk1
	agent = localhost:9312:chunk2
	agent = localhost:9312:chunk3
	agent = localhost:9312:chunk4
}
</programlisting>
Note how one of the chunks is searched locally and the same instance
of searchd queries itself to launch searches through three other ones
in parallel.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
agent = localhost:9312:chunk2 # contact itself
agent = /var/run/searchd.s:chunk2
agent = searchbox2:9312:chunk3,chunk4 # search remote indexes
</programlisting>
</sect2>


<sect2 id="conf-agent-blackhole"><title>agent_blackhole</title>
<para>
Remote blackhole agent declaration in the <link linkend="distributed">distributed index</link>.
Multi-value, optional, default is empty.
Introduced in version 0.9.9-rc1.
</para>
<para>
<option>agent_blackhole</option> lets you fire-and-forget queries
to remote agents. That is useful for debugging (or just testing)
production clusters: you can setup a separate debugging/testing searchd
instance, and forward the requests to this instance from your production
master (aggregator) instance without interfering with production work.
Master searchd will attempt to connect and query blackhole agent
normally, but it will neither wait nor process any responses.
Also, all network errors on blackhole agents will be ignored.
The value format is completely identical to regular
<link linkend="conf-agent">agent</link> directive.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
agent_blackhole = testbox:9312:testindex1,testindex2
</programlisting>
</sect2>


<sect2 id="conf-agent-connect-timeout"><title>agent_connect_timeout</title>
<para>
Remote agent connection timeout, in milliseconds.
Optional, default is 1000 (ie. 1 second).
</para>
<para>
When connecting to remote agents, <filename>searchd</filename>
will wait at most this much time for connect() call to complete
succesfully. If the timeout is reached but connect() does not complete,
and <link linkend="api-func-setretries">retries</link> are enabled,
retry will be initiated.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
agent_connect_timeout = 300
</programlisting>
</sect2>


<sect2 id="conf-agent-query-timeout"><title>agent_query_timeout</title>
<para>
Remote agent query timeout, in milliseconds.
Optional, default is 3000 (ie. 3 seconds).
</para>
<para>
After connection, <filename>searchd</filename> will wait at most this
much time for remote queries to complete. This timeout is fully separate
from connection timeout; so the maximum possible delay caused by
a remote agent equals to the sum of <code>agent_connection_timeout</code> and
<code>agent_query_timeout</code>. Queries will <b>not</b> be retried
if this timeout is reached; a warning will be produced instead.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
agent_query_timeout = 10000 # our query can be long, allow up to 10 sec
</programlisting>
</sect2>


<sect2 id="conf-preopen"><title>preopen</title>
<para>
Whether to pre-open all index files, or open them per each query.
Optional, default is 0 (do not preopen).
</para>
<para>
This option tells <filename>searchd</filename> that it should pre-open
all index files on startup (or rotation) and keep them open while it runs.
Currently, the default mode is <b>not</b> to pre-open the files (this may
change in the future). Preopened indexes take a few (currently 2) file
descriptors per index. However, they save on per-query <code>open()</code> calls;
and also they are invulnerable to subtle race conditions that may happen during
index rotation under high load. On the other hand, when serving many indexes
(100s to 1000s), it still might be desired to open the on per-query basis
in order to save file descriptors.
</para>
<para>
This directive does not affect <filename>indexer</filename> in any way,
it only affects <filename>searchd</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
preopen = 1
</programlisting>
</sect2>


<sect2 id="conf-ondisk-dict"><title>ondisk_dict</title>
<para>
Whether to keep the dictionary file (.spi) for this index on disk, or precache it in RAM.
Optional, default is 0 (precache in RAM).
Introduced in version 0.9.9-rc1.
</para>
<para>
The dictionary (.spi) can be either kept on RAM or on disk. The default
is to fully cache it in RAM. That improves performance, but might cause
too much RAM pressure, especially if prefixes or infixes were used.
Enabling <option>ondisk_dict</option> results in 1 additional disk IO
per keyword per query, but reduces memory footprint.
</para>
<para>
This directive does not affect <filename>indexer</filename> in any way,
it only affects <filename>searchd</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
ondisk_dict = 1
</programlisting>
</sect2>


<sect2 id="conf-inplace-enable"><title>inplace_enable</title>
<para>
Whether to enable in-place index inversion.
Optional, default is 0 (use separate temporary files).
Introduced in version 0.9.9-rc1.
</para>
<para>
<option>inplace_enable</option> greatly reduces indexing disk footprint,
at a cost of slightly slower indexing (it uses around 2x less disk,
but yields around 90-95% the original performance).
</para>
<para>
Indexing involves two major phases. The first phase collects,
processes, and partially sorts documents by keyword, and writes
the intermediate result to temporary files (.tmp*). The second
phase fully sorts the documents, and creates the final index
files. Thus, rebuilding a production index on the fly involves
around 3x peak disk footprint: 1st copy for the intermediate
temporary files, 2nd copy for newly constructed copy, and 3rd copy
for the old index that will be serving production queries in the meantime.
(Intermediate data is comparable in size to the final index.)
That might be too much disk footprint for big data collections,
and <option>inplace_enable</option> allows to reduce it.
When enabled, it reuses the temporary files, outputs the
final data back to them, and renames them on completion.
However, this might require additional temporary data chunk
relocation, which is where the performance impact comes from.
</para>
<para>
This directive does not affect <filename>searchd</filename> in any way,
it only affects <filename>indexer</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
inplace_enable = 1
</programlisting>
</sect2>


<sect2 id="conf-inplace-hit-gap"><title>inplace_hit_gap</title>
<para>
<link linkend="conf-inplace-enable">In-place inversion</link> fine-tuning option.
Controls preallocated hitlist gap size.
Optional, default is 0.
Introduced in version 0.9.9-rc1.
</para>
<para>
This directive does not affect <filename>searchd</filename> in any way,
it only affects <filename>indexer</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
inplace_hit_gap = 1M
</programlisting>
</sect2>


<sect2 id="conf-inplace-docinfo-gap"><title>inplace_docinfo_gap</title>
<para>
<link linkend="conf-inplace-enable">In-place inversion</link> fine-tuning option.
Controls preallocated docinfo gap size.
Optional, default is 0.
Introduced in version 0.9.9-rc1.
</para>
<para>
This directive does not affect <filename>searchd</filename> in any way,
it only affects <filename>indexer</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
inplace_docinfo_gap = 1M
</programlisting>
</sect2>


<sect2 id="conf-inplace-reloc-factor"><title>inplace_reloc_factor</title>
<para>
<link linkend="conf-inplace-reloc-factor">In-place inversion</link> fine-tuning option.
Controls relocation buffer size within indexing memory arena.
Optional, default is 0.1.
Introduced in version 0.9.9-rc1.
</para>
<para>
This directive does not affect <filename>searchd</filename> in any way,
it only affects <filename>indexer</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
inplace_reloc_factor = 0.1
</programlisting>
</sect2>


<sect2 id="conf-inplace-write-factor"><title>inplace_write_factor</title>
<para>
<link linkend="conf-inplace-write-factor">In-place inversion</link> fine-tuning option.
Controls in-place write buffer size within indexing memory arena.
Optional, default is 0.1.
Introduced in version 0.9.9-rc1.
</para>
<para>
This directive does not affect <filename>searchd</filename> in any way,
it only affects <filename>indexer</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
inplace_write_factor = 0.1
</programlisting>
</sect2>


<sect2 id="conf-index-exact-words"><title>index_exact_words</title>
<para>
Whether to index the original keywords along with the stemmed/remapped versions.
Optional, default is 0 (do not index).
Introduced in version 0.9.9-rc1.
</para>
<para>
When enabled, <option>index_exact_words</option> forces <filename>indexer</filename>
to put the raw keywords in the index along with the stemmed versions. That, in turn,
enables <link linkend="extended-syntax">exact form operator</link> in the query language to work.
This impacts the index size and the indexing time. However, searching performance
is not impacted at all.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
index_exact_words = 1
</programlisting>
</sect2>


<sect2 id="conf-overshort-step"><title>overshort_step</title>
<para>
Position increment on overshort (less that <link linkend="conf-min-word-len">min_word_len</link>) keywords.
Optional, allowed values are 0 and 1, default is 1.
Introduced in version 0.9.9-rc1.
</para>
<para>
This directive does not affect <filename>searchd</filename> in any way,
it only affects <filename>indexer</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
overshort_step = 1
</programlisting>
</sect2>


<sect2 id="conf-stopword-step"><title>stopword_step</title>
<para>
Position increment on <link linkend="conf-stopwords">stopwords</link>.
Optional, allowed values are 0 and 1, default is 1.
Introduced in version 0.9.9-rc1.
</para>
<para>
This directive does not affect <filename>searchd</filename> in any way,
it only affects <filename>indexer</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
stopword_step = 1
</programlisting>
</sect2>


<sect2 id="conf-hitless-words"><title>hitless_words</title>
<para>
Hitless words list.
Optional, allowed values are 'all', or a list file name.
Introduced in version 1.10-beta.
</para>
<para>
By default, Sphinx full-text index stores not only a list of matching
documents for every given keyword, but also a list of its in-document positions
(aka hitlist). Hitlists enables phrase, proximity, strict order and other
advanced types of searching, as well as phrase proximity ranking. However,
hitlists for specific frequent keywords (that can not be stopped for
some reason despite being frequent) can get huge and thus slow to process
while querying. Also, in some cases we might only care about boolean
keyword matching, and never need position-based searching operators
(such as phrase matching) nor phrase ranking.
</para>
<para>
<option>hitless_words</option> lets you create indexes that either
do not have positional information (hitlists) at all, or skip it for
specific keywords.
</para>
<para>
Hitless index will generally use less space than the respective
regular index (about 1.5x can be expected). Both indexing and searching
should be faster, at a cost of missing positional query and ranking support.
When searching, positional queries (eg. phrase queries) will be automatically
converted to respective non-positional (document-level) or combined queries.
For instance, if keywords "hello" and "world" are hitless, "hello world"
phrase query will be converted to (hello &amp; world) bag-of-words query,
matching all documents that mention either of the keywords but not necessarily
the exact phrase. And if, in addition, keywords "simon" and "says" are not
hitless, "simon says hello world" will be converted to ("simon says" &amp;
hello &amp; world) query, matching all documents that contain "hello" and
"world" anywhere in the document, and also "simon says" as an exact phrase.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
hitless_words = all
</programlisting>
</sect2>


<sect2 id="conf-expand-keywords"><title>expand_keywords</title>
<para>
Expand keywords with exact forms and/or stars when possible.
Optional, default is 0 (do not expand keywords).
Introduced in version 1.10-beta.
</para>
<para>
Queries against indexes with <option>expand_keywords</option> feature
enabled are internally expanded as follows. If the index was built with
prefix or infix indexing enabled, every keyword gets internally replaced
with a disjunction of keyword itself and a respective prefix or infix
(keyword with stars). If the index was built with both stemming and
<link linkend="conf-index-exact-words">index_exact_words</link> enabled,
exact form is also added. Here's an example that shows how internal
expansion works when all of the above (infixes, stemming, and exact
words) are combined:
<programlisting>
running -> ( running | *running* | =running )
</programlisting>
</para>
<para>
Expanded queries take naturally longer to complete, but can possibly
improve the search quality, as the documents with exact form matches
should be ranked generally higher than documents with stemmed or infix matches.
</para>
<para>
Note that the existing query syntax does not allowe to emulate this
kind of expansion, because internal expansion works on keyword level and
expands keywords within phrase or quorum operators too (which is not
possible through the query syntax).
</para>
<para>
This directive does not affect <filename>indexer</filename> in any way,
it only affects <filename>searchd</filename>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
expand_keywords = 1
</programlisting>
</sect2>


<sect2 id="conf-blend-chars"><title>blend_chars</title>
<para>
Blended characters list.
Optional, default is empty.
Introduced in version 1.10-beta.
</para>
<para>
Blended characters are indexed both as separators and valid characters.
For instance, assume that &amp; is configured as blended and AT&amp;T
occurs in an indexed document. Three different keywords will get indexed,
namely "at&amp;t", treating blended characters as valid, plus "at" and "t",
treating them as separators.
</para>
<para>
Positions for tokens obtained by replacing blended characters with whitespace
are assigned as usual, so regular keywords will be indexed just as if there was
no <option>blend_chars</option> specified at all. An additional token that
mixes blended and non-blended characters will be put at the starting position.
For instance, if the field contents are "AT&amp;T company" occurs in the very
beginning of the text field, "at" will be given position 1, "t" position 2, 
"company" positin 3, and "AT&amp;T" will also be given position 1 ("blending"
with the opening regular keyword). Thus, querying for either AT&amp;T or just
AT will match that document, and querying for "AT T" as a phrase also match it.
Last but not least, phrase query for "AT&amp;T company" will <emphasis>also</emphasis>
match it, despite the position
</para>
<para>
Blended characters can overlap with special characters used in query
syntax (think of T-Mobile or @twitter). Where possible, query parser will
automatically handle blended character as blended. For instance, "hello @twitter"
within quotes (a phrase operator) would handle @-sign as blended, because
@-syntax for field operator is not allowed within phrases. Otherwise,
the character would be handled as an operator. So you might want to
escape the keywords.
</para>
<para>
Starting with version 1.11-beta, blended characters can be remapped,
so that multiple different blended characters could be normalized into
just one base form. This is useful when indexing multiple alternative
Unicode codepoints with equivalent glyphs.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
blend_chars = +, &amp;, U+23
blend_chars = +, &amp;->+ # 1.11 and above
</programlisting>
</sect2>


<sect2 id="conf-blend-mode"><title>blend_mode</title>
<para>
Blended tokens indexing mode.
Optional, default is <option>trim_none</option>.
Introduced in version 1.11-beta.
</para>
<para>
By default, tokens that mix blended and non-blended characters
get indexed in there entirety. For instance, when both at-sign and
an exclamation are in <option>blend_chars</option>, "@dude!" will get
result in two tokens indexed: "@dude!" (with all the blended characters)
and "dude" (without any). Therefore "@dude" query will <emphasis>not</emphasis>
match it.
</para>
<para>
<option>blend_mode</option> directive adds flexibility to this indexing
behavior. It takes a comma-separated list of options.
<programlisting>
blend_mode = option [, option [, ...]]
option = trim_none | trim_head | trim_tail | trim_both | skip_pure
</programlisting>
</para>
<para>
Options specify token indexing variants. If multiple options are
specified, multiple variants of the same token will be indexed.
Regular keywords (resulting from that token by replacing blended
with whitespace) are always be indexed.
<variablelist>
<varlistentry>
	<term>trim_none</term>
	<listitem>Index the entire token.</listitem>
</varlistentry>
<varlistentry>
	<term>trim_head</term>
	<listitem>Trim heading blended characters, and index the resulting token.</listitem>
</varlistentry>
<varlistentry>
	<term>trim_tail</term>
	<listitem>Trim trailing blended characters, and index the resulting token.</listitem>
</varlistentry>
<varlistentry>
	<term>trim_both</term>
	<listitem>Trim both heading and trailing blended characters, and index the resulting token.</listitem>
</varlistentry>
<varlistentry>
	<term>skip_pure</term>
	<listitem>Do not index the token if it's purely blended, that is, consists of blended characters only.</listitem>
</varlistentry>
</variablelist>
Returning to the "@dude!" example above, setting <option>blend_mode = trim_head,
trim_tail</option> will result in two tokens being indexed, "@dude" and "dude!".
In this particular example, <option>trim_both</option> would have no effect,
because trimming both blended characters results in "dude" which is already
indexed as a regular keyword. Indexing "@U.S.A." with <option>trim_both</option>
(and assuming that dot is blended two) would result in "U.S.A" being indexed.
Last but not least, <option>skip_pure</option> enables you to fully ignore
sequences of blended characters only. For example, "one @@@ two" would be
indexed exactly as "one two", and match that as a phrase. That is not the case
by default because a fully blended token gets indexed and offsets the second
keyword position.
</para>
<para>
Default behavior is to index the entire token, equivalent to
<option>blend_mode = trim_none</option>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
blend_mode = trim_tail, skip_pure
</programlisting>
</sect2>


<sect2 id="conf-rt-mem-limit"><title>rt_mem_limit</title>
<para>
RAM chunk size limit.
Optional, default is empty.
Introduced in version 1.10-beta.
</para>
<para>
RT index keeps some data in memory (so-called RAM chunk) and
also maintains a number of on-disk indexes (so-called disk chunks).
This directive lets you control the RAM chunk size. Once there's
too much data to keep in RAM, RT index will flush it to disk,
activate a newly created disk chunk, and reset the RAM chunk.
</para>
<para>
The limit is pretty strict; RT index should never allocate more
memory than it's limited to. The memory is not preallocated either,
hence, specifying 512 MB limit and only inserting 3 MB of data
should result in allocating 3 MB, not 512 MB.
</para>
<para>
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
rt_mem_limit = 512M
</programlisting>
</sect2>


<sect2 id="conf-rt-field"><title>rt_field</title>
<para>
Full-text field declaration.
Multi-value, mandatory
Introduced in version 1.10-beta.
</para>
<para>
Full-text fields to be indexed are declared using <option>rt_field</option>
directive. The names must be unique. The order is preserved; and so field values
in INSERT statements without an explicit list of inserted columns will have to be
in the same order as configured.
</para>
<para>
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
rt_field = author
rt_field = title
rt_field = content
</programlisting>
</sect2>


<sect2 id="conf-rt-attr-uint"><title>rt_attr_uint</title>
<para>
Unsigned integer attribute declaration.
Multi-value (an arbitrary number of attributes is allowed), optional.
Declares an unsigned 32-bit attribute.
Introduced in version 1.10-beta.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
rt_attr_uint = gid
</programlisting>
</sect2>


<sect2 id="conf-rt-attr-bigint"><title>rt_attr_bigint</title>
<para>
BIGINT attribute declaration.
Multi-value (an arbitrary number of attributes is allowed), optional.
Declares a signed 64-bit attribute.
Introduced in version 1.10-beta.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
rt_attr_bigint = guid
</programlisting>
</sect2>


<sect2 id="conf-rt-attr-float"><title>rt_attr_float</title>
<para>
Floating point attribute declaration.
Multi-value (an arbitrary number of attributes is allowed), optional.
Declares a single precision, 32-bit IEEE 754 format float attribute.
Introduced in version 1.10-beta.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
rt_attr_float = gpa
</programlisting>
</sect2>


<sect2 id="conf-rt-attr-timestamp"><title>rt_attr_timestamp</title>
<para>
Timestamp attribute declaration.
Multi-value (an arbitrary number of attributes is allowed), optional.
Introduced in version 1.10-beta.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
rt_attr_timestamp = date_added
</programlisting>
</sect2>


<sect2 id="conf-rt-attr-string"><title>rt_attr_string</title>
<para>
String attribute declaration.
Multi-value (an arbitrary number of attributes is allowed), optional.
Introduced in version 1.10-beta.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
rt_attr_string = author
</programlisting>
</sect2>


</sect1>
<sect1 id="confgroup-indexer"><title><filename>indexer</filename> program configuration options</title>


<sect2 id="conf-mem-limit"><title>mem_limit</title>
<para>
Indexing RAM usage limit.
Optional, default is 32M.
</para>
<para>
Enforced memory usage limit that the <filename>indexer</filename>
will not go above. Can be specified in bytes, or kilobytes
(using K postfix), or megabytes (using M postfix); see the example.
This limit will be automatically raised if set to extremely low value
causing I/O buffers to be less than 8 KB; the exact lower bound
for that depends on the indexed data size. If the buffers are
less than 256 KB, a warning will be produced.
</para>
<para>
Maximum possible limit is 2047M. Too low values can hurt
indexing speed, but 256M to 1024M should be enough for most
if not all datasets. Setting this value too high can cause
SQL server timeouts. During the document collection phase,
there will be periods when the memory buffer is partially
sorted and no communication with the database is performed;
and the database server can timeout. You can resolve that
either by raising timeouts on SQL server side or by lowering
<code>mem_limit</code>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
mem_limit = 256M
# mem_limit = 262144K # same, but in KB
# mem_limit = 268435456 # same, but in bytes
</programlisting>
</sect2>


<sect2 id="conf-max-iops"><title>max_iops</title>
<para>
Maximum I/O operations per second, for I/O throttling.
Optional, default is 0 (unlimited).
</para>
<para>
I/O throttling related option.
It limits maximum count of I/O operations (reads or writes) per any given second.
A value of 0 means that no limit is imposed.
</para>
<para>
<filename>indexer</filename> can cause bursts of intensive disk I/O during
indexing, and it might desired to limit its disk activity  (and keep something
for other programs running on the same machine, such as <filename>searchd</filename>).
I/O throttling helps to do that. It works by enforcing a minimum guaranteed
delay between subsequent disk I/O operations performed by <filename>indexer</filename>. 
Modern SATA HDDs are able to perform up to 70-100+ I/O operations per second
(that's mostly limited by disk heads seek time). Limiting indexing I/O
to a fraction of that can help reduce search performance dedgradation
caused by indexing.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
max_iops = 40
</programlisting>
</sect2>


<sect2 id="conf-max-iosize"><title>max_iosize</title>
<para>
Maximum allowed I/O operation size, in bytes, for I/O throttling.
Optional, default is 0 (unlimited).
</para>
<para>
I/O throttling related option. It limits maximum file I/O operation
(read or write) size for all operations performed by <filename>indexer</filename>.
A value of 0 means that no limit is imposed.
Reads or writes that are bigger than the limit
will be split in several smaller operations, and counted as several operation
by <link linkend="conf-max-iops">max_iops</link> setting. At the time of this
writing, all I/O calls should be under 256 KB (default internal buffer size)
anyway, so <code>max_iosize</code> values higher than 256 KB must not affect anything.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
max_iosize = 1048576
</programlisting>
</sect2>


<sect2 id="conf-max-xmlpipe2-field"><title>max_xmlpipe2_field</title>
<para>
Maximum allowed field size for XMLpipe2 source type, bytes.
Optional, default is 2 MB.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
max_xmlpipe2_field = 8M
</programlisting>
</sect2>


<sect2 id="conf-write-buffer"><title>write_buffer</title>
<para>
Write buffer size, bytes.
Optional, default is 1 MB.
</para>
<para>
Write buffers are used to write both temporary and final index
files when indexing. Larger buffers reduce the number of required
disk writes. Memory for the buffers is allocated in addition to
<link linkend="conf-mem-limit">mem_limit</link>. Note that several
(currently up to 4) buffers for different files will be allocated,
proportionally increasing the RAM usage.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
write_buffer = 4M
</programlisting>
</sect2>


<sect2 id="conf-max-file-field-buffer"><title>max_file_field_buffer</title>
<para>
Maximum file field adaptive buffer size, bytes.
Optional, default is 8 MB, minimum is 1 MB.
</para>
<para>
File field buffer is used to load files referred to from
<link linkend="conf-sql-file-field">sql_file_field</link> columns.
This buffer is adaptive, starting at 1 MB at first allocation,
and growing in 2x steps until either file contents can be loaded,
or maximum buffer size, specified by <option>max_file_field_buffer</option>
directive, is reached.
</para>
<para>
Thus, if there are no file fields are specified, no buffer
is allocated at all.  If all files loaded during indexing are under
(for example) 2 MB in size, but <option>max_file_field_buffer</option> 
value is 128 MB, peak buffer usage would still be only 2 MB. However,
files over 128 MB would be entirely skipped.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
max_file_field_buffer = 128M
</programlisting>
</sect2>


</sect1>
<sect1 id="confgroup-searchd"><title><filename>searchd</filename> program configuration options</title>


<sect2 id="conf-listen"><title>listen</title>
<para>
This setting lets you specify IP address and port, or Unix-domain
socket path, that <code>searchd</code> will listen on.
Introduced in version 0.9.9-rc1.
</para>
<para>
The informal grammar for <code>listen</code> setting is:
<programlisting>
listen = ( address ":" port | port | path ) [ ":" protocol ]
</programlisting>
I.e. you can specify either an IP address (or hostname) and port
number, or just a port number, or Unix socket path. If you specify
port number but not the address, <code>searchd</code> will listen on
all network interfaces. Unix path is identified by a leading slash.
</para>
<para>
Starting with version 0.9.9-rc2, you can also specify a protocol
handler (listener) to be used for connections on this socket.
Supported protocol values are 'sphinx' (Sphinx 0.9.x API protocol)
and 'mysql41' (MySQL protocol used since 4.1 upto at least 5.1).
More details on MySQL protocol support can be found in
<xref linkend="sphinxql"/> section.
</para>
<bridgehead>Examples:</bridgehead>
<programlisting>
listen = localhost
listen = localhost:5000
listen = 192.168.0.1:5000
listen = /var/run/sphinx.s
listen = 9312
listen = localhost:9306:mysql41
</programlisting>
<para>
There can be multiple listen directives, <code>searchd</code> will
listen for client connections on all specified ports and sockets.  If
no <code>listen</code> directives are found then the server will listen
on all available interfaces using the default SphinxAPI port 9312.
Starting with 1.10-beta, it will also listen on default SphinxQL 
port 9306. Both port numbers are assigned by IANA (see
<ulink url="http://www.iana.org/assignments/port-numbers">http://www.iana.org/assignments/port-numbers</ulink>
for details) and should therefore be available.
</para>
<para>
Unix-domain sockets are not supported on Windows.
</para>
</sect2>


<sect2 id="conf-address"><title>address</title>
<para>
Interface IP address to bind on.
Optional, default is 0.0.0.0 (ie. listen on all interfaces).
<b>DEPRECATED</b>, use <link linkend="conf-listen">listen</link> instead.
</para>
<para>
<code>address</code> setting lets you specify which network interface
<filename>searchd</filename> will bind to, listen on, and accept incoming
network connections on. The default value is 0.0.0.0 which means to listen
on all interfaces. At the time, you can <b>not</b> specify multiple interfaces.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
address = 192.168.0.1
</programlisting>
</sect2>


<sect2 id="conf-port"><title>port</title>
<para>
<filename>searchd</filename> TCP port number.
<b>DEPRECATED</b>, use <link linkend="conf-listen">listen</link> instead.
Used to be mandatory. Default port number is 9312.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
port = 9312
</programlisting>
</sect2>


<sect2 id="conf-log"><title>log</title>
<para>
Log file name.
Optional, default is 'searchd.log'.
All <filename>searchd</filename> run time events will be logged in this file.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
log = /var/log/searchd.log
</programlisting>
</sect2>


<sect2 id="conf-query-log"><title>query_log</title>
<para>
Query log file name.
Optional, default is empty (do not log queries).
All search queries will be logged in this file. The format is described in <xref linkend="query-log-format"/>.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
query_log = /var/log/query.log
</programlisting>
</sect2>


<sect2 id="conf-query-log-format"><title>query_log_format</title>
<para>
Query log format.
Optional, allowed values are 'plain' and 'sphinxql', default is 'plain'.
Introduced in version 1.11-beta.
</para>
<para>
Starting with version 1.11-beta, two different log formats are supported.
The default one logs queries in a custom text format. The new one logs
valid SphinxQL statements. This directive allows to switch between the two
formats on search daemon startup. The log format can also be altered
on the fly, using <code>SET GLOBAL query_log_format=sphinxql</code> syntax.
Refer to <xref linkend="query-log-format"/> for more discussion and format
details.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
query_log_format = sphinxql
</programlisting>
</sect2>


<sect2 id="conf-read-timeout"><title>read_timeout</title>
<para>
Network client request read timeout, in seconds.
Optional, default is 5 seconds.
<filename>searchd</filename> will forcibly close the client connections which fail to send a query within this timeout.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
read_timeout = 1
</programlisting>
</sect2>


<sect2 id="conf-client-timeout"><title>client_timeout</title>
<para>
Maximum time to wait between requests (in seconds) when using
persistent connections. Optional, default is five minutes.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
client_timeout = 3600
</programlisting>
</sect2>


<sect2 id="conf-max-children"><title>max_children</title>
<para>
Maximum amount of children to fork (or in other words, concurrent searches to run in parallel).
Optional, default is 0 (unlimited).
</para>
<para>
Useful to control server load. There will be no more than this much concurrent
searches running, at all times. When the limit is reached, additional incoming
clients are dismissed with temporarily failure (SEARCHD_RETRY) status code
and a message stating that the server is maxed out.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
max_children = 10
</programlisting>
</sect2>


<sect2 id="conf-pid-file"><title>pid_file</title>
<para>
<filename>searchd</filename> process ID file name.
Mandatory.
</para>
<para>
PID file will be re-created (and locked) on startup. It will contain
head daemon process ID while the daemon is running, and it will be unlinked
on daemon shutdown. It's mandatory because Sphinx uses it internally
for a number of things: to check whether there already is a running instance
of <filename>searchd</filename>; to stop <filename>searchd</filename>;
to notify it that it should rotate the indexes. Can also be used for
different external automation scripts.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
pid_file = /var/run/searchd.pid
</programlisting>
</sect2>


<sect2 id="conf-max-matches"><title>max_matches</title>
<para>
Maximum amount of matches that the daemon keeps in RAM for each index and can return to the client.
Optional, default is 1000.
</para>
<para>
Introduced in order to control and limit RAM usage, <code>max_matches</code>
setting defines how much matches will be kept in RAM while searching each index.
Every match found will still be <emphasis>processed</emphasis>; but only
best N of them will be kept in memory and return to the client in the end.
Assume that the index contains 2,000,000 matches for the query. You rarely
(if ever) need to retrieve <emphasis>all</emphasis> of them. Rather, you need
to scan all of them, but only choose "best" at most, say, 500 by some criteria
(ie. sorted by relevance, or price, or anything else), and display those
500 matches to the end user in pages of 20 to 100 matches. And tracking
only the best 500 matches is much more RAM and CPU efficient than keeping
all 2,000,000 matches, sorting them, and then discarding everything but
the first 20 needed to display the search results page. <code>max_matches</code>
controls N in that "best N" amount.
</para>
<para>
This parameter noticeably affects per-query RAM and CPU usage.
Values of 1,000 to 10,000 are generally fine, but higher limits must be
used with care. Recklessly raising <code>max_matches</code> to 1,000,000
means that <filename>searchd</filename> will have to allocate and
initialize 1-million-entry matches buffer for <emphasis>every</emphasis>
query. That will obviously increase per-query RAM usage, and in some cases
can also noticeably impact performance.
</para>
<para>
<b>CAVEAT EMPTOR!</b> Note that there also is <b>another</b> place where this limit
is enforced. <code>max_matches</code> can be decreased on the fly
through the <link linkend="api-func-setlimits">corresponding API call</link>,
and the default value in the API is <b>also</b> set to 1,000. So in order
to retrieve more than 1,000 matches to your application, you will have
to change the configuration file, restart searchd, and set proper limit
in <link linkend="api-func-setlimits">SetLimits()</link> call.
Also note that you can not set the value in the API higher than the value
in the .conf file. This is prohibited in order to have some protection
against malicious and/or malformed requests.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
max_matches = 10000
</programlisting>
</sect2>


<sect2 id="conf-seamless-rotate"><title>seamless_rotate</title>
<para>
Prevents <filename>searchd</filename> stalls while rotating indexes with huge amounts of data to precache.
Optional, default is 1 (enable seamless rotation).
</para>
<para>
Indexes may contain some data that needs to be precached in RAM.
At the moment, <filename>.spa</filename>, <filename>.spi</filename> and
<filename>.spm</filename> files are fully precached (they contain attribute data,
MVA data, and keyword index, respectively.)
Without seamless rotate, rotating an index tries to use as little RAM
as possible and works as follows:
<orderedlist>
<listitem>new queries are temporarly rejected (with "retry" error code);</listitem>
<listitem><filename>searchd</filename> waits for all currently running queries to finish;</listitem>
<listitem>old index is deallocated and its files are renamed;</listitem>
<listitem>new index files are renamed and required RAM is allocated;</listitem>
<listitem>new index attribute and dictionary data is preloaded to RAM;</listitem>
<listitem><filename>searchd</filename> resumes serving queries from new index.</listitem>
</orderedlist>
</para>
<para>
However, if there's a lot of attribute or dictionary data, then preloading step
could take noticeble time - up to several minutes in case of preloading 1-5+ GB files.
</para>
<para>
With seamless rotate enabled, rotation works as follows:
<orderedlist>
<listitem>new index RAM storage is allocated;</listitem>
<listitem>new index attribute and dictionary data is asynchronously preloaded to RAM;</listitem>
<listitem>on success, old index is deallocated and both indexes' files are renamed;</listitem>
<listitem>on failure, new index is deallocated;</listitem>
<listitem>at any given moment, queries are served either from old or new index copy.</listitem>
</orderedlist>
</para>
<para>
Seamless rotate comes at the cost of higher <emphasis role="bold">peak</emphasis>
memory usage during the rotation (because both old and new copies of
<filename>.spa/.spi/.spm</filename> data need to be in RAM while
preloading new copy). Average usage stays the same.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
seamless_rotate = 1
</programlisting>
</sect2>


<sect2 id="conf-preopen-indexes"><title>preopen_indexes</title>
<para>
Whether to forcibly preopen all indexes on startup.
Optional, default is 1 (preopen everything).
</para>
<para>
Starting with 1.11-beta, the default value for this
option is now 1 (foribly preopen all indexes). In prior
versions, it used to be 0 (use per-index settings).
</para>
<para>
When set to 1, this directive overrides and enforces
<link linkend="conf-preopen">preopen</link> on all indexes.
They will be preopened, no matter what is the per-index
<code>preopen</code> setting. When set to 0, per-index
settings can take effect. (And they default to 0.)
</para>
<para>
Pre-opened indexes avoid races between search queries
and rotations that can cause queries to fail occasionally.
They also make <filename>searchd</filename> use more file
handles. In most scenarios it's therefore preferred and
recommended to preopen indexes.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
preopen_indexes = 1
</programlisting>
</sect2>


<sect2 id="conf-unlink-old"><title>unlink_old</title>
<para>
Whether to unlink .old index copies on succesful rotation.
Optional, default is 1 (do unlink).
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
unlink_old = 0
</programlisting>
</sect2>

<sect2 id="conf-attr-flush-period"><title>attr_flush_period</title>
<para>
When calling <code>UpdateAttributes()</code> to update document attributes in
real-time, changes are first written to the in-memory copy of attributes
(<option>docinfo</option> must be set to <option>extern</option>).
Then, once <filename>searchd</filename> shuts down normally (via <code>SIGTERM</code>
being sent), the changes are written to disk.
Introduced in version 0.9.9-rc1.
</para>
<para>Starting with 0.9.9-rc1, it is possible to tell <filename>searchd</filename>
to periodically write these changes back to disk, to avoid them being lost. The time
between those intervals is set with <option>attr_flush_period</option>, in seconds.
</para>
<para>It defaults to 0, which disables the periodic flushing, but flushing will
still occur at normal shut-down.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
attr_flush_period = 900 # persist updates to disk every 15 minutes
</programlisting>
</sect2>


<sect2 id="conf-ondisk-dict-default"><title>ondisk_dict_default</title>
<para>
Instance-wide defaults for <link linkend="conf-ondisk-dict">ondisk_dict</link> directive.
Optional, default it 0 (precache dictionaries in RAM).
Introduced in version 0.9.9-rc1.
</para>
<para>
This directive lets you specify the default value of
<link linkend="conf-ondisk-dict">ondisk_dict</link> for all the indexes
served by this copy of <filename>searchd</filename>. Per-index directive
take precedence, and will overwrite this instance-wide default value,
allowing for fine-grain control.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
ondisk_dict_default = 1 # keep all dictionaries on disk
</programlisting>
</sect2>


<sect2 id="conf-max-packet-size"><title>max_packet_size</title>
<para>
Maximum allowed network packet size.
Limits both query packets from clients, and response packets from remote agents in distributed environment.
Only used for internal sanity checks, does not directly affect RAM use or performance.
Optional, default is 8M.
Introduced in version 0.9.9-rc1.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
max_packet_size = 32M
</programlisting>
</sect2>


<sect2 id="conf-mva-updates-pool"><title>mva_updates_pool</title>
<para>
Shared pool size for in-memory MVA updates storage.
Optional, default size is 1M.
Introduced in version 0.9.9-rc1.
</para>
<para>
This setting controls the size of the shared storage pool for updated MVA values.
Specifying 0 for the size disable MVA updates at all. Once the pool size limit
is hit, MVA update attempts will result in an error. However, updates on regular
(scalar) attributes will still work. Due to internal technical difficulties,
currently it is <b>not</b> possible to store (flush) <b>any</b> updates on indexes
where MVA were updated; though this might be implemented in the future.
In the meantime, MVA updates are intended to be used as a measure to quickly
catchup with latest changes in the database until the next index rebuild;
not as a persistent storage mechanism.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
mva_updates_pool = 16M
</programlisting>
</sect2>


<sect2 id="conf-crash-log-path"><title>crash_log_path</title>
<para>
Deprecated debugging setting, path (formally prefix) for crash log files.
Introduced in version 0.9.9-rc1. Deprecated in version 1.11-beta,
as crash debugging information now gets logged into searchd.log
in text form, and separate binary crash logs are no longer needed.
</para>
</sect2>


<sect2 id="conf-max-filters"><title>max_filters</title>
<para>
Maximum allowed per-query filter count.
Only used for internal sanity checks, does not directly affect RAM use or performance.
Optional, default is 256.
Introduced in version 0.9.9-rc1.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
max_filters = 1024
</programlisting>
</sect2>


<sect2 id="conf-max-filter-values"><title>max_filter_values</title>
<para>
Maximum allowed per-filter values count.
Only used for internal sanity checks, does not directly affect RAM use or performance.
Optional, default is 4096.
Introduced in version 0.9.9-rc1.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
max_filter_values = 16384
</programlisting>
</sect2>


<sect2 id="conf-listen-backlog"><title>listen_backlog</title>
<para>
TCP listen backlog.
Optional, default is 5.
</para>
<para>
Windows builds currently (as of 0.9.9) can only process the requests
one by one. Concurrent requests will be enqueued by the TCP stack
on OS level, and requests that can not be enqueued will immediately
fail with "connection refused" message. listen_backlog directive
controls the length of the connection queue. Non-Windows builds
should work fine with the default value.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
listen_backlog = 20
</programlisting>
</sect2>


<sect2 id="conf-read-buffer"><title>read_buffer</title>
<para>
Per-keyword read buffer size.
Optional, default is 256K.
</para>
<para>
For every keyword occurrence in every search query, there are
two associated read buffers (one for document list and one for
hit list). This setting lets you control their sizes, increasing
per-query RAM use, but possibly decreasing IO time.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
read_buffer = 1M
</programlisting>
</sect2>


<sect2 id="conf-read-unhinted"><title>read_unhinted</title>
<para>
Unhinted read size.
Optional, default is 32K.
</para>
<para>
When querying, some reads know in advance exactly how much data
is there to be read, but some currently do not. Most prominently,
hit list size in not currently known in advance. This setting
lest you control how much data to read in such cases. It will
impact hit list IO time, reducing it for lists larger than
unhinted read size, but raising it for smaller lists. It will
<b>not</b> affect RAM use because read buffer will be already
allocated. So it should be not greater than read_buffer.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
read_unhinted = 32K
</programlisting>
</sect2>


<sect2 id="conf-max-batch-queries"><title>max_batch_queries</title>
<para>
Limits the amount of queries per batch.
Optional, default is 32.
</para>
<para>
Makes searchd perform a sanity check of the amount of the queries
submitted in a single batch when using <link linkend="multi-queries">multi-queries</link>.
Set it to 0 to skip the check.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
max_batch_queries = 256
</programlisting>
</sect2>


<sect2 id="conf-subtree-docs-cache"><title>subtree_docs_cache</title>
<para>
Max common subtree document cache size, per-query.
Optional, default is 0 (disabled).
</para>
<para>
Limits RAM usage of a common subtree optimizer (see <xref linkend="multi-queries"/>).
At most this much RAM will be spent to cache document entries per each query.
Setting the limit to 0 disables the optimizer.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
subtree_docs_cache = 8M
</programlisting>
</sect2>


<sect2 id="conf-subtree-hits-cache"><title>subtree_hits_cache</title>
<para>
Max common subtree hit cache size, per-query.
Optional, default is 0 (disabled).
</para>
<para>
Limits RAM usage of a common subtree optimizer (see <xref linkend="multi-queries"/>).
At most this much RAM will be spent to cache keyword occurrences (hits) per each query.
Setting the limit to 0 disables the optimizer.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
subtree_hits_cache = 16M
</programlisting>
</sect2>


<sect2 id="conf-workers"><title>workers</title>
<para>
Multi-processing mode (MPM).
Optional; allowed values are none, fork, prefork, and threads.
Default is fork on Unix based systems, and threads on Windows.
Introduced in version 1.10-beta.
</para>
<para>
Lets you choose how <filename>searchd</filename> processes multiple
concurrent requests. The possible values are:
<variablelist>
<varlistentry>
	<term>none</term>
	<listitem>All requests will be handled serially, one-by-one.
		Prior to 1.x, this was the only mode available on Windows.
	</listitem>
</varlistentry>
<varlistentry>
	<term>fork</term>
	<listitem>A new child process will be forked to handle every
		incoming request. Historically, this is the default mode.
	</listitem>
</varlistentry>
<varlistentry>
	<term>prefork</term>
	<listitem>On startup, <filename>searchd</filename> will pre-fork
		a number of worker processes, and pass the incoming requests
		to one of those children.
	</listitem>
</varlistentry>
<varlistentry>
	<term>threads</term>
	<listitem>A new thread will be created to handle every
		incoming request. This is the only mode compatible with
		RT indexing backend.
	</listitem>
</varlistentry>
</variablelist>
</para>
<para>
Historically, <filename>searchd</filename> used fork-based model,
which generally performs OK but spends a noticeable amount of CPU
in fork() system call when there's a high amount of (tiny) requests
per second. Prefork mode was implemented to alleviate that; with
prefork, worker processes are basically only created on startup
and re-created on index rotation, somewhat reducing fork() call
pressure.
</para>
<para>
Threads mode was implemented along with RT backend and is required
to use RT indexes. (Regular disk-based indexes work in all the
available modes.)
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
workers = threads
</programlisting>
</sect2>


<sect2 id="conf-dist-threads"><title>dist_threads</title>
<para>
Max local worker threads to use for parallelizable requests (searching a distributed index; building a batch of snippets).
Optional, default is 0, which means to disable in-request parallelism.
Introduced in version 1.10-beta.
</para>
<para>
Distributed index can include several local indexes. <option>dist_threads</option>
lets you easily utilize multiple CPUs/cores for that (previously existing
alternative was to specify the indexes as remote agents, pointing searchd
to itself and paying some network overheads).
</para>
<para>
When set to a value N greater than 1, this directive will create up to
N threads for every query, and schedule the specific searches within these
threads. For example, if there are 7 local indexes to search and dist_threads
is set to 2, then 2 parallel threads would be created: one that sequentially
searches 4 indexes, and another one that searches the other 3 indexes.
</para>
<para>
In case of CPU bound workload, setting <option>dist_threads</option>
to 1x the number of cores is advised (creating more threads than cores
will not improve query time). In case of mixed CPU/disk bound workload
it might sometimes make sense to use more (so that all cores could be
utilizes even when there are threads that wait for I/O completion).
</para>
<para>
Note that <option>dist_threads</option> does <b>not</b> require
threads MPM. You can perfectly use it with fork or prefork MPMs too.
</para>
<para>
Starting with version 1.11-beta, building a batch of snippets
with <option>load_files</option> flag enabled can also be parallelized.
Up to <option>dist_threads</option> threads are be created to process
those files. That speeds up snippet extraction when the total amount
of document data to process is significant (hundreds of megabytes).
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
index dist_test
{
	type = distributed
	local = chunk1
	local = chunk2
	local = chunk3
	local = chunk4
}

# ...

dist_threads = 4
</programlisting>
</sect2>


<sect2 id="conf-binlog-path"><title>binlog_path</title>
<para>
Binary log (aka transaction log) files path.
Optional, default is build-time configured data directory.
Introduced in version 1.10-beta.
</para>
<para>
Binary logs are used for crash recovery of RT index data that
would otherwise only be stored in RAM.  When logging is enabled,
every transaction COMMIT-ted into RT index gets written into
a log file.  Logs are then automatically replayed on startup
after an unclean shutdown, recovering the logged changes.
</para>
<para>
<option>binlog_path</option> directive specifies the binary log
files location.  It should contain just the path; <option>searchd</option>
will create and unlink multiple binlog.* files in that path as necessary
(binlog data, metadata, and lock files, etc).
</para>
<para>
Empty value disables binary logging. That improves performance,
but puts RT index data at risk.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
binlog_path = # disable logging
binlog_path = /var/data # /var/data/binlog.001 etc will be created
</programlisting>
</sect2>


<sect2 id="conf-binlog-flush"><title>binlog_flush</title>
<para>
Binary log transaction flush/sync mode.
Optional, default is 2 (flush every transaction, sync every second).
Introduced in version 1.10-beta.
</para>
<para>
This directive controls how frequently will binary log be flushed
to OS and synced to disk. Three modes are supported:
<itemizedlist>
<listitem>0, flush and sync every second. Best performance,
but up to 1 second worth of committed transactions can be lost
both on daemon crash, or OS/hardware crash.
</listitem>
<listitem>1, flush and sync every transaction. Worst performance,
but every committed transaction data is guaranteed to be saved.
</listitem>
<listitem>2, flush every transaction, sync every second.
Good performance, and every committed transaction is guaranteed
to be saved in case of daemon crash. However, in case of OS/hardware
crash up to 1 second worth of committed transactions can be lost.
</listitem>
</itemizedlist>
</para>
<para>
For those familiar with MySQL and InnoDB, this directive is entirely
similar to <option>innodb_flush_log_at_trx_commit</option>. In most
cases, the default hybrid mode 2 provides a nice balance of speed
and safety, with full RT index data protection against daemon crashes,
and some protection against hardware ones.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
binlog_flush = 1 # ultimate safety, low speed
</programlisting>
</sect2>


<sect2 id="conf-binlog-max-log-size"><title>binlog_max_log_size</title>
<para>
Maximum binary log file size.
Optional, default is 0 (do not reopen binlog file based on size).
Introduced in version 1.10-beta.
</para>
<para>
A new binlog file will be forcibly opened once the current binlog file
reaches this limit. This achieves a finer granularity of logs and can yield
more efficient binlog disk usage under certain borderline workloads.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
binlog_max_log_size = 16M
</programlisting>
</sect2>


<sect2 id="conf-collation-server"><title>collation_server</title>
<para>
Default server collation.
Optional, default is libc_ci.
Introduced in version 1.11-beta.
</para>
<para>
Specifies the default collation used for incoming requests. 
The collation can be overridden on a per-query basis.
Refer to <xref linkend="collations"/> section for the list of available collations and other details.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
collation_server = utf8_ci
</programlisting>
</sect2>


<sect2 id="conf-collation-libc-locale"><title>collation_libc_locale</title>
<para>
Server libc locale.
Optional, default is C.
Introduced in version 1.11-beta.
</para>
<para>
Specifies the libc locale, affecting the libc-based collations.
Refer to <xref linkend="collations"/> section for the details.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
collation_libc_locale = fr_FR
</programlisting>
</sect2>


<sect2 id="conf-plugin-dir"><title>plugin_dir</title>
<para>
Trusted location for the dynamic libraries (UDFs).
Optional, default is empty (no location).
Introduced in version 1.11-beta.
</para>
<para>
Specifies the trusted directory from which the
<link linkend="udf">UDF libraries</link> can be loaded. Requires
<link linkend="conf-workers">workers = thread</link> to take effect.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
workers = threads
plugin_dir = /usr/local/sphinx/lib
</programlisting>
</sect2>


<sect2 id="conf-mysql-version-string"><title>mysql_version_string</title>
<para>
A server version string to return via MySQL protocol.
Optional, default is empty (return Sphinx version).
Introduced in version 1.11-beta.
</para>
<para>
Several picky MySQL client libraries depend on a particular version
number format used by MySQL, and moreover, sometimes choose a different
execution path based on the reported version number (rather than the
indicated capabilities flags). For instance, Python MySQLdb 1.2.2 throws
an exception when the version number is not in X.Y.ZZ format; MySQL .NET
connector 6.3.x fails internally on version numbers 1.x along with
a certain combination of flags, etc. To workaround that, you can use
<option>mysql_version_string</option> directive and have <filename>searchd</filename>
report a different version to clients connecting over MySQL protocol.
(By default, it reports its own version.)
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
mysql_version_string = 5.0.37
</programlisting>
</sect2>


<sect2 id="conf-rt-flush-period"><title>rt_flush_period</title>
<para>
RT indexes RAM chunk flush check period, in seconds.
Optional, default is 0 (do not flush).
Introduced in version 1.11-beta.
</para>
<para>
Actively updated RT indexes that however fully fit in RAM chunks
can result in ever-growing binlogs, impacting disk use and crash
recovery time. With this directive the search daemon performs
periodic flush checks, and eligible RAM chunks can get saved,
enabling consequential binlog cleanup. See <xref linkend="rt-binlog"/>
for more details.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
rt_flush_period = 3600
</programlisting>
</sect2>


<sect2 id="conf-thread-stack"><title>thread_stack</title>
<para>
Per-thread stack size.
Optional, default is 64K.
Introduced in version 1.11-beta.
</para>
<para>
In the <code>workers = threads</code> mode, every request is processed
with a separate thread that needs its own stack space. By default, 64K per
thread are allocated for stack. However, extremely complex search requests
might eventually exhaust the default stack and require more. For instance,
a query that matches a few thousand keywords (either directly or through
term expansion) can eventually run out of stack. Previously, that resulted
in crashes. Starting with 1.11-beta, <filename>searchd</filename> attempts
to estimate the expected stack use, and blocks the potentially dangerous
queries. To process such queries, you can either the thread stack size
by using the <code>thread_stack</code> directive (or switch to a different
<code>workers</code> setting if that is possible).
</para>
<para>
A query with N levels of nesting is estimated to require approximately
30+0.12*N KB of stack, meaning that the default 64K is enough for queries
with upto 300 levels, 150K for upto 1000 levels, etc. If the stack size limit
is not met, <filename>searchd</filename> fails the query and reports
the required stack size in the error message.
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
thread_stack = 256K
</programlisting>
</sect2>


<sect2 id="conf-expansion-limit"><title>expansion_limit</title>
<para>
The maximum number of expanded keywords for a single wildcard.
Optional, default is 0 (no limit).
Introduced in version 1.11-beta.
</para>
<para>
When doing substring searches against indexes built with
<code>dict = keywords</code> enabled, a single wildcard may
potentially result in thousands and even millions of matched
keywords (think of matching 'a*' against the entire Oxford
dictionary). This directive lets you limit the impact
of such expansions. Setting <code>expansion_limit = N</code>
restricts expansions to no more than N of the most frequent
matching keywords (per each wildcard in the query).
</para>
<bridgehead>Example:</bridgehead>
<programlisting>
expansion_limit = 16
</programlisting>
</sect2>


</sect1>


</chapter>

<!--
<chapter id="developers"><title>Developer's corner</title>

<sect1 id="architecture-overview"><title>Sphinx architecture overview</title>
(to be added)
</sect1>

<sect1 id="adding-data-sources"><title>Adding new data source drivers</title>
(to be added)
</sect1>

<sect1 id="adding-data-sources"><title>API porting guidelines</title>
(to be added)
</sect1>

</chapter>
-->

<appendix id="changelog"><title>Sphinx revision history</title>

<sect1 id="rel111"><title>Version 1.11-beta, ???</title>
<itemizedlist>
<listitem>added <link linkend="sphinxql-show-tables">SHOW TABLES</link>, <link linkend="sphinxql-describe">DESCRIBE</link> SphinxQL statements</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel110"><title>Version 1.10-beta, 19 jul 2010</title>
<itemizedlist>
<listitem>added RT indexes support (<xref linkend="rt-indexes"/>)</listitem>
<listitem>added prefork and threads support (<link linkend="conf-workers">workers</link> directives)</listitem>
<listitem>added multi-threaded local searches in distributed indexes (<link linkend="conf-dist-threads">dist_threads</link> directive)</listitem>
<listitem>added common subquery cache (<link linkend="conf-subtree-docs-cache">subtree_docs_cache</link>,
	<link linkend="conf-subtree-hits-cache">subtree_hits_cache</link> directives)</listitem>
<listitem>added string attributes support (<link linkend="conf-sql-attr-string">sql_attr_string</link>,
	<link linkend="conf-sql-field-string">sql_field_string</link>,
	<link linkend="conf-xmlpipe-attr-string">xml_attr_string</link>,
	<link linkend="conf-xmlpipe-field-string">xml_field_string</link> directives)</listitem>
<listitem>added indexing-time word counter (<link linkend="conf-sql-attr-str2wordcount">sql_attr_str2wordcount</link>,
	<link linkend="conf-sql-field-str2wordcount">sql_field_str2wordcount</link> directives)</listitem>
<listitem>added <link linkend="sphinxql-call-snippets">CALL SNIPPETS()</link>,
	<link linkend="sphinxql-call-keywords">CALL KEYWORDS()</link> SphinxQL statements</listitem>
<listitem>added <option>field_weights</option>, <option>index_weights</option> options to
	SphinxQL <link linkend="sphinxql-select">SELECT</link> statement</listitem>
<listitem>added insert-only SphinxQL-talking tables to SphinxSE (connection='sphinxql://host[:port]/index')</listitem>
<listitem>added <option>select</option> option to SphinxSE queries</listitem>
<listitem>added backtrace on crash to <filename>searchd</filename></listitem>
<listitem>added SQL+FS indexing, aka loading files by names fetched from SQL
	(<link linkend="conf-sql-file-field">sql_file_field</link> directive)</listitem>
<listitem>added a watchdog in threads mode to <filename>searchd</filename></listitem>
<listitem>added automatic row phantoms elimination to index merge</listitem>
<listitem>added hitless indexing support (hitless_words directive)</listitem>
<listitem>added --check, --strip-path, --htmlstrip, --dumphitlist ... --wordid switches to <link linkend="ref-indextool">indextool</link></listitem>
<listitem>added --stopwait, --logdebug switches to <link linkend="ref-searchd">searchd</link></listitem>
<listitem>added --dump-rows, --verbose switches to <link linkend="ref-indexer">indexer</link></listitem>
<listitem>added "blended" characters indexing support (<link linkend="conf-blend-chars">blend_chars</link> directive)</listitem>
<listitem>added joined/payload field indexing (<link linkend="conf-sql-joined-field">sql_joined_field</link> directive)</listitem>
<listitem>added <link linkend="api-func-flushattributes">FlushAttributes() API call</link></listitem>
<listitem>added query_mode, force_all_words, limit_passages, limit_words, start_passage_id, load_files, html_strip_mode,
	allow_empty options, and %PASSAGE_ID% macro in before_match, after_match options
	to <link linkend="api-func-buildexcerpts">BuildExcerpts()</link> API call</listitem>
<listitem>added @groupby/@count/@distinct columns support to SELECT (but not to expressions)</listitem>
<listitem>added query-time keyword expansion support (<link linkend="conf-expand-keywords">expand_keywords</link> directive,
	<link linkend="api-func-setrankingmode">SPH_RANK_SPH04</link> ranker)</listitem>
<listitem>added query batch size limit option (<link linkend="conf-max-batch-queries">max_batch_queries</link> directive; was hardcoded)</listitem>
<listitem>added SINT() function to expressions</listitem>
<listitem>improved SphinxQL syntax error reporting</listitem>
<listitem>improved expression optimizer (better constant handling)</listitem>
<listitem>improved dash handling within keywords (no longer treated as an operator)</listitem>
<listitem>improved snippets (better passage selection/trimming, around option now a hard limit)</listitem>
<listitem>optimized index format that yields ~20-30% smaller indexes</listitem>
<listitem>optimized sorting code (indexing time 1-5% faster on average; 100x faster in worst case)</listitem>
<listitem>optimized searchd startup time (moved .spa preindexing to indexer), added a progress bar</listitem>
<listitem>optimized queries against indexes with many attributes (eliminated redundant copying)</listitem>
<listitem>optimized 1-keyword queries (performace regression introduced in 0.9.9)</listitem>
<listitem>optimized SphinxQL protocol overheads, and performance on bigger result sets</listitem>
<listitem>optimized unbuffered attributes writes on index merge</listitem>
<listitem>changed attribute handling, duplicate names are strictly forbidden now</listitem>
<listitem>fixed that SphinxQL sessions could stall shutdown</listitem>
<listitem>fixed consts with leading minus in SphinxQL</listitem>
<listitem>fixed AND/OR precedence in expressions</listitem>
<listitem>fixed #334, AVG() on integers was not computed in floats</listitem>
<listitem>fixed #371, attribute flush vs 2+ GB files</listitem>
<listitem>fixed #373, segfault on distributed queries vs certain libc versions</listitem>
<listitem>fixed #398, stopwords not stopped in prefix/infix indexes</listitem>
<listitem>fixed #404, erroneous MVA failures in indextool --check</listitem>
<listitem>fixed #408, segfault on certain query batches (regular scan, plus a scan with MVA groupby)</listitem>
<listitem>fixed #431, occasional shutdown hangs in preforked workers</listitem>
<listitem>fixed #436, trunk checkout builds vs Solaris sh</listitem>
<listitem>fixed #440, escaping vs parentheses declared as valid in charset_table</listitem>
<listitem>fixed #442, occasional non-aligned free in MVA indexing</listitem>
<listitem>fixed #447, occasional crashes in MVA indexing</listitem>
<listitem>fixed #449, pconn busyloop on aborted clients on certain arches</listitem>
<listitem>fixed #465, build issue on Alpha</listitem>
<listitem>fixed #468, build issue in libsphinxclient</listitem>
<listitem>fixed #472, multiple stopword files failing to load</listitem>
<listitem>fixed #489, buffer overflow in query logging</listitem>
<listitem>fixed #493, Python API assertion after error returned from Query()</listitem>
<listitem>fixed #500, malformed MySQL packet when sending MVAs</listitem>
<listitem>fixed #504, SIGPIPE in libsphinxclient</listitem>
<listitem>fixed #506, better MySQL protocol commands support in SphinxQL (PING etc)</listitem>
<listitem>fixed #509, indexing ranged results from stored procedures</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel099"><title>Version 0.9.9-release, 02 dec 2009</title>
<itemizedlist>
<listitem>added Open, Close, Status calls to libsphinxclient (C API)</listitem>
<listitem>added automatic persistent connection reopening to PHP, Python APIs</listitem>
<listitem>added 64-bit value/range filters, fullscan mode support to SphinxSE</listitem>
<listitem>MAJOR CHANGE, our IANA assigned ports are 9312 and 9306 respectively (goodbye, trusty 3312)</listitem>
<listitem>MAJOR CHANGE, erroneous filters now fail with an error (were silently ignored before)</listitem>
<listitem>optimized unbuffered .spa writes on merge</listitem>
<listitem>optimized 1-keyword queries ranking in extended2 mode</listitem>
<listitem>fixed #441 (IO race in case of highly conccurent load on a preopened)</listitem>
<listitem>fixed #434 (distrubuted indexes were not searchable via MySQL protocol)</listitem>
<listitem>fixed #317 (indexer MVA progress counter)</listitem>
<listitem>fixed #398 (stopwords not removed from search query)</listitem>
<listitem>fixed #328 (broken cutoff)</listitem>
<listitem>fixed #250 (now quoting paths w/spaces when installing Windows service)</listitem>
<listitem>fixed #348 (K-list was not updated on merge)</listitem>
<listitem>fixed #357 (destination index were not K-list-filtered on merge)</listitem>
<listitem>fixed #369 (precaching .spi files over 2 GBs)</listitem>
<listitem>fixed #438 (missing boundary proximity matches)</listitem>
<listitem>fixed #371 (.spa flush in case of files over 2 GBs)</listitem>
<listitem>fixed #373 (crashes on distributed queries via mysql proto)</listitem>
<listitem>fixed critical bugs in hit merging code</listitem>
<listitem>fixed #424 (ordinals could be misplaced during indexing in case of bitfields etc)</listitem>
<listitem>fixed #426 (failing SE build on Solaris; thanks to Ben Beecher)</listitem>
<listitem>fixed #423 (typo in SE caused crash on SHOW STATUS)</listitem>
<listitem>fixed #363 (handling of read_timeout over 2147 seconds)</listitem>
<listitem>fixed #376 (minor error message mismatch)</listitem>
<listitem>fixed #413 (minus in SphinxQL)</listitem>
<listitem>fixed #417 (floats w/o leading digit in SphinxQL)</listitem>
<listitem>fixed #403 (typo in SetFieldWeights name in Java API)</listitem>
<listitem>fixed index rotation vs persistent connections</listitem>
<listitem>fixed backslash handling in SphinxQL parser</listitem>
<listitem>fixed uint unpacking vs. PHP 5.2.9 (possibly other versions)</listitem>
<listitem>fixed #325 (filter settings send from SphinxSE)</listitem>
<listitem>fixed #352 (removed mysql wrapper around close() in SphinxSE)</listitem>
<listitem>fixed #389 (display error messages through SphinxSE status variable)</listitem>
<listitem>fixed linking with port-installed iconv on OS X</listitem>
<listitem>fixed negative 64-bit unpacking in PHP API</listitem>
<listitem>fixed #349 (escaping backslash in query emulation mode)</listitem>
<listitem>fixed #320 (disabled multi-query route when select items differ)</listitem>
<listitem>fixed #353 (better quorum counts check)</listitem>
<listitem>fixed #341 (merging of trailing hits; maybe other ranking issues too)</listitem>
<listitem>fixed #368 (partially; @field "" caused crashes; now resets field limit)</listitem>
<listitem>fixed #365 (field mask was leaking on field-limited terms)</listitem>
<listitem>fixed #339 (updated debug query dumper)</listitem>
<listitem>fixed #361 (added SetConnectTimeout() to Java API)</listitem>
<listitem>fixed #338 (added missing fullscan to mode check in Java API)</listitem>
<listitem>fixed #323 (added floats support to SphinxQL)</listitem>
<listitem>fixed #340 (support listen=port:proto syntax too)</listitem>
<listitem>fixed #332 (\r is legal SphinxQL space now)</listitem>
<listitem>fixed xmlpipe2 K-lists</listitem>
<listitem>fixed #322 (safety gaps in mysql protocol row buffer)</listitem>
<listitem>fixed #313 (return keyword stats for empty indexes too)</listitem>
<listitem>fixed #344 (invalid checkpoints after merge)</listitem>
<listitem>fixed #326 (missing CLOCK_xxx on FreeBSD)</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel099rc2"><title>Version 0.9.9-rc2, 08 apr 2009</title>
<itemizedlist>
<listitem>added IsConnectError(), Open(), Close() calls to Java API (bug #240)</listitem>
<listitem>added <link linkend="conf-read-buffer">read_buffer</link>, <link linkend="conf-read-unhinted">read_unhinted</link> directives</listitem>
<listitem>added checks for build options returned by mysql_config (builds on Solaris now)</listitem>
<listitem>added fixed-RAM index merge (bug #169)</listitem>
<listitem>added logging chained queries count in case of (optimized) multi-queries</listitem>
<listitem>added <link linkend="sort-expr">GEODIST()</link> function</listitem>
<listitem>added <link linkend="ref-searchd">--status switch to searchd</link></listitem>
<listitem>added MySpell (OpenOffice) affix file support (bug #281)</listitem>
<listitem>added <link linkend="conf-odbc-dsn">ODBC support</link> (both Windows and UnixODBC)</listitem>
<listitem>added support for @id in IN() (bug #292)</listitem>
<listitem>added support for <link linkend="api-func-setselect">aggregate functions</link> in GROUP BY (namely AVG, MAX, MIN, SUM)</listitem>
<listitem>added <link linkend="sphinxse-snippets">MySQL UDF that builds snippets</link> using searchd</listitem>
<listitem>added <link linkend="conf-write-buffer">write_buffer</link> directive (defaults to 1M)</listitem>
<listitem>added <link linkend="conf-xmlpipe-fixup-utf8">xmlpipe_fixup_utf8</link> directive</listitem>
<listitem>added suggestions sample</listitem>
<listitem>added microsecond precision int64 timer (bug #282)</listitem>
<listitem>added <link linkend="conf-listen-backlog">listen_backlog directive</link></listitem>
<listitem>added <link linkend="conf-max-xmlpipe2-field">max_xmlpipe2_field</link> directive</listitem>
<listitem>added <link linkend="sphinxql">initial SphinxQL support</link> to mysql41 handler, SELECT .../SHOW WARNINGS/STATUS/META are handled</listitem>
<listitem>added support for different network protocols, and mysql41 protocol</listitem>
<listitem>added <link linkend="api-func-setrankingmode">fieldmask ranker</link>, updated SphinxSE list of rankers</listitem>
<listitem>added <link linkend="conf-mysql-ssl">mysql_ssl_xxx</link> directives</listitem>
<listitem>added <link linkend="ref-searchd">--cpustats (requires clock_gettime()) and --status switches</link> to searchd</listitem>
<listitem>added performance counters, <link linkend="api-func-status">Status()</link> API call</listitem>
<listitem>added <link linkend="conf-overshort-step">overshort_step</link> and <link linkend="conf-stopword-step">stopword_step</link> directives</listitem>
<listitem>added <link linkend="extended-syntax">strict order operator</link> (aka operator before, eg. "one &lt;&lt; two &lt;&lt; three")</listitem>
<listitem>added <link linkend="ref-indextool">indextool</link> utility, moved --dumpheader there, added --debugdocids, --dumphitlist options</listitem>
<listitem>added own RNG, reseeded on @random sort query (bug #183)</listitem>
<listitem>added <link linkend="extended-syntax">field-start and field-end modifiers support</link> (syntax is "^hello world$"; field-end requires reindex)</listitem>
<listitem>added MVA attribute support to IN() function</listitem>
<listitem>added <link linkend="sort-expr">AND, OR, and NOT support</link> to expressions</listitem>
<listitem>improved logging of (optimized) multi-queries (now logging chained query count)</listitem>
<listitem>improved handshake error handling, fixed protocol version byte order (omg)</listitem>
<listitem>updated SphinxSE to protocol 1.22</listitem>
<listitem>allowed phrase_boundary_step=-1 (trick to emulate keyword expansion)</listitem>
<listitem>removed SPH_MAX_QUERY_WORDS limit</listitem>
<listitem>fixed CLI search vs documents missing from DB (bug #257)</listitem>
<listitem>fixed libsphinxclient results leak on subsequent sphinx_run_queries call (bug #256)</listitem>
<listitem>fixed libsphinxclient handling of zero max_matches and cutoff (bug #208)</listitem>
<listitem>fixed Java API over-64K string reads (eg. big snippets) in Java API (bug #181)</listitem>
<listitem>fixed Java API 2nd Query() after network error in 1st Query() call (bug #308)</listitem>
<listitem>fixed typo-class bugs in SetFilterFloatRange (bug #259), SetSortMode (bug #248)</listitem>
<listitem>fixed missing @@relaxed support (bug #276), fixed missing error on @nosuchfield queries, documented @@relaxed</listitem>
<listitem>fixed UNIX socket permissions to 0777 (bug #288)</listitem>
<listitem>fixed xmlpipe2 crash on schemas with no fields, added better document structure checks</listitem>
<listitem>fixed (and optimized) expr parser vs IN() with huge (10K+) args count</listitem>
<listitem>fixed double EarlyCalc() in fullscan mode (minor performance impact)</listitem>
<listitem>fixed phrase boundary handling in some cases (on buffer end, on trailing whitespace)</listitem>
<listitem>fixes in snippets (aka excerpts) generation</listitem>
<listitem>fixed inline attrs vs id64 index corruption</listitem>
<listitem>fixed head searchd crash on config re-parse failure</listitem>
<listitem>fixed handling of numeric keywords with leading zeroes such as "007" (bug #251)</listitem>
<listitem>fixed junk in SphinxSE status variables (bug #304)</listitem>
<listitem>fixed wordlist checkpoints serialization (bug #236)</listitem>
<listitem>fixed unaligned docinfo id access (bug #230)</listitem>
<listitem>fixed GetRawBytes() vs oversized blocks (headers with over 32K charset_table should now work, bug #300)</listitem>
<listitem>fixed buffer overflow caused by too long dest wordform, updated tests</listitem>
<listitem>fixed IF() return type (was always int, is deduced now)</listitem>
<listitem>fixed legacy queries vs. special chars vs. multiple indexes</listitem>
<listitem>fixed write-write-read socket access pattern vs Nagle vs delays vs FreeBSD (oh wow)</listitem>
<listitem>fixed exceptions vs query-parser issue</listitem>
<listitem>fixed late calc vs @weight in expressions (bug #285)</listitem>
<listitem>fixed early lookup/calc vs filters (bug #284)</listitem>
<listitem>fixed emulated MATCH_ANY queries (empty proximity and phrase queries are allowed now)</listitem>
<listitem>fixed MATCH_ANY ranker vs fields with no matches</listitem>
<listitem>fixed index file size vs inplace_enable (bug #245)</listitem>
<listitem>fixed that old logs were not closed on USR1 (bug #221)</listitem>
<listitem>fixed handling of '!' alias to NOT operator (bug #237)</listitem>
<listitem>fixed error handling vs query steps (step failure was not reported)</listitem>
<listitem>fixed querying vs inline attributes</listitem>
<listitem>fixed stupid bug in escaping code, fixed EscapeString() and made it static</listitem>
<listitem>fixed parser vs @field -keyword, foo|@field bar, "" queries (bug #310)</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel099rc1"><title>Version 0.9.9-rc1, 17 nov 2008</title>
<itemizedlist>
<listitem>added <link linkend="conf-min-stemming-len">min_stemming_len</link> directive</listitem>
<listitem>added <link linkend="api-func-isconnecterror">IsConnectError()</link> API call (helps distingusih API vs remote errors)</listitem>
<listitem>added duplicate log messages filter to searchd</listitem>
<listitem>added --nodetach debugging switch to searchd</listitem>
<listitem>added blackhole agents support for debugging/testing (<link linkend="conf-agent-blackhole">agent_blackhole</link> directive)</listitem>
<listitem>added <link linkend="conf-max-filters">max_filters</link>, <link linkend="conf-max-filter-values">max_filter_values</link> directives (were hardcoded before)</listitem>
<listitem>added int64 expression evaluation path, automatic inference, and BIGINT() enforcer function</listitem>
<listitem>added crash handler for debugging (<link linkend="conf-crash-log-path">crash_log_path</link> directive)</listitem>
<listitem>added MS SQL (aka SQL Server) source support (Windows only, <link linkend="conf-mssql-winauth">mssql_winauth</link> and <link linkend="conf-mssql-unicode">mssql_unicode</link> directives)</listitem>
<listitem>added indexer-side column unpacking feature (<link linkend="conf-unpack-zlib">unpack_zlib</link>, <link linkend="conf-unpack-mysqlcompress">unpack_mysqlcompress</link> directives)</listitem>
<listitem>added nested brackers and NOTs support to <link linkend="extended-syntax">query language</link>, rewritten query parser</listitem>
<listitem>added persistent connections support (<link linkend="api-func-open">Open()</link> and <link linkend="api-func-close">Close()</link> API calls)</listitem>
<listitem>added <link linkend="conf-index-exact-words">index_exact_words</link> feature, and exact form operator to query language ("hello =world")</listitem>
<listitem>added status variables support to SphinxSE (SHOW STATUS LIKE 'sphinx_%')</listitem>
<listitem>added <link linkend="conf-max-packet-size">max_packet_size</link> directive (was hardcoded at 8M before)</listitem>
<listitem>added UNIX socket support, and multi-interface support (<link linkend="conf-listen">listen</link> directive)</listitem>
<listitem>added star-syntax support to <link linkend="api-func-buildexcerpts">BuildExcerpts()</link> API call</listitem>
<listitem>added inplace inversion of .spa and .spp (<link linkend="conf-inplace-enable">inplace_enable</link> directive, 1.5-2x less disk space for indexing)</listitem>
<listitem>added builtin Czech stemmer (morphology=stem_cz)</listitem>
<listitem>added <link linkend="sort-expr">IDIV(), NOW(), INTERVAL(), IN() functions</link> to expressions</listitem>
<listitem>added index-level early-reject based on filters</listitem>
<listitem>added MVA updates feature (<link linkend="conf-mva-updates-pool">mva_updates_pool</link> directive)</listitem>
<listitem>added select-list feature with computed expressions support (see <link linkend="api-func-setselect">SetSelect()</link> API call, test.php --select switch), protocol 1.22</listitem>
<listitem>added integer expressions support (2x faster than float)</listitem>
<listitem>added multiforms support (multiple source words in wordforms file)</listitem>
<listitem>added <link linkend="api-func-setrankingmode">legacy rankers</link> (MATCH_ALL/MATCH_ANY/etc), removed legacy matching code (everything runs on V2 engine now)</listitem>
<listitem>added <link linkend="extended-syntax">field position limit</link> modifier to field operator (syntax: @title[50] hello world)</listitem>
<listitem>added killlist support (<link linkend="conf-sql-query-killlist">sql_query_killlist</link> directive, --merge-killlists switch)</listitem>
<listitem>added on-disk SPI support (<link linkend="conf-ondisk-dict">ondisk_dict</link> directive)</listitem>
<listitem>added indexer IO stats</listitem>
<listitem>added periodic .spa flush (<link linkend="conf-attr-flush-period">attr_flush_period</link> directive)</listitem>
<listitem>added config reload on SIGHUP</listitem>
<listitem>added per-query attribute overrides feature (see <link linkend="api-func-setoverride">SetOverride()</link> API call); protocol 1.21</listitem>
<listitem>added signed 64bit attrs support (<link linkend="conf-sql-attr-bigint">sql_attr_bigint</link> directive)</listitem>
<listitem>improved HTML stripper to also skip PIs (&lt;? ... ?&gt;, such as &lt;?php ... ?&gt;)</listitem>
<listitem>improved excerpts speed (upto 50x faster on big documents)</listitem>
<listitem>fixed a short window of searchd inaccessibility on startup (started listen()ing too early before)</listitem>
<listitem>fixed .spa loading on systems where read() is 2GB capped</listitem>
<listitem>fixed infixes vs morphology issues</listitem>
<listitem>fixed backslash escaping, added backslash to EscapeString()</listitem>
<listitem>fixed handling of over-2GB dictionary files (.spi)</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel0981"><title>Version 0.9.8.1, 30 oct 2008</title>
<itemizedlist>
<listitem>added configure script to libsphinxclient</listitem>
<listitem>changed proximity/quorum operator syntax to require whitespace after length</listitem>
<listitem>fixed potential head process crash on SIGPIPE during "maxed out" message</listitem>
<listitem>fixed handling of incomplete remote replies (caused over-degraded distributed results, in rare cases)</listitem>
<listitem>fixed sending of big remote requests (caused distributed requests to fail, in rare cases)</listitem>
<listitem>fixed FD_SET() overflow (caused searchd to crash on startup, in rare cases)</listitem>
<listitem>fixed MVA vs distributed indexes (caused loss of 1st MVA value in result set)</listitem>
<listitem>fixed tokenizing of exceptions terminated by specials (eg. "GPS AT&amp;T" in extended mode)</listitem>
<listitem>fixed buffer overrun in stemmer on overlong tokens occasionally emitted by proximity/quorum operator parser (caused crashes on certain proximity/quorum queries)</listitem>
<listitem>fixed wordcount ranker (could be dropping hits)</listitem>
<listitem>fixed --merge feature (numerous different fixes, caused broken indexes)</listitem>
<listitem>fixed --merge-dst-range performance</listitem>
<listitem>fixed prefix/infix generation for stopwords</listitem>
<listitem>fixed ignore_chars vs specials</listitem>
<listitem>fixed misplaced F_SETLKW check (caused certain build types, eg. RPM build on FC8, to fail)</listitem>
<listitem>fixed dictionary-defined charsets support in spelldump, added \x-style wordchars support</listitem>
<listitem>fixed Java API to properly send long strings (over 64K; eg. long document bodies for excerpts)</listitem>
<listitem>fixed Python API to accept offset/limit of 'long' type</listitem>
<listitem>fixed default ID range (that filtered out all 64-bit values) in Java and Python APIs</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel098"><title>Version 0.9.8, 14 jul 2008</title>
<bridgehead>Indexing</bridgehead>
<itemizedlist>
<listitem>added support for 64-bit document and keyword IDs, --enable-id64 switch to configure</listitem>
<listitem>added support for floating point attributes</listitem>
<listitem>added support for bitfields in attributes, <link linkend="conf-sql-attr-bool">sql_attr_bool</link> directive and bit-widths part in <link linkend="conf-sql-attr-uint">sql_attr_uint</link> directive</listitem>
<listitem>added support for multi-valued attributes (MVA)</listitem>
<listitem>added metaphone preprocessor</listitem>
<listitem>added libstemmer library support, provides stemmers for a number of additional languages</listitem>
<listitem>added xmlpipe2 source type, that supports arbitrary fields and attributes</listitem>
<listitem>added word form dictionaries, <link linkend="conf-wordforms">wordforms</link> directive (and spelldump utility)</listitem>
<listitem>added tokenizing exceptions, <link linkend="conf-exceptions">exceptions</link> directive</listitem>
<listitem>added an option to fully remove element contents to HTML stripper, <link linkend="conf-html-remove-elements">html_remove_elements</link> directive</listitem>
<listitem>added HTML entities decoder (with full XHTML1 set support) to HTML stripper</listitem>
<listitem>added per-index HTML stripping settings, <link linkend="conf-html-strip">html_strip</link>, <link linkend="conf-html-index-attrs">html_index_attrs</link>, and <link linkend="conf-html-remove-elements">html_remove_elements</link> directives</listitem>
<listitem>added IO load throttling, <link linkend="conf-max-iops">max_iops</link> and <link linkend="conf-max-iosize">max_iosize</link> directives</listitem>
<listitem>added SQL load throttling, <link linkend="conf-sql-ranged-throttle">sql_ranged_throttle</link> directive</listitem>
<listitem>added an option to index prefixes/infixes for given fields only, <link linkend="conf-prefix-fields">prefix_fields</link> and <link linkend="conf-infix-fields">infix_fields</link> directives</listitem>
<listitem>added an option to ignore certain characters (instead of just treating them as whitespace), <link linkend="conf-ignore-chars">ignore_chars</link> directive</listitem>
<listitem>added an option to increment word position on phrase boundary characters, <link linkend="conf-phrase-boundary">phrase_boundary</link> and <link linkend="conf-phrase-boundary-step">phrase_boundary_step</link> directives</listitem>
<listitem>added --merge-dst-range switch (and filters) to index merging feature (--merge switch)</listitem>
<listitem>added <link linkend="conf-mysql-connect-flags">mysql_connect_flags</link> directive (eg. to reduce indexing time MySQL network traffic and/or time)</listitem>
<listitem>improved ordinals sorting; now runs in fixed RAM</listitem>
<listitem>improved handling of documents with zero/NULL ids, now skipping them instead of aborting</listitem>
</itemizedlist>
<bridgehead>Search daemon</bridgehead>
<itemizedlist>
<listitem>added an option to unlink old index on succesful rotation, <link linkend="conf-unlink-old">unlink_old</link> directive</listitem>
<listitem>added an option to keep index files open at all times (fixes subtle races on rotation), <link linkend="conf-preopen">preopen</link> and <link linkend="conf-preopen-indexes">preopen_indexes</link> directives</listitem>
<listitem>added an option to profile searchd disk I/O, --iostats command-line option</listitem>
<listitem>added an option to rotate index seamlessly (fully avoids query stalls), <link linkend="conf-seamless-rotate">seamless_rotate</link> directive</listitem>
<listitem>added HTML stripping support to excerpts (uses per-index settings)</listitem>
<listitem>added 'exact_phrase', 'single_passage', 'use_boundaries', 'weight_order 'options to <link linkend="api-func-buildexcerpts">BuildExcerpts()</link> API call</listitem>
<listitem>added distributed attribute updates propagation</listitem>
<listitem>added distributed retries on master node side</listitem>
<listitem>added log reopen on SIGUSR1</listitem>
<listitem>added --stop switch (sends SIGTERM to running instance)</listitem>
<listitem>added Windows service mode, and --servicename switch</listitem>
<listitem>added Windows --rotate support</listitem>
<listitem>improved log timestamping, now with millisecond precision</listitem>
</itemizedlist>
<bridgehead>Querying</bridgehead>
<itemizedlist>
<listitem>added extended engine V2 (faster, cleaner, better; SPH_MATCH_EXTENDED2 mode)</listitem>
<listitem>added ranking modes support (V2 engine only; <link linkend="api-func-setrankingmode">SetRankingMode()</link> API call)</listitem>
<listitem>added quorum searching support to query language (V2 engine only; example: "any three of all these words"/3)</listitem>
<listitem>added query escaping support to query language, and <link linkend="api-func-escapestring">EscapeString()</link> API call</listitem>
<listitem>added multi-field syntax support to query language (example: "@(field1,field2) something"), and @@relaxed field checks option</listitem>
<listitem>added optional star-syntax ('word*') support in keywords, <link linkend="conf-enable-star">enable_star</link> directive (for prefix/infix indexes only)</listitem>
<listitem>added full-scan support (query must be fully empty; can perform block-reject optimization)</listitem>
<listitem>added COUNT(DISTINCT(attr)) calculation support, <link linkend="api-func-setgroupdistinct">SetGroupDistinct()</link> API call</listitem>
<listitem>added group-by on MVA support, <link linkend="api-func-setarrayresult">SetArrayResult()</link> PHP API call</listitem>
<listitem>added per-index weights feature, <link linkend="api-func-setindexweights">SetIndexWeights()</link> API call</listitem>
<listitem>added geodistance support, <link linkend="api-func-setgeoanchor">SetGeoAnchor()</link> API call</listitem>
<listitem>added result set sorting by arbitrary expressions in run time (eg. "@weight+log(price)*2.5"), SPH_SORT_EXPR mode</listitem>
<listitem>added result set sorting by @custom compile-time sorting function (see src/sphinxcustomsort.inl)</listitem>
<listitem>added result set sorting by @random value</listitem>
<listitem>added result set merging for indexes with different schemas</listitem>
<listitem>added query comments support (3rd arg to <link linkend="api-func-query">Query()</link>/<link linkend="api-func-addquery">AddQuery()</link> API calls, copied verbatim to query log)</listitem>
<listitem>added keyword extraction support, <link linkend="api-func-buildkeywords">BuildKeywords()</link> API call</listitem>
<listitem>added binding field weights by name, <link linkend="api-func-setfieldweights">SetFieldWeights()</link> API call</listitem>
<listitem>added optional limit on query time, <link linkend="api-func-setmaxquerytime">SetMaxQueryTime()</link> API call</listitem>
<listitem>added optional limit on found matches count (4rd arg to <link linkend="api-func-setlimits">SetLimits()</link> API call, so-called 'cutoff')</listitem>
</itemizedlist>
<bridgehead>APIs and SphinxSE</bridgehead>
<itemizedlist>
<listitem>added pure C API (libsphinxclient)</listitem>
<listitem>added Ruby API (thanks to Dmytro Shteflyuk)</listitem>
<listitem>added Java API</listitem>
<listitem>added SphinxSE support for MVAs (use varchar), floats (use float), 64bit docids (use bigint)</listitem>
<listitem>added SphinxSE options "floatrange", "geoanchor", "fieldweights", "indexweights", "maxquerytime", "comment", "host" and "port"; and support for "expr:CLAUSE"</listitem>
<listitem>improved SphinxSE max query size (using MySQL condition pushdown), upto 256K now</listitem>
</itemizedlist>
<bridgehead>General</bridgehead>
<itemizedlist>
<listitem>added scripting (shebang syntax) support to config files (example: #!/usr/bin/php in the first line)</listitem>
<listitem>added unified config handling and validation to all programs</listitem>
<listitem>added unified documentation </listitem>
<listitem>added .spec file for RPM builds</listitem>
<listitem>added automated testing suite</listitem>
<listitem>improved index locking, now fcntl()-based instead of buggy file-existence-based</listitem>
<listitem>fixed unaligned RAM accesses, now works on SPARC and ARM</listitem>
</itemizedlist>
<bridgehead id="rel098-fixes-since-rc2">Changes and fixes since 0.9.8-rc2</bridgehead>
<itemizedlist>
<listitem>added pure C API (libsphinxclient)</listitem>
<listitem>added Ruby API</listitem>
<listitem>added SetConnectTimeout() PHP API call</listitem>
<listitem>added allowed type check to UpdateAttributes() handler (bug #174)</listitem>
<listitem>added defensive MVA checks on index preload (protection against broken indexes, bug #168)</listitem>
<listitem>added sphinx-min.conf sample file</listitem>
<listitem>added --without-iconv switch to configure</listitem>
<listitem>removed redundant -lz dependency in searchd</listitem>
<listitem>removed erroneous "xmlpipe2 deprecated" warning</listitem>
<listitem>fixed EINTR handling in piped read (bug #166)</listitem>
<listitem>fixup query time before logging and sending to client (bug #153)</listitem>
<listitem>fixed attribute updates vs full-scan early-reject index (bug #149)</listitem>
<listitem>fixed gcc warnings (bug #160)</listitem>
<listitem>fixed mysql connection attempt vs pgsql source type (bug #165)</listitem>
<listitem>fixed 32-bit wraparound when preloading over 2 GB files</listitem>
<listitem>fixed "out of memory" message vs over 2 GB allocs (bug #116)</listitem>
<listitem>fixed unaligned RAM access detection on ARM (where unaligned reads do not crash but produce wrong results)</listitem>
<listitem>fixed missing full scan results in some cases</listitem>
<listitem>fixed several bugs in --merge, --merge-dst-range</listitem>
<listitem>fixed @geodist vs MultiQuery and filters, @expr vs MultiQuery</listitem>
<listitem>fixed GetTokenEnd() vs 1-grams (was causing crash in excerpts)</listitem>
<listitem>fixed sql_query_range to handle empty strings in addition to NULL strings (Postgres specific)</listitem>
<listitem>fixed morphology=none vs infixes</listitem>
<listitem>fixed case sensitive attributes names in UpdateAttributes()</listitem>
<listitem>fixed ext2 ranking vs. stopwords (now using atompos from query parser)</listitem>
<listitem>fixed EscapeString() call</listitem>
<listitem>fixed escaped specials (now handled as whitespace if not in charset)</listitem>
<listitem>fixed schema minimizer (now handles type/size mismatches)</listitem>
<listitem>fixed word stats in extended2; stemmed form is now returned</listitem>
<listitem>fixed spelldump case folding vs dictionary-defined character sets</listitem>
<listitem>fixed Postgres BOOLEAN handling </listitem>
<listitem>fixed enforced "inline" docinfo on empty indexes (normally ok, but index merge was really confused)</listitem>
<listitem>fixed rare count(distinct) out-of-bounds issue (it occasionaly caused too high @distinct values)</listitem>
<listitem>fixed hangups on documents with id=DOCID_MAX in some cases</listitem>
<listitem>fixed rare crash in tokenizer (prefixed synonym vs. input stream eof)</listitem>
<listitem>fixed query parser vs "aaa (bbb ccc)|ddd" queries</listitem>
<listitem>fixed BuildExcerpts() request in Java API</listitem>
<listitem>fixed Postgres specific memory leak</listitem>
<listitem>fixed handling of overshort keywords (less than min_word_len)</listitem>
<listitem>fixed HTML stripper (now emits space after indexed attributes)</listitem>
<listitem>fixed 32-field case in query parser</listitem>
<listitem>fixed rare count(distinct) vs. querying multiple local indexes vs. reusable sorter issue</listitem>
<listitem>fixed sorting of negative floats in SPH_SORT_EXTENDED mode</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel097"><title>Version 0.9.7, 02 apr 2007</title>
<itemizedlist>
<listitem>added support for <option>sql_str2ordinal_column</option></listitem>
<listitem>added support for upto 5 sort-by attrs (in extended sorting mode)</listitem>
<listitem>added support for separate groups sorting clause (in group-by mode)</listitem>
<listitem>added support for on-the-fly attribute updates (PRE-ALPHA; will change heavily; use for preliminary testing ONLY)</listitem>
<listitem>added support for zero/NULL attributes</listitem>
<listitem>added support for 0.9.7 features to SphinxSE</listitem>
<listitem>added support for n-grams (alpha, 1-grams only for now)</listitem>
<listitem>added support for warnings reported to client</listitem>
<listitem>added support for exclude-filters</listitem>
<listitem>added support for prefix and infix indexing (see <option>max_prefix_len</option>, <option>max_infix_len</option>)</listitem>
<listitem>added <option>@*</option> syntax to reset current field to query language</listitem>
<listitem>added removal of duplicate entries in query index order</listitem>
<listitem>added PHP API workarounds for PHP signed/unsigned braindamage</listitem>
<listitem>added locks to avoid two concurrent indexers working on same index</listitem>
<listitem>added check for existing attributes vs. <option>docinfo=none</option> case</listitem>
<listitem>improved groupby code a lot (better precision, and upto 25x times faster in extreme cases)</listitem>
<listitem>improved error handling and reporting</listitem>
<listitem>improved handling of broken indexes (reports error instead of hanging/crashing)</listitem>
<listitem>improved <option>mmap()</option> limits for attributes and wordlists (now able to map over 4 GB on x64 and over 2 GB on x32 where possible)</listitem>
<listitem>improved <option>malloc()</option> pressure in head daemon (search time should not degrade with time any more)</listitem>
<listitem>improved <filename>test.php</filename> command line options</listitem>
<listitem>improved error reporting (distributed query, broken index etc issues now reported to client)</listitem>
<listitem>changed default network packet size to be 8M, added extra checks</listitem>
<listitem>fixed division by zero in BM25 on 1-document collections (in extended matching mode)</listitem>
<listitem>fixed <filename>.spl</filename> files getting unlinked</listitem>
<listitem>fixed crash in schema compatibility test</listitem>
<listitem>fixed UTF-8 Russian stemmer</listitem>
<listitem>fixed requested matches count when querying distributed agents</listitem>
<listitem>fixed signed vs. unsigned issues everywhere (ranged queries, CLI search output, and obtaining docid)</listitem>
<listitem>fixed potential crashes vs. negative query offsets</listitem>
<listitem>fixed 0-match docs vs. extended mode vs. stats</listitem>
<listitem>fixed group/timestamp filters being ignored if querying from older clients</listitem>
<listitem>fixed docs to mention <option>pgsql</option> source type</listitem>
<listitem>fixed issues with explicit '&amp;' in extended matching mode</listitem>
<listitem>fixed wrong assertion in SBCS encoder</listitem>
<listitem>fixed crashes with no-attribute indexes after rotate</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel097rc2"><title>Version 0.9.7-rc2, 15 dec 2006</title>
<itemizedlist>
<listitem>added support for extended matching mode (query language)</listitem>
<listitem>added support for extended sorting mode (sorting clauses)</listitem>
<listitem>added support for SBCS excerpts</listitem>
<listitem>added <option>mmap()ing</option> for attributes and wordlist (improves search time, speeds up <option>fork()</option> greatly)</listitem>
<listitem>fixed attribute name handling to be case insensitive</listitem>
<listitem>fixed default compiler options to simplify post-mortem debugging (added <option>-g</option>, removed <option>-fomit-frame-pointer</option>)</listitem>
<listitem>fixed rare memory leak</listitem>
<listitem>fixed "hello hello" queries in "match phrase" mode</listitem>
<listitem>fixed issue with excerpts, texts and overlong queries</listitem>
<listitem>fixed logging multiple index name (no longer tokenized)</listitem>
<listitem>fixed trailing stopword not flushed from tokenizer</listitem>
<listitem>fixed boolean evaluation</listitem>
<listitem>fixed pidfile being wrongly <option>unlink()ed</option> on <option>bind()</option> failure</listitem>
<listitem>fixed <option>--with-mysql-includes/libs</option> (they conflicted with well-known paths)</listitem>
<listitem>fixes for 64-bit platforms</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel097rc"><title>Version 0.9.7-rc1, 26 oct 2006</title>
<itemizedlist>
<listitem>added alpha index merging code</listitem>
<listitem>added an option to decrease <option>max_matches</option> per-query</listitem>
<listitem>added an option to specify IP address for searchd to listen on</listitem>
<listitem>added support for unlimited amount of configured sources and indexes</listitem>
<listitem>added support for group-by queries</listitem>
<listitem>added support for /2 range modifier in charset_table</listitem>
<listitem>added support for arbitrary amount of document attributes</listitem>
<listitem>added logging filter count and index name</listitem>
<listitem>added <option>--with-debug</option> option to configure to compile in debug mode</listitem>
<listitem>added <option>-DNDEBUG</option> when compiling in default mode</listitem>
<listitem>improved search time (added doclist size hints, in-memory wordlist cache, and used VLB coding everywhere)</listitem>
<listitem>improved (refactored) SQL driver code (adding new drivers should be very easy now)</listitem>
<listitem>improved exceprts generation</listitem>
<listitem>fixed issue with empty sources and ranged queries</listitem>
<listitem>fixed querying purely remote distributed indexes</listitem>
<listitem>fixed suffix length check in English stemmer in some cases</listitem>
<listitem>fixed UTF-8 decoder for codes over U+20000 (for CJK)</listitem>
<listitem>fixed UTF-8 encoder for 3-byte sequences (for CJK)</listitem>
<listitem>fixed overshort (less than <option>min_word_len</option>) words prepended to next field</listitem>
<listitem>fixed source connection order (indexer does not connect to all sources at once now)</listitem>
<listitem>fixed line numbering in config parser</listitem>
<listitem>fixed some issues with index rotation</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel096"><title>Version 0.9.6, 24 jul 2006</title>
<itemizedlist>
<listitem>added support for empty indexes</listitem>
<listitem>added support for multiple sql_query_pre/post/post_index</listitem>
<listitem>fixed timestamp ranges filter in "match any" mode</listitem>
<listitem>fixed configure issues with --without-mysql and --with-pgsql options</listitem>
<listitem>fixed building on Solaris 9</listitem>
</itemizedlist>
</sect1>

<sect1 id="rel096rc1"><title>Version 0.9.6-rc1, 26 jun 2006</title>
<itemizedlist>
<listitem>added boolean queries support (experimental, beta version)</listitem>
<listitem>added simple file-based query cache (experimental, beta version)</listitem>
<listitem>added storage engine for MySQL 5.0 and 5.1 (experimental, beta version)</listitem>
<listitem>added GNU style <filename>configure</filename> script</listitem>
<listitem>added new searchd protocol (all binary, and should be backwards compatible)</listitem>
<listitem>added distributed searching support to searchd</listitem>
<listitem>added PostgreSQL driver</listitem>
<listitem>added excerpts generation</listitem>
<listitem>added <option>min_word_len</option> option to index</listitem>
<listitem>added <option>max_matches</option> option to searchd, removed hardcoded MAX_MATCHES limit</listitem>
<listitem>added initial documentation, and a working <filename>example.sql</filename></listitem>
<listitem>added support for multiple sources per index</listitem>
<listitem>added soundex support</listitem>
<listitem>added group ID ranges support</listitem>
<listitem>added <option>--stdin</option> command-line option to search utility</listitem>
<listitem>added <option>--noprogress</option> option to indexer</listitem>
<listitem>added <option>--index</option> option to search</listitem>
<listitem>fixed UTF-8 decoder (3-byte codepoints did not work)</listitem>
<listitem>fixed PHP API to handle big result sets faster</listitem>
<listitem>fixed config parser to handle empty values properly</listitem>
<listitem>fixed redundant <code>time(NULL)</code> calls in time-segments mode</listitem>
</itemizedlist>
</sect1>

</appendix>

</book>